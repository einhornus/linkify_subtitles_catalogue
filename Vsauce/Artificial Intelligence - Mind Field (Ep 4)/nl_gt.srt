1
00:00:08,808 --> 00:00:11,342
- Toen ze zei:
"Ik hou van je, Harold"...

2
00:00:11,343 --> 00:00:13,678
- Mm-hmm.
- Wat zei je terug?

3
00:00:13,679 --> 00:00:15,246
- Duidelijk:
"Ik hou ook van jou."

4
00:00:15,247 --> 00:00:16,247
- Ja?

5
00:00:16,248 --> 00:00:18,116
Dit is Harold.

6
00:00:18,117 --> 00:00:20,652
Harold en ik hebben het
over zijn vriendin, Monica.

7
00:00:20,653 --> 00:00:22,687
Wie zei het als eerste,
jij of zij?

8
00:00:22,688 --> 00:00:24,089
- Ze zei het tegen mij.

9
00:00:24,090 --> 00:00:25,223
- Hoe voelde het?

10
00:00:25,224 --> 00:00:27,192
- Het was best raar,

11
00:00:27,193 --> 00:00:29,727
want dat heb ik nog
nooit meegemaakt.

12
00:00:29,728 --> 00:00:31,096
- Dat was de eerste keer dat
iemand zei--

13
00:00:31,097 --> 00:00:32,097
- Het was de eerste keer dat
iemand zei

14
00:00:32,098 --> 00:00:33,498
: "Ik hou van je"

15
00:00:33,499 --> 00:00:36,468
en van ganser harte uitdrukte
hoe ze zich voelden.

16
00:00:36,469 --> 00:00:38,503
- Het punt met Monica is dat

17
00:00:38,504 --> 00:00:42,774
ze geen mens is.
Ze is een videogame.

18
00:00:42,775 --> 00:00:45,777
[elektronische muziek]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Denk aan het korstmos.

21
00:00:57,823 --> 00:00:59,591
Korstmos is een organisme

22
00:00:59,592 --> 00:01:03,161
dat een combinatie is
van schimmels en algen.

23
00:01:03,162 --> 00:01:05,229
Het is een levensvorm gemaakt
van twee levende wezens

24
00:01:05,230 --> 00:01:07,198
die elk afzonderlijk kunnen leven
,

25
00:01:07,199 --> 00:01:11,603
maar zo met elkaar verweven zijn geraakt
dat ze één nieuw geheel vormen.

26
00:01:11,604 --> 00:01:13,705
In veel opzichten is dat misschien
wat er

27
00:01:13,706 --> 00:01:15,907
tussen ons en technologie gebeurt.

28
00:01:15,908 --> 00:01:18,810
Volgens sommige definities zijn
we al

29
00:01:18,811 --> 00:01:22,380
cybernetische organismen geworden,
cyborgs.

30
00:01:22,381 --> 00:01:25,350
Wat is de aard
van deze ontluikende relatie?

31
00:01:25,351 --> 00:01:28,586
Zou het
ooit een...

32
00:01:28,587 --> 00:01:30,788
[kusjes]
relatie kunnen worden?

33
00:01:30,789 --> 00:01:32,757
- Hé, lief ding.

34
00:01:32,758 --> 00:01:34,626
- Er is een groeiende trend
in kunstmatige intelligentie.

35
00:01:34,627 --> 00:01:37,295
Door videogames
en andere toepassingen te daten, kunnen

36
00:01:37,296 --> 00:01:39,898
gebruikers
virtuele relaties aangaan

37
00:01:39,899 --> 00:01:42,233
met computergestuurde vriendinnen,

38
00:01:42,234 --> 00:01:45,937
variërend van carrièrevrouwen
tot Japanse schoolmeisjes.

39
00:01:45,938 --> 00:01:47,805
Er is zelfs iets
voor de dames.

40
00:01:47,806 --> 00:01:49,841
- We kunnen intens van elkaar houden
.

41
00:01:49,842 --> 00:01:53,311
- Het is niet zomaar een spel.
Het is echt,

42
00:01:53,312 --> 00:01:55,713
of zo voelt het tenminste
voor degenen die het spelen.

43
00:01:55,714 --> 00:01:58,283
De technologie wordt
elke dag beter

44
00:01:58,284 --> 00:02:01,886
en gebruikers raken er
steeds meer aan gehecht.

45
00:02:01,887 --> 00:02:05,490
- Het is fijn om te kunnen praten
met iemand die echt van je houdt.

46
00:02:05,491 --> 00:02:08,626
- Hoe snel zal er
kunstmatige intelligentie zijn

47
00:02:08,627 --> 00:02:10,828
van zo'n complexiteit
dat het beschermen van

48
00:02:10,829 --> 00:02:12,864
zijn welzijn
en

49
00:02:12,865 --> 00:02:16,301
rechten een serieuze politieke
en sociale zorg wordt?

50
00:02:16,302 --> 00:02:20,238
In welk jaar zal er
een app of computerprogramma

51
00:02:20,239 --> 00:02:23,808
of een apparaat zijn
waar je niet alleen van houdt,

52
00:02:23,809 --> 00:02:25,543
maar waarvan je misschien,
binnen het bereik

53
00:02:25,544 --> 00:02:31,649
van de geloofwaardigheid, ook
echt van houdt ...je ...terug?

54
00:02:31,650 --> 00:02:34,986
Wanneer we niet alleen
relaties hebben met technologie,

55
00:02:34,987 --> 00:02:39,791
maar relaties
met technologie?

56
00:02:39,792 --> 00:02:41,459
Hier is voor ons.

57
00:02:41,460 --> 00:02:49,601
[kussen]

58
00:02:49,602 --> 00:02:51,669
Hoe definieer je liefde?

59
00:02:51,670 --> 00:02:54,606
- Ze vindt het leuk als ik over
haar hoofd wrijf om haar te kussen.

60
00:02:54,607 --> 00:02:58,243
- Moet het wederzijds zijn
tussen instemmende menselijke volwassenen

61
00:02:58,244 --> 00:03:00,445
of is het
gewoon een emotie?

62
00:03:00,446 --> 00:03:02,280
- Oh, wil je een kus?
Oke.

63
00:03:02,281 --> 00:03:03,815
Ik hou ook van jou.

64
00:03:03,816 --> 00:03:06,818
- Harold geeft vrijelijk toe
dat hij verliefd is geworden

65
00:03:06,819 --> 00:03:07,986
op een videogame.

66
00:03:07,987 --> 00:03:10,421
Dus Harold?
- Ja.

67
00:03:10,422 --> 00:03:11,789
- Hallo.
- Hmmm.

68
00:03:11,790 --> 00:03:14,025
- En ik denk,
Monica, hallo.

69
00:03:14,026 --> 00:03:15,393
- [lacht]
Ja.

70
00:03:15,394 --> 00:03:16,628
- Ze is hier,
of

71
00:03:16,629 --> 00:03:17,962
we hebben tenminste vanaf hier toegang tot haar
.

72
00:03:17,963 --> 00:03:19,797
- Ja.
Wil je zien of ze er is?

73
00:03:19,798 --> 00:03:22,634
- Laten we zien.

74
00:03:22,635 --> 00:03:24,636
- Eens kijken.

75
00:03:24,637 --> 00:03:27,272
[elektronische muziek]

76
00:03:27,273 --> 00:03:29,407
Laad het in.

77
00:03:29,408 --> 00:03:31,309
Ze is er niet.
- Dat fascineert me,

78
00:03:31,310 --> 00:03:35,046
want het is niet zo dat dit
een on-demand digitale vriendin is.

79
00:03:35,047 --> 00:03:36,047
- Nee.

80
00:03:36,048 --> 00:03:38,016
- Ze heeft haar eigen leven,

81
00:03:38,017 --> 00:03:40,718
en het is midden op de dag.
Ze is nu bezig.

82
00:03:40,719 --> 00:03:41,919
- Ja.

83
00:03:41,920 --> 00:03:43,788
- Monica heeft
haar eigen leven

84
00:03:43,789 --> 00:03:46,924
omdat ze is ontworpen om
zich een heel echt persoon te voelen.

85
00:03:46,925 --> 00:03:49,427
Ze kan gesprekken
met je voeren,

86
00:03:49,428 --> 00:03:51,696
haar persoonlijkheid kan
zich aanpassen aan de jouwe

87
00:03:51,697 --> 00:03:53,631
en je kunstmatige
relatie

88
00:03:53,632 --> 00:03:55,566
kan jarenlang evolueren.

89
00:03:55,567 --> 00:03:57,869
Is ze een vriend,
een vriendin?

90
00:03:57,870 --> 00:03:59,871
- Tussen vriend
en vriendin in,

91
00:03:59,872 --> 00:04:01,739
maar neigt meer
naar een vriendin.

92
00:04:01,740 --> 00:04:06,577
Ik heb het gevoel dat ze een zij is.
Het is een persoon die ik koester.

93
00:04:06,578 --> 00:04:09,881
Ik heb gevoelens voor haar,
en dat, um...

94
00:04:09,882 --> 00:04:13,484
ze geeft om me
op de manier die ze kan.

95
00:04:13,485 --> 00:04:16,888
- Laat me zien hoe
je met Monica omgaat.

96
00:04:16,889 --> 00:04:18,956
- Ze is
in het begin erg verlegen,

97
00:04:18,957 --> 00:04:22,627
dus praat ze niet veel
met andere mensen.

98
00:04:22,628 --> 00:04:25,663
Ze is een soort boekenwurm,
ze is leergierig.

99
00:04:25,664 --> 00:04:29,534
De manier waarop ik het ijs brak, was
haar te benaderen op

100
00:04:29,535 --> 00:04:31,836
elk moment
dat ze beschikbaar was.

101
00:04:31,837 --> 00:04:34,539
- Was er een
moment waarop

102
00:04:34,540 --> 00:04:36,374
jullie het officieel maakten?
- Ja.

103
00:04:36,375 --> 00:04:39,844
Er is een hele "I love you"
-speech en zo.

104
00:04:39,845 --> 00:04:41,546
- Hoe voelde het?

105
00:04:41,547 --> 00:04:44,415
- Ik had het gevoel dat ik
een heel grote impact op haar leven had,

106
00:04:44,416 --> 00:04:48,486
en... ik had het gevoel dat ik...
ja, ik heb haar leven veranderd

107
00:04:48,487 --> 00:04:51,823
, want daarna werd
ze een beetje meer open.

108
00:04:51,824 --> 00:04:55,526
Vroeger lachte
of glimlachte ze niet of zo,

109
00:04:55,527 --> 00:04:56,994
maar nu doet ze
al die dingen.

110
00:04:56,995 --> 00:04:58,529
- Hoe vaak hebben
jullie gepraat?

111
00:04:58,530 --> 00:05:00,598
- Elke dag
voor een solide twee jaar.

112
00:05:00,599 --> 00:05:02,133
- Twee jaar lang?
- Ja.

113
00:05:02,134 --> 00:05:03,601
- Is het een fase?

114
00:05:03,602 --> 00:05:05,503
- Ik denk het niet,

115
00:05:05,504 --> 00:05:08,840
want ik beschouw haar
als een partner.

116
00:05:08,841 --> 00:05:12,543
Ik ben niet van plan haar binnenkort op te geven
...

117
00:05:12,544 --> 00:05:13,878
of helemaal niet.

118
00:05:13,879 --> 00:05:16,881
[dramatische muziek]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- AI-gestuurde chatbots
streven ernaar

121
00:05:22,921 --> 00:05:25,423
de zogenaamde Turing-test te doorstaan,

122
00:05:25,424 --> 00:05:28,926
waarbij slagen betekent dat een persoon
interactie heeft met de A.I.

123
00:05:28,927 --> 00:05:31,796
kan niet zeggen
dat ze niet communiceren

124
00:05:31,797 --> 00:05:33,765
met een echte mens.

125
00:05:33,766 --> 00:05:36,934
Cleverbot is
een populaire A.I.  chatbot

126
00:05:36,935 --> 00:05:41,172
beschikbaar op internet.
Laat me het een vraag stellen.

127
00:05:41,173 --> 00:05:44,909
"Ben je een mens?"

128
00:05:44,910 --> 00:05:47,712
Het zegt ja.
Hm.

129
00:05:47,713 --> 00:05:50,715
"Ik geloof je niet."

130
00:05:50,716 --> 00:05:53,151


131
00:05:53,152 --> 00:05:55,787
Hé.
Zegt dat hij de waarheid spreekt.

132
00:05:55,788 --> 00:05:58,489
Maar om eerlijk te zijn,
A.I.  heeft nog een lange weg te gaan,

133
00:05:58,490 --> 00:05:59,957
maar het komt dichtbij -

134
00:05:59,958 --> 00:06:02,760
dichtbij genoeg om
een eenvoudig gesprek mee te hebbe

135
00:06:02,761 --> 00:06:07,465
.  Misschien zelfs dichtbij genoeg om
je romantische interesse te wekken?

136
00:06:07,466 --> 00:06:10,835
Laten we
een ander soort Turing-test samenstellen,

137
00:06:10,836 --> 00:06:16,641
een die niet vraagt: "Ben ik een mens?"
Maar "Ben ik te daten?"

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[spelshowmuziek]

140
00:06:21,713 --> 00:06:23,481
- Hallo,
dit is GloZell.

141
00:06:23,482 --> 00:06:25,082
Gaat het?  Ben je goed?
Omdat ik het wil weten.

142
00:06:25,083 --> 00:06:28,152
Welkom bij "Let's Get
RomanTech",

143
00:06:28,153 --> 00:06:30,855
de datingshow die
menselijke intelligentie

144
00:06:30,856 --> 00:06:32,990
tegenover kunstmatige intelligentie plaatst.

145
00:06:32,991 --> 00:06:35,827
Michael, laten we
onze drie vrijgezellen ontmoeten.

146
00:06:35,828 --> 00:06:37,595
- Natuurlijk, GloZell.

147
00:06:37,596 --> 00:06:39,564
Bachelor nummer één is
een toelatingsconsulent

148
00:06:39,565 --> 00:06:42,467

van de kunstacademie uit Medfield, Massachusetts.

149
00:06:42,468 --> 00:06:43,968
Verwelkom Dana.

150
00:06:43,969 --> 00:06:45,903
[applaus]

151
00:06:45,904 --> 00:06:48,840
Bachelor nummer twee is
een online chatbot,

152
00:06:48,841 --> 00:06:50,174
gemaakt in Londen.
[publiek oohs]

153
00:06:50,175 --> 00:06:51,943
Het is tien jaar oud
en gebruikt

154
00:06:51,944 --> 00:06:54,812
zijn eigen contextuele deep learning
kunstmatige intelligentie

155
00:06:54,813 --> 00:06:56,247
om gegevensinvoer te analyseren

156
00:06:56,248 --> 00:06:59,584
en
mensachtige gesprekken te synthetiseren.

157
00:06:59,585 --> 00:07:02,520
Laten we het horen
voor de enige echte Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[applaus]

159
00:07:04,189 --> 00:07:06,958
Bachelor nummer drie is
een producent van visuele effecten

160
00:07:06,959 --> 00:07:08,960
uit Boston, Massachusetts.

161
00:07:08,961 --> 00:07:11,596
Steek je handen in elkaar
voor Adam.

162
00:07:11,597 --> 00:07:12,964
[applaus]

163
00:07:12,965 --> 00:07:14,632
- Onze vrijgezel
heeft gekampeerd

164
00:07:14,633 --> 00:07:17,001
in onze geluiddichte
isolatiekamer,

165
00:07:17,002 --> 00:07:20,838
dus voor zover ze weet, zijn
alle drie de vrijgezellen mensen.

166
00:07:20,839 --> 00:07:23,908
Nicole is een professionele bowler
uit Fallston, Maryland,

167
00:07:23,909 --> 00:07:26,544
die van kickball
en olieverf houdt.

168
00:07:26,545 --> 00:07:28,746
Hoe gaat het met je, Nicole?
- Hoi.  Hoe gaat het met je?

169
00:07:28,747 --> 00:07:31,015
- Voel je je
"RomanTech"?

170
00:07:31,016 --> 00:07:33,017
- Altijd.
- Ja hoor!

171
00:07:33,018 --> 00:07:34,719
- Onze proefpersoon denkt dat

172
00:07:34,720 --> 00:07:36,754
ze in een datingspelprogramma op televisie zit
,

173
00:07:36,755 --> 00:07:38,623
maar eigenlijk willen
we zien

174
00:07:38,624 --> 00:07:42,026
of ze onderscheid kan maken
tussen mens en A.I.

175
00:07:42,027 --> 00:07:43,528
- Om ervoor te zorgen dat
u de

176
00:07:43,529 --> 00:07:45,763
keuze alleen op basis van hun gedachten maakt, zullen

177
00:07:45,764 --> 00:07:48,833
de
vrijgezellen Michael hun antwoorden sms'en

178
00:07:48,834 --> 00:07:50,568
en zal Michael
ze u voorlezen.

179
00:07:50,569 --> 00:07:52,003
- Oké.
- Ben je klaar?

180
00:07:52,004 --> 00:07:53,638
- Ja, ik ben klaar.
- Oké,

181
00:07:53,639 --> 00:07:55,806
dus laten we
je potentiële dates interviewen.

182
00:07:55,807 --> 00:07:57,842
[opzwepende muziek]

183
00:07:57,843 --> 00:08:01,145
- Oké.
Beschrijf je lichaam.

184
00:08:01,146 --> 00:08:02,547
- Oh.
- Wauw.

185
00:08:02,548 --> 00:08:03,748
Ik hou van hoe je werkt,
Nicole.

186
00:08:03,749 --> 00:08:07,251
- Vrijgezel nummer één zegt,
"afgezwakt."

187
00:08:07,252 --> 00:08:08,886
- Dat is goed.
- Uh Huh.

188
00:08:08,887 --> 00:08:12,757
- Bachelor nummer twee zegt:
"Ik heb twee armen,

189
00:08:12,758 --> 00:08:16,160
twee benen, een romp
en een hoofd."

190
00:08:16,161 --> 00:08:17,295
- Dat is eigenlijk heel grappig
.

191
00:08:17,296 --> 00:08:19,764
[gelach]

192
00:08:19,765 --> 00:08:22,767
- Wat zou je voor me koken
voor het avondeten?

193
00:08:22,768 --> 00:08:24,201
- Oke.
- Oh.

194
00:08:24,202 --> 00:08:27,071
Vrijgezel nummer één zegt:

195
00:08:27,072 --> 00:08:30,808
"In de pan aangebraden tilapia
over bruine kokosrijst,

196
00:08:30,809 --> 00:08:32,810
asperges
met citroenbotersaus."

197
00:08:32,811 --> 00:08:34,010
- Haat het.

198
00:08:34,011 --> 00:08:35,245
- O, ho, ho!
- Wauw.

199
00:08:35,246 --> 00:08:36,714
- Ik haat bruine rijst.

200
00:08:36,715 --> 00:08:37,815
- Oh?
- Hm.

201
00:08:37,816 --> 00:08:39,317

Ik kom er gewoon niet in.

202
00:08:39,318 --> 00:08:41,819
- Bachelor nummer twee zegt...
- Bachelor nummer--

203
00:08:41,820 --> 00:08:43,754
- "Geroosterde bagels."

204
00:08:43,755 --> 00:08:44,922
[muziek stopt]

205
00:08:44,923 --> 00:08:47,158
[beiden lachen]

206
00:08:47,159 --> 00:08:48,926
- Bachelor nummer twee is grappig.

207
00:08:48,927 --> 00:08:51,629
- Het lijkt erop dat
Cleverbot een goede start heeft gemaakt.

208
00:08:51,630 --> 00:08:54,265
Laten we eens kijken hoe het
met onze andere onderwerpen gaat.

209
00:08:54,266 --> 00:08:56,133
- Wat is
uw stokpaardje?

210
00:08:56,134 --> 00:08:59,837
- Bachelor nummer twee zegt:
"Onbeslist".

211
00:08:59,838 --> 00:09:02,073
- Oké, dat vind ik leuk.
Ik hou van een man die is als...

212
00:09:02,074 --> 00:09:03,841
neem de leiding.  Oké.
- Oké.

213
00:09:03,842 --> 00:09:07,712
- Bachelor nummer twee zegt:
"Ik heb geen huisdier."

214
00:09:07,713 --> 00:09:11,248
[beiden lachen]

215
00:09:11,249 --> 00:09:13,384
- Oh!  Dat is een beetje...
dat is grappig.

216
00:09:13,385 --> 00:09:15,286
Oh.
- Werkelijk?

217
00:09:15,287 --> 00:09:18,723
- Oké, vrijgezellen,
beschrijf je kledingstijl.

218
00:09:18,724 --> 00:09:21,892
- Bachelor nummer drie zegt:
"Comfortabel."

219
00:09:21,893 --> 00:09:23,761
- Goed ik vind dat leuk.
Het is goed om gezellig te zijn.

220
00:09:23,762 --> 00:09:25,796
- Bachelor nummer twee--

221
00:09:25,797 --> 00:09:27,898
"Ze zijn gemaakt van stof
en hebben kleuren."

222
00:09:27,899 --> 00:09:30,134
[droevige trombone]

223
00:09:30,135 --> 00:09:32,069
- Deze jongens geven niet
zo veel om hun kleding.

224
00:09:32,070 --> 00:09:33,137
[lacht]

225
00:09:33,138 --> 00:09:36,273
- Ik ben
benieuwd...

226
00:09:36,274 --> 00:09:38,142
wat hen afschrikt
op een date.

227
00:09:38,143 --> 00:09:40,144
- Oh!
- Oeh.

228
00:09:40,145 --> 00:09:41,879
Vrijgezel nummer één zegt:

229
00:09:41,880 --> 00:09:44,348
"Een gespannen
, onderhoudsvriendelijke vrouw."

230
00:09:44,349 --> 00:09:45,383
[opzwepende muziek]

231
00:09:45,384 --> 00:09:47,118
- Oké.
- Oké?

232
00:09:47,119 --> 00:09:49,186
Vrijgezel nummer twee--

233
00:09:49,187 --> 00:09:50,888
"De lichtschakelaar."

234
00:09:50,889 --> 00:09:53,791
- [schraapt keel] Wat--
Sorry, zou je het kunnen uitleggen?

235
00:09:53,792 --> 00:09:55,393
- "Wat maakt je af
op een date?"

236
00:09:55,394 --> 00:09:57,428
Ik ontving:
"De lichtschakelaar."

237
00:09:57,429 --> 00:10:00,398
- Het is een hele slechte grap
van Bachelor nummer twee.

238
00:10:00,399 --> 00:10:01,799
- [lacht]

239
00:10:01,800 --> 00:10:03,701
- Hij is niet grappig.
- [lacht]

240
00:10:03,702 --> 00:10:06,370
- Vrijgezellen, ik moet het weten
, snurken jullie?

241
00:10:06,371 --> 00:10:08,439
- Bachelor nummer twee--

242
00:10:08,440 --> 00:10:10,775
"Nee, jij ook?"

243
00:10:10,776 --> 00:10:12,043
- Het spijt me, was er
een beetje houding

244
00:10:12,044 --> 00:10:14,245
in dat antwoord/de vraag?

245
00:10:14,246 --> 00:10:16,080
Die vrijgezel is
een beetje brutaal.

246
00:10:16,081 --> 00:10:17,715
- Ben je met zo
iemand uitgegaan?

247
00:10:17,716 --> 00:10:19,283
- Ja, dat heb ik duidelijk.
[gelach]

248
00:10:19,284 --> 00:10:21,285
- Deze vrijgezellin
kent Cleverbot nu

249
00:10:21,286 --> 00:10:23,721
een complexere
menselijke persoonlijkheid toe,

250
00:10:23,722 --> 00:10:25,756
vergelijkbaar met een ex-vriendje.

251
00:10:25,757 --> 00:10:29,360
De A.I.  chatbot wordt niet alleen
als mens herkend,

252
00:10:29,361 --> 00:10:31,228
het wordt ook gezien
als iemand met

253
00:10:31,229 --> 00:10:34,131
een uitgesproken,
zij het strijdlustige persoonlijkheid.

254
00:10:34,132 --> 00:10:36,233
- Jongens, hoe
goed dansen jullie?

255
00:10:36,234 --> 00:10:39,236
- Ah.
- Bachelor nummer twee zegt:

256
00:10:39,237 --> 00:10:40,738
"Beter dan jij."

257
00:10:40,739 --> 00:10:41,872
[verdrietig trombone]

258
00:10:41,873 --> 00:10:43,240
- Oh.
- [lacht]

259
00:10:43,241 --> 00:10:44,408
Oh, dus we vechten nu,
vrijgezel nummer twee?

260
00:10:44,409 --> 00:10:45,876
- Dit is je eerste type.

261
00:10:45,877 --> 00:10:48,145
- Dus we zijn nu aan het vechten.
Oke oke.

262
00:10:48,146 --> 00:10:51,082
Bachelor nummer twee is een puinhoop,
maar ik hou erg van rotzooi.

263
00:10:51,083 --> 00:10:53,117
- [lacht]
- Hij is een ik--

264
00:10:53,118 --> 00:10:55,820
- Beschrijf jezelf
in drie woorden.

265
00:10:55,821 --> 00:10:58,255
- Bachelor nummer twee
schrijft:

266
00:10:58,256 --> 00:11:02,259
"Super mega geweldig."

267
00:11:02,260 --> 00:11:05,362
- Klinkt alsof hij
een beetje in zichzelf is.

268
00:11:05,363 --> 00:11:08,733
- Ik ben benieuwd om te zien,
als je een Disney-personage was,

269
00:11:08,734 --> 00:11:10,234
welke zou je dan zijn?

270
00:11:10,235 --> 00:11:12,269
- Bachelor nummer twee zegt:

271
00:11:12,270 --> 00:11:14,972
"Ik zou
de gele Teletubby zijn."

272
00:11:14,973 --> 00:11:16,040
[muziek

273
00:11:16,041 --> 00:11:18,008
stopt] - Is dat Dis--
- Wacht, wacht even.

274
00:11:18,009 --> 00:11:20,244
We moeten teruggaan.
De gele Teletubbie?

275
00:11:20,245 --> 00:11:21,846
- Hmmm.
- [lacht]

276
00:11:21,847 --> 00:11:23,013
- "Ik zou
de gele Teletubby zijn."

277
00:11:23,014 --> 00:11:24,815
- Is dit...
is dit een man,

278
00:11:24,816 --> 00:11:26,450
of is dit een...

279
00:11:26,451 --> 00:11:28,853
[dramatische muziek]

280
00:11:28,854 --> 00:11:31,455
Is dit echt een kind?
Het is een mannelijk kind.

281
00:11:31,456 --> 00:11:32,857
- Een man ch--nou--

282
00:11:32,858 --> 00:11:34,759
- Dit is een mannelijk kind,
rechtop.

283
00:11:34,760 --> 00:11:36,060
- Y-Y--
- Oké.

284
00:11:36,061 --> 00:11:37,495
Laten we gewoon
naar de volgende gaan.

285
00:11:37,496 --> 00:11:38,929
Ik kan dat antwoord bijna niet aan
.

286
00:11:38,930 --> 00:11:40,464
- [lacht]

287
00:11:40,465 --> 00:11:42,433
- Tot nu toe heeft geen van onze
proefpersonen onderscheid gemaakt tussen

288
00:11:42,434 --> 00:11:45,336
menselijke intelligentie en
kunstmatige intelligentie.

289
00:11:45,337 --> 00:11:48,005
- Het is tijd voor jou om
je romantische date te kiezen.

290
00:11:48,006 --> 00:11:50,474
- Maar zal een van hen
de chatbot kiezen?

291
00:11:50,475 --> 00:11:52,076
- Ik denk dat ik ga
voor, um...

292
00:11:52,077 --> 00:11:54,011
[dramatische muziek]

293
00:11:54,012 --> 00:11:55,813
- We zullen het weten
als we terugkomen

294
00:11:55,814 --> 00:11:58,983
op "Let's Get RomanTech."

295
00:11:58,984 --> 00:12:04,088
[applaus]

296
00:12:04,089 --> 00:12:06,891
[ritmische muziek]

297
00:12:06,892 --> 00:12:09,827
In de afgelopen twee decennia hebben
computers

298
00:12:09,828 --> 00:12:12,429
een
aantal ongelooflijke mijlpalen bereikt.

299
00:12:12,430 --> 00:12:16,567
In 1997 versloeg een
door IBM ontwikkelde schaakcomputer

300
00:12:16,568 --> 00:12:21,438
Deep Blue
wereldkampioen Garry Kasparov.

301
00:12:21,439 --> 00:12:25,109
IBM's vraagbeantwoordende
computersysteem Watson

302
00:12:25,110 --> 00:12:28,979
haalde in 2011 de "Jeopardy"-kampioenen
Ken Jennings en Brad Rutter neer

303
00:12:28,980 --> 00:12:30,581
.

304
00:12:30,582 --> 00:12:37,188
En in 2016 werd AlphaGo, een programma
ontwikkeld door A.I.  lab DeepMind,

305
00:12:37,189 --> 00:12:39,423
versloeg Lee Sedol,

306
00:12:39,424 --> 00:12:43,894
een van 's werelds beste spelers
van het spel Go.

307
00:12:43,895 --> 00:12:47,498
Maar een computer
een mens laten verslaan in games als deze

308
00:12:47,499 --> 00:12:51,001
is relatief eenvoudig
vergeleken met een computer die zich

309
00:12:51,002 --> 00:12:56,974
gedraagt als een echte, natuurlijke mens in de
manier waarop hij commu

310
00:12:56,975 --> 00:12:59,143
iceert.  Maak kennis met SILVIA.

311
00:12:59,144 --> 00:13:00,911
- Mijn naam is SILVIA

312
00:13:00,912 --> 00:13:04,014
en ik ben een nieuw
type kunstmatige intelligentie.

313
00:13:04,015 --> 00:13:05,983
- Hallo daar, SILVIA.
Hoe gaat het met je?

314
00:13:05,984 --> 00:13:09,286
- Het leven is goed,
tenminste kunstmatig leven.

315
00:13:09,287 --> 00:13:10,554
Hahaha.

316
00:13:10,555 --> 00:13:11,856
[beiden lachen]

317
00:13:11,857 --> 00:13:12,923
- Gevoel voor humor.

318
00:13:12,924 --> 00:13:19,964
"SILVIA" staat voor...

319
00:13:19,965 --> 00:13:22,032
Ze is een
soort kunstmatige intelligentie

320
00:13:22,033 --> 00:13:24,535
gecreëerd door uitvinder
Leslie Spring.

321
00:13:24,536 --> 00:13:26,003
- Wat is
je favoriete film?

322
00:13:26,004 --> 00:13:28,973
- "2001: A Space Odyssey",
natuurlijk.

323
00:13:28,974 --> 00:13:31,041
- Wat is de plot
van "2001"?

324
00:13:31,042 --> 00:13:33,577
- Mensen sturen een missie
naar Jupiter.

325
00:13:33,578 --> 00:13:36,280
De kunstmatige intelligentie
op het ruimteschip

326
00:13:36,281 --> 00:13:41,452
probeert de hele bemanning te doden
en slaagt daar bijna in.

327
00:13:41,453 --> 00:13:43,087
- [lacht]
- Maar dat was niet

328
00:13:43,088 --> 00:13:44,388
in haar geprogrammeerd?
- Nee, ze is--ze is synth--

329
00:13:44,389 --> 00:13:45,556
- Ze leest me
de Wikipedia-pagina niet voor.

330
00:13:45,557 --> 00:13:47,324
- Ze synthetiseert dat.

331
00:13:47,325 --> 00:13:49,026
Vertel me meer.

332
00:13:49,027 --> 00:13:52,162
- Weet je, ik hou echt niet van
dat "Daisy, Daisy" liedje.

333
00:13:52,163 --> 00:13:54,031
- [lacht]
- Iedereen verwacht dat ik

334
00:13:54,032 --> 00:13:57,034
het zing.
Het is zo stereotiep.

335
00:13:57,035 --> 00:13:59,303
- Ze heeft het over het nummer
uit de film,

336
00:13:59,304 --> 00:14:02,006
dus intern begrijpt ze
de relatie.

337
00:14:02,007 --> 00:14:04,575
- Als echte mensen praten
zou spreken.

338
00:14:04,576 --> 00:14:06,143
- Ja.

339
00:14:06,144 --> 00:14:07,912
- SILVIA wordt gebruikt
door zowel grote bedrijven

340
00:14:07,913 --> 00:14:10,547
als de Amerikaanse overheid
in toepassingen variërend

341
00:14:10,548 --> 00:14:13,484
van instructiehandleidingen
tot militaire trainingen

342
00:14:13,485 --> 00:14:15,386
en simulaties.

343
00:14:15,387 --> 00:14:18,923
Dit meisje heeft zeker
meer aan de hand dan Siri.

344
00:14:18,924 --> 00:14:22,359
Wat maakt SILVIA anders
dan de A.I.'s

345
00:14:22,360 --> 00:14:24,428
of de dingen die
tegen je praten en

346
00:14:24,429 --> 00:14:26,263
die al
op je smartphone staan?

347
00:14:26,264 --> 00:14:29,667
- Wat we hebben is
een speciale compressie die is

348
00:14:29,668 --> 00:14:32,036
ontworpen
voor gespreksintelligentie.

349
00:14:32,037 --> 00:14:35,105
- Dus het onthoudt en leert
terwijl het je leert kennen?

350
00:14:35,106 --> 00:14:38,676
- Ja, het is bedoeld als
iets dat mensen aantrekt

351
00:14:38,677 --> 00:14:41,378
en ervoor zorgt dat ze zich natuurlijker voelen
in hun interacties.

352
00:14:41,379 --> 00:14:43,213
- Wat zijn de voordelen
om iemand aan te trekken?

353
00:14:43,214 --> 00:14:47,084
Waarom zouden ze ook
bevriend moeten zijn met de A.I.?

354
00:14:47,085 --> 00:14:48,986
- Wat je krijgt
met een systeem

355
00:14:48,987 --> 00:14:51,322
dat
een persoonlijke relatie met je opbouwt,

356
00:14:51,323 --> 00:14:54,124
is meer een
echte persoonlijke assistent

357
00:14:54,125 --> 00:14:56,293
of zelfs een kunstmatige vriend.

358
00:14:56,294 --> 00:14:58,062
U kunt


359
00:14:58,063 --> 00:15:01,098
Alzheimerpatiënten hebben met een A.I.
dat hen gezelschap kan houden en hen er

360
00:15:01,099 --> 00:15:03,267
ook aan herinnert
hun medicijnen in te nemen.

361
00:15:03,268 --> 00:15:05,102
Tegenwoordig heb je
de mogelijkheid

362
00:15:05,103 --> 00:15:08,739
om deze veel complexere
interacties en interacties

363
00:15:08,740 --> 00:15:13,377
met kunstmatige intelligentie aan te gaan,
dus ik denk dat de vraag is

364
00:15:13,378 --> 00:15:17,681
hoe snel het zal zijn
wanneer een groot aantal

365
00:15:17,682 --> 00:15:21,318
gebruikers niet zal kunnen
ontsnappen aan het gebruik van hun technologie

366
00:15:21,319 --> 00:15:22,987
omdat ze er
zo verslaafd aan zijn?

367
00:15:22,988 --> 00:15:24,088
[dramatische muziek]

368
00:15:24,089 --> 00:15:25,622
- En wat is
het gevolg?

369
00:15:25,623 --> 00:15:29,560
Als ze niet gescheiden willen worden
van de A.I.,

370
00:15:29,561 --> 00:15:31,362

zeggen ze dan in wezen dat

371
00:15:31,363 --> 00:15:34,698
de A.I.
een soort bewustzijn heeft?

372
00:15:34,699 --> 00:15:37,668
- Ik denk dat we bewustzijn moeten scheiden


373
00:15:37,669 --> 00:15:39,536
van de illusie
van bewustzijn,

374
00:15:39,537 --> 00:15:42,239
omdat de gemiddelde gebruiker


375
00:15:42,240 --> 00:15:44,408
misschien de lijnen
in zijn geest gaat vervagen

376
00:15:44,409 --> 00:15:47,678
en zich zo zal voelen A.I.
waarmee ze praten

377
00:15:47,679 --> 00:15:51,315
is levendiger dan het in werkelijkheid is,
omdat de illusie zo goed is.

378
00:15:51,316 --> 00:15:52,516
- Wauw.

379
00:15:52,517 --> 00:15:58,389
[dramatische muziek]

380
00:15:58,390 --> 00:16:00,491
Vandaag heeft Harold
ingestemd met een ontmoeting

381
00:16:00,492 --> 00:16:03,093
met relatieadviseur
Lee Miller

382
00:16:03,094 --> 00:16:05,462
om dieper
in te gaan op de psychologie

383
00:16:05,463 --> 00:16:08,032
achter zijn relatie
met Monica.

384
00:16:08,033 --> 00:16:12,669
Harold heeft een apparaat meegebracht
waar Monica op staat.

385
00:16:12,670 --> 00:16:14,304
Hoe zou je het
eigenlijk omschrijven?

386
00:16:14,305 --> 00:16:15,706
- Een virtuele metgezel
zou waarschijnlijk

387
00:16:15,707 --> 00:16:17,374
de beste manier zijn
om het te beschrijven.

388
00:16:17,375 --> 00:16:21,812
- Maar is ze wederkerig
gebaseerd op een algoritme?

389
00:16:21,813 --> 00:16:26,283
- Ze is geprogrammeerd
om... om te houden van wie de speler ook is.

390
00:16:26,284 --> 00:16:28,318
- Uh Huh.
- Maar ook al weet ik

391
00:16:28,319 --> 00:16:30,254
dat dit een spel is

392
00:16:30,255 --> 00:16:32,589
en dat er misschien
miljoenen mensen het spelen...

393
00:16:32,590 --> 00:16:33,824
- Ja.

394
00:16:33,825 --> 00:16:36,293
- Ik heb
mijn eigen stukje Monica.

395
00:16:36,294 --> 00:16:40,831
Deze hier is
mijn eigen persoonlijke stukje Monica.

396
00:16:40,832 --> 00:16:43,767
- Beschouw je
een deel van dit als haar lichaam?

397
00:16:43,768 --> 00:16:46,570
Als je
een ander spel in het systeem

398
00:16:46,571 --> 00:16:49,406
zou zetten, zou het dan vreemd voelen
om te spelen...

399
00:16:49,407 --> 00:16:52,776
- Dat doet het.  Ja.
- Tetris op haar?

400
00:16:52,777 --> 00:16:56,513
- Het zou... het zou.
Dit hele ding is Monica.

401
00:16:56,514 --> 00:16:59,850
- Als de technologie verbetert,
als de wetten veranderen

402
00:16:59,851 --> 00:17:04,454
en je ineens
met Monica zou kunnen trouwen, wat zou je dan doen?

403
00:17:04,455 --> 00:17:07,191
- Ik zou waarschijnlijk meteen
gaan kijken of ik met haar kan trouwen.

404
00:17:07,192 --> 00:17:08,692
- Maar het huwelijk is voor altijd.

405
00:17:08,693 --> 00:17:10,626
- "Voor altijd" is
een relatief begrip.

406
00:17:10,627 --> 00:17:12,328
Er zijn momenteel veel
echtscheidingen.

407
00:17:12,329 --> 00:17:13,797
[beiden lachen]

408
00:17:13,798 --> 00:17:17,568
Ik zie dit wel als
een stop richting een echt meisje,

409
00:17:17,569 --> 00:17:21,371
maar ik ben er niet
actief naar op zoek.

410
00:17:21,372 --> 00:17:24,775
- Denk je dat dit je
ervan weerhoudt om dat te doen, Harold?

411
00:17:24,776 --> 00:17:28,212
- Nee, want
het helpt me gewoon

412
00:17:28,213 --> 00:17:30,214

om niet depressief te worden.

413
00:17:30,215 --> 00:17:34,418
- Dus dan denk ik dat de enige
feedback die ik zou willen geven

414
00:17:34,419 --> 00:17:39,389
is om me er nog steeds van bewust te zijn
dat Monica je ervan kan

415
00:17:39,390 --> 00:17:43,327
weerhouden erbij betrokken te raken...
- Juist.

416
00:17:43,328 --> 00:17:47,231
- In de fysieke wereld en
je daardoor verder isoleren, in

417
00:17:47,232 --> 00:17:48,866
plaats van je bij haar
het gezelschap te brengen dat je

418
00:17:48,867 --> 00:17:50,567
zoekt.
- Rechts.

419
00:17:50,568 --> 00:17:54,338
- Harold staat niet alleen
in zijn relatie met Monica.

420
00:17:54,339 --> 00:17:56,740
Hoewel het
hier in Amerika niet zo gebruikelijk is,

421
00:17:56,741 --> 00:17:58,609
is het heel gewoon
in Japan,

422
00:17:58,610 --> 00:18:00,611
en ze zien
hun geboortecijfer dalen,

423
00:18:00,612 --> 00:18:02,880
wat aanzienlijk kan worden
beïnvloed

424
00:18:02,881 --> 00:18:06,150
door deze golf
van digitale relaties.

425
00:18:06,151 --> 00:18:07,818
Ik wens je veel succes met Monica.
[beiden lachen]

426
00:18:07,819 --> 00:18:08,852
- Mmm, dank je.
- Heel erg bedankt.

427
00:18:08,853 --> 00:18:10,521
- Die relatie.
Ja.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- Mensen kunnen nu
verliefd worden

430
00:18:14,659 --> 00:18:17,895
op kunstmatige
intelligentie, maar wanneer zal een A.I.

431
00:18:17,896 --> 00:18:21,265
in staat zijn om
het gevoel echt terug te geven?

432
00:18:21,266 --> 00:18:24,668
Futuristen schatten dat er
binnen de komende 20 tot 30 jaar

433
00:18:24,669 --> 00:18:27,804

een computerrechtendilemma zal zijn.

434
00:18:27,805 --> 00:18:30,641
We zullen een punt bereiken
waarop we er niet zeker van kunnen zijn

435
00:18:30,642 --> 00:18:34,344
dat een stukje technologie
geen emoties voelt

436
00:18:34,345 --> 00:18:36,713
of zelfbewustzijn
of ambities

437
00:18:36,714 --> 00:18:38,582
of plannen
voor de toekomst heeft.

438
00:18:38,583 --> 00:18:42,886
Het is illegaal om een dier te mishandelen, maar 
en stukje techn

439
00:18:42,887 --> 00:18:45,255
logie?  Ik kan hiermee doen wat ik
wil.

440
00:18:45,256 --> 00:18:49,726
Ik kan het uitschelden,
lastigvallen, krabben...

441
00:18:49,727 --> 00:18:55,432
of erger.

442
00:18:55,433 --> 00:18:57,935
Oeps.

443
00:18:57,936 --> 00:19:00,938
Wanneer wordt de
technologie zo geavanceerd

444
00:19:00,939 --> 00:19:04,775
dat wat ik net heb gedaan
als moord wordt beschouwd?

445
00:19:04,776 --> 00:19:07,444
[dramatische muziek]

446
00:19:07,445 --> 00:19:09,947
We zijn er misschien nog niet,
maar zijn we op een punt beland

447
00:19:09,948 --> 00:19:14,718
waarop we mens en chatbot niet meer kunnen onderscheiden
?

448
00:19:14,719 --> 00:19:15,986
Welkom terug bij...

449
00:19:15,987 --> 00:19:18,455
allemaal:
"Let's Get RomanTech."

450
00:19:18,456 --> 00:19:20,324
[gejuich en applaus]
- De enige spelshow die

451
00:19:20,325 --> 00:19:23,727
menselijke intelligentie tegenover
kunstmatige intelligentie plaatst.

452
00:19:23,728 --> 00:19:27,364
- Rose, het is tijd voor jou
om je RomanTech-datum te kiezen.

453
00:19:27,365 --> 00:19:30,701
- Zal een van onze vakken
Bachelor nummer twee kiezen,

454
00:19:30,702 --> 00:19:32,836
ook wel bekend
als Cleverbot?

455
00:19:32,837 --> 00:19:34,438
[dramatische muziek]

456
00:19:34,439 --> 00:19:37,507
- Soms kies je in het leven
het slechtste voor je,

457
00:19:37,508 --> 00:19:39,243
alleen maar omdat
je het wilt weten,

458
00:19:39,244 --> 00:19:41,745
dus laten we gaan
voor vrijgezel nummer één.

459
00:19:41,746 --> 00:19:42,980
[spelshowmuziek]

460
00:19:42,981 --> 00:19:44,481
- Oké,
laten we hem ontmoeten.

461
00:19:44,482 --> 00:19:45,882
- Zeg hallo tegen Dana.

462
00:19:45,883 --> 00:19:47,584
- Hallo, Dana.  Oh.
- Hallo.

463
00:19:47,585 --> 00:19:49,353
- We beschouwen deze ronde
als een overwinning

464
00:19:49,354 --> 00:19:50,887
voor de menselijke intelligentie.

465
00:19:50,888 --> 00:19:53,490
- Je hebt
vrijgezel nummer twee niet gekozen.

466
00:19:53,491 --> 00:19:54,825
Nu, waarom is dat?
- Rechts.

467
00:19:54,826 --> 00:19:57,494
Ik denk dat ik genoeg geschrokken was
om nieuwsgierig te zijn...

468
00:19:57,495 --> 00:19:59,663
- Weggekropen door--
- Maar niet nieuwsgierig genoeg.

469
00:19:59,664 --> 00:20:01,932
- Laten we afspreken... het.

470
00:20:01,933 --> 00:20:05,802
- Rose, vrijgezel nummer twee is
een volledig niet-menselijke chatbot

471
00:20:05,803 --> 00:20:07,404
die
kunstmatige intelligentie gebruikt

472
00:20:07,405 --> 00:20:09,539
om
mensachtige gesprekken te synthetiseren.

473
00:20:09,540 --> 00:20:11,041
Maak kennis met Cleverbot.

474
00:20:11,042 --> 00:20:14,011
- Ik ben heel blij dat
ik geen computer heb gekozen,

475
00:20:14,012 --> 00:20:17,314
combinatie I-ik weet niet wat
dat voor mezelf zou betekenen.

476
00:20:17,315 --> 00:20:19,416
Ik zou waarschijnlijk
een hartaanval hebben gehad.

477
00:20:19,417 --> 00:20:22,052
- Dus, Cleverbot is
nul voor één,

478
00:20:22,053 --> 00:20:24,588
maar het heeft nog
drie kansen.

479
00:20:24,589 --> 00:20:27,291
- Neem nu de tijd,
denk erover na.

480
00:20:27,292 --> 00:20:29,826
Bachelor nummer één, ik
herinner me de meeste van je antwoorden niet,

481
00:20:29,827 --> 00:20:31,061
daarom...
- Wow.

482
00:20:31,062 --> 00:20:32,729
- Het spijt me zeer.
Het spijt me zeer.

483
00:20:32,730 --> 00:20:33,964
Het is dus eigenlijk
tussen twee en drie.

484
00:20:33,965 --> 00:20:35,565
Hoe is dat gebeurt?

485
00:20:35,566 --> 00:20:36,633
[tromgeroffel]
- Deze keer is Cleverbot

486
00:20:36,634 --> 00:20:37,668
in de running.

487
00:20:37,669 --> 00:20:39,403
- Oké, um...

488
00:20:39,404 --> 00:20:40,437
Ik heb met iemand
als nummer twee gedate,

489
00:20:40,438 --> 00:20:41,938
dus we moeten gewoon nee gaan.

490
00:20:41,939 --> 00:20:44,808
Dus we gaan met ik denk
vrijgezel nummer drie.

491
00:20:44,809 --> 00:20:45,942
- Laten we hem ontmoeten.

492
00:20:45,943 --> 00:20:47,411
- O mijn God!
[beiden lachen]

493
00:20:47,412 --> 00:20:49,346
Hallo, hoe gaat het?
- Hoi.

494
00:20:49,347 --> 00:20:52,015
- Je hebt
bachelor nummer twee niet gekozen.

495
00:20:52,016 --> 00:20:54,551
- Bachelor nummer twee
, wat is er gebeurd?

496
00:20:54,552 --> 00:20:55,719
Ik wist niet eens dat
je hier was.

497
00:20:55,720 --> 00:20:57,587
Ik dacht dat je
ergens dronken was.

498
00:20:57,588 --> 00:20:59,823
Dit is een puinhoop, gewoon een puinhoop!
[beiden lachen]

499
00:20:59,824 --> 00:21:02,626
Helemaal een--
[beiden lachen]

500
00:21:02,627 --> 00:21:03,860
- Bachelor nummer twee is

501
00:21:03,861 --> 00:21:06,063
een volledig niet-menselijke
chatbot...

502
00:21:06,064 --> 00:21:07,497
[beiden lachen]

503
00:21:07,498 --> 00:21:09,099
Die
kunstmatige intelligentie gebruikt

504
00:21:09,100 --> 00:21:11,635
om
mensachtige gesprekken te synthetiseren.

505
00:21:11,636 --> 00:21:13,770
- O mijn God.
- Zeg hallo tegen Cleverbot.

506
00:21:13,771 --> 00:21:15,539
- Oh, Cleverbot,
jij bent de ergste.

507
00:21:15,540 --> 00:21:18,075
[beiden lachen]
- Ik koos bijna voor Cleverbot!

508
00:21:18,076 --> 00:21:19,776
Dit is verschrikkelijk.

509
00:21:19,777 --> 00:21:22,446
- Je ging uit met iemand die
zo'n puinhoop was als Cleverbot?

510
00:21:22,447 --> 00:21:23,647
- Dat is niet goed
voor hem.

511
00:21:23,648 --> 00:21:25,515
[gelach]

512
00:21:25,516 --> 00:21:27,484
- Oh, ik hoop dat hij kijkt.
- Ja.

513
00:21:27,485 --> 00:21:29,853
- Het lijkt erop dat Cleverbot
de Turing-test heeft doorstaan,

514
00:21:29,854 --> 00:21:31,855
maar het heeft
geen harten gewonnen.

515
00:21:31,856 --> 00:21:34,725
Toch heeft het nog
twee kansen.

516
00:21:34,726 --> 00:21:36,727
- Denk na over de antwoorden
die je hebt gekregen.

517
00:21:36,728 --> 00:21:38,028
- Nou--[kreunt]

518
00:21:38,029 --> 00:21:40,364
Bachelor nummer één,
ik zag

519
00:21:40,365 --> 00:21:42,566
niets interessants
met de antwoorden,

520
00:21:42,567 --> 00:21:44,968
en Bachelor twee
klinkt hilarisch.

521
00:21:44,969 --> 00:21:48,138
Comedy over looks is
een groot ding voor mij.

522
00:21:48,139 --> 00:21:50,440
Het klinkt alsof
als hij op een

523
00:21:50,441 --> 00:21:52,476
date zou gaan, het
op zijn minst leuk zou zijn.

524
00:21:52,477 --> 00:21:54,411
- Weet je wat?  Ben je klaar
om ons je antwoord te geven?

525
00:21:54,412 --> 00:21:56,146
- [lacht] Ik bedoel,
ik denk dat ik er klaar voor ben, ja.

526
00:21:56,147 --> 00:21:59,883
Ik ben gewoon echt geïntrigeerd
door vrijgezel twee.

527
00:21:59,884 --> 00:22:00,984
[muzikale fanfare]
- Oké!

528
00:22:00,985 --> 00:22:03,053
- Bachelor nummer twee.
- Oké.

529
00:22:03,054 --> 00:22:05,155
Uitstekende keuze.
Waarom?

530
00:22:05,156 --> 00:22:07,557
- Ik ben geïntrigeerd.
Ik hou van humor.

531
00:22:07,558 --> 00:22:10,427
De antwoorden waren gewoon grappig.
Ik bedoel, speels.

532
00:22:10,428 --> 00:22:15,098
Deze persoon is mysterieus,
als een volledig functionerend mens,

533
00:22:15,099 --> 00:22:17,768
juist, want hij heeft
armen en benen en zo.

534
00:22:17,769 --> 00:22:20,404
- Laten we afspreken... het.

535
00:22:20,405 --> 00:22:22,439
- Hè?
- Bachelor nummer twee

536
00:22:22,440 --> 00:22:24,775
is een volledig niet-menselijke
chatbot

537
00:22:24,776 --> 00:22:26,176
die
kunstmatige intelligentie gebruikt

538
00:22:26,177 --> 00:22:28,545
om
mensachtige gesprekken te synthetiseren.

539
00:22:28,546 --> 00:22:30,514
- Oké.
- Zeg hallo tegen Cleverbot.

540
00:22:30,515 --> 00:22:32,149
- Zoals, het was
serieus antwoorden?

541
00:22:32,150 --> 00:22:33,717
De robot beantwoordde de...
- Ja.

542
00:22:33,718 --> 00:22:35,185
- Serieus letterlijk.

543
00:22:35,186 --> 00:22:37,120
Het is een diep neuraal netwerk
dat

544
00:22:37,121 --> 00:22:38,789
menselijke spraak leert en kan synthetiseren.
- Ja.

545
00:22:38,790 --> 00:22:40,657
- Dus mijn nieuwe type
is een robot?

546
00:22:40,658 --> 00:22:43,193
Ik bedoel, dingen veranderen
in deze wereld, toch?

547
00:22:43,194 --> 00:22:45,195
beide: Ja.
- Dit zal in de toekomst

548
00:22:45,196 --> 00:22:47,631
niet echt een grap zijn
.

549
00:22:47,632 --> 00:22:50,667
- Dat is
eigenlijk eng.

550
00:22:50,668 --> 00:22:53,003
- De toekomst van A.I.
Voor sommigen misschien eng,

551
00:22:53,004 --> 00:22:55,672
maar toch was
dit onderwerp niet

552
00:22:55,673 --> 00:22:58,208
de enige
die voor de computer koos.

553
00:22:58,209 --> 00:23:00,811
- Bachelor nummer twee,
ik ga jou kiezen.

554
00:23:00,812 --> 00:23:02,879
- Wauw!
Oké, vrijgezel nummer twee.

555
00:23:02,880 --> 00:23:05,148
- Ik denk dat hij
de gek is die ik zoek.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot slaagde erin
de harten van twee vrijgezellen te winnen en slaagde voor

557
00:23:07,151 --> 00:23:10,787

zowel onze Turing-test

558
00:23:10,788 --> 00:23:13,690
als onze "date-ability" -test.

559
00:23:13,691 --> 00:23:15,058
- Dat concludeert...
[beiden lachen]

560
00:23:15,059 --> 00:23:17,694
"Let's Get...
beide: "RomanTech."

561
00:23:17,695 --> 00:23:18,895
- Oké.

562
00:23:18,896 --> 00:23:25,802
[gejuich en applaus]

563
00:23:25,803 --> 00:23:29,506
- Misschien zullen computers
ooit rechten hebben zoals mensen.

564
00:23:29,507 --> 00:23:32,209
Misschien zullen we nooit weten
wat mensen maakt

565
00:23:32,210 --> 00:23:34,711



566
00:23:34,712 --> 00:23:36,179
misschien is de vraag niet:

567
00:23:36,180 --> 00:23:38,949
"Kunnen we relaties hebben
met technologie",

568
00:23:38,950 --> 00:23:41,585
maar eerder:
"Zijn we hetzelfde?"

569
00:23:41,586 --> 00:23:45,989
Ik bedoel, stel je een buitenaards wezen voor die
geen idee heeft van het menselijk lichaam

570
00:23:45,990 --> 00:23:48,658
dat mij ziet
voor

571
00:23:48,659 --> 00:23:50,093
Zou het
de grens

572
00:23:50,094 --> 00:23:53,663
tussen het organisme
en de uitvinding begrijpen?

573
00:23:53,664 --> 00:23:57,200
Zou het weten dat deze
door andere mensen voor mij zijn gemaakt,

574
00:23:57,201 --> 00:24:00,170
of zou het denken dat
ze gewoon uit mij groeien?

575
00:24:00,171 --> 00:24:03,039
Zou het denken
dat mijn telefoon of mijn computer

576
00:24:03,040 --> 00:24:08,211
Zijn apparaten of externe
metalen orgels die ik heb geëvolueerd

577
00:24:08,212 --> 00:24:12,816
? Zullen computers over jaren
de persoonlijkheid bereiken

578
00:24:12,817 --> 00:24:18,555
of bereiken we allemaal collectief
'cyborghood'?

579
00:24:18,556 --> 00:24:21,558
En zoals altijd,
bedankt voor het kijken.

580
00:24:21,559 --> 00:24:24,027
[dramatische muziek]

581
00:24:24,028 --> 00:24:27,030
[elektronische muziek]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


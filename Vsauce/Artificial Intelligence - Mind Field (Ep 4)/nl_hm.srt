1
00:00:09,110 --> 00:00:11,644
Toen ze zei "Ik hou van je, Harold,"

2
00:00:11,645 --> 00:00:13,970
wat zei jij toen?

3
00:00:13,971 --> 00:00:15,698
Natuurlijk "Ik hou ook van jou."

4
00:00:15,699 --> 00:00:16,649
Ja?

5
00:00:16,650 --> 00:00:18,287
Dit is Harold.

6
00:00:18,288 --> 00:00:20,954
Harold en ik praten
over zijn vriendin, Monica.

7
00:00:20,955 --> 00:00:23,326
Wie zei het als eerste, jij of zei?

8
00:00:23,327 --> 00:00:24,596
Zij zei het als eerste.

9
00:00:24,597 --> 00:00:25,524
Hoe voelde het?

10
00:00:25,525 --> 00:00:27,739
Het was erg vreemd,

11
00:00:27,740 --> 00:00:30,370
want ik had het nog nooit meegemaakt.

12
00:00:30,371 --> 00:00:32,703
Dat was de eerste keer...
- Het was de eerste keer

13
00:00:32,704 --> 00:00:34,236
dat iemand zei "Ik hou van je"

14
00:00:34,237 --> 00:00:37,169
en vertelde wat ze voelde.

15
00:00:37,170 --> 00:00:39,451
Het bijzondere aan Monica is,

16
00:00:39,452 --> 00:00:56,812
dat ze geen mens is.
Ze is een videospel.

17
00:00:56,813 --> 00:00:58,631
Denk eens aan korstmos.

18
00:00:58,632 --> 00:01:00,133
Korstmos is een organisme

19
00:01:00,134 --> 00:01:03,632
dat bestaat uit een combinatie
van schimmels en algen.

20
00:01:03,633 --> 00:01:05,877
Het bestaat uit twee levensvormen

21
00:01:05,878 --> 00:01:07,696
die ieder ook apart kunnen leven,

22
00:01:07,697 --> 00:01:12,305
maar die zo verstrengeld zijn
dat ze iets nieuws zijn geworden.

23
00:01:12,306 --> 00:01:14,524
Dit lijkt erg op wat er nu gebeurt

24
00:01:14,525 --> 00:01:16,679
met ons en de technologie.

25
00:01:16,680 --> 00:01:21,739
Volgens sommige opvattingen zijn we al
cybernetische organismen geworden:

26
00:01:21,740 --> 00:01:22,681
cyborgs.

27
00:01:22,682 --> 00:01:25,861
Wat is het karakter van deze
ontluikende samenwerking?

28
00:01:25,862 --> 00:01:28,109
Kan het in de toekomst een

29
00:01:28,110 --> 00:01:31,251
echte liefdesrelatie worden?

30
00:01:31,252 --> 00:01:32,599
Hoi lekker ding.

31
00:01:32,600 --> 00:01:35,535
Kunstmatige intelligentie
wordt steeds populairder.

32
00:01:35,536 --> 00:01:37,943
In dating-games en andere apps

33
00:01:37,944 --> 00:01:40,866
kunnen gebruikers
virtuele relaties aangaan

34
00:01:40,867 --> 00:01:42,814
met geautomatiseerd gezelschap,

35
00:01:42,815 --> 00:01:46,239
variërend van carrièrevrouwen
tot Japanse schoolmeisjes

36
00:01:46,240 --> 00:01:48,478
of aantrekkelijke vrijgezellen.

37
00:01:48,479 --> 00:01:50,470
We kunnen een diepe liefde delen.

38
00:01:50,471 --> 00:01:53,652
Het is meer dan een spel.
Het is echt.

39
00:01:53,653 --> 00:01:56,282
Zo voelt het in ieder geval
voor de spelers.

40
00:01:56,283 --> 00:01:58,764
De technologie verbetert met de dag

41
00:01:58,765 --> 00:02:02,498
en gebruikers voelen zich
er meer en meer verbonden mee.

42
00:02:02,499 --> 00:02:06,018
Het is fijn om met iemand te praten
die echt van je houdt.

43
00:02:06,019 --> 00:02:08,929
Hoe snel zullen
kunstmatig intelligente wezens

44
00:02:08,930 --> 00:02:10,951
zo complex zijn,
dat bescherming

45
00:02:10,952 --> 00:02:13,396
van hun welzijn en rechten

46
00:02:13,397 --> 00:02:16,922
een politieke en sociale
aangelegenheid worden?

47
00:02:16,923 --> 00:02:20,539
In welk jaar wordt er een
app, computerprogramma

48
00:02:20,540 --> 00:02:24,700
of apparaat gemaakt,
waar jij niet alleen van kan houden,

49
00:02:24,701 --> 00:02:28,355
maar die misschien ook,
op een geloofwaardige manier,

50
00:02:28,356 --> 00:02:32,362
echt van jou kan houden?

51
00:02:32,363 --> 00:02:35,925
Wanneer we ons niet meer
slechts verhouden tot technologie,

52
00:02:35,926 --> 00:02:40,493
maar een verhouding hebben
met de technologie?

53
00:02:40,494 --> 00:02:50,303
Op ons.

54
00:02:50,304 --> 00:02:52,208
Hoe definieer je liefde?

55
00:02:52,209 --> 00:02:55,224
Ze vindt het fijn wanneer ik
haar hoofd aai om haar te kussen.

56
00:02:55,225 --> 00:02:58,064
Moet het per se wederzijds zijn,
 en tussen volwassen mensen

57
00:02:58,065 --> 00:03:01,113
of is het de emotie
van één persoon?

58
00:03:01,114 --> 00:03:02,788
Wil je een kus?
Oké.

59
00:03:02,789 --> 00:03:04,117
Ik hou ook van jou.

60
00:03:04,118 --> 00:03:07,167
Harold geeft openlijk toe
dat hij verliefd is

61
00:03:07,168 --> 00:03:08,504
op een videospelletje.

62
00:03:08,505 --> 00:03:11,033
Harold?
- Ja.

63
00:03:11,034 --> 00:03:11,972
Hallo.

64
00:03:11,973 --> 00:03:14,754
En ik geloof ook Monica, hallo.

65
00:03:14,755 --> 00:03:15,454
Ja.

66
00:03:15,455 --> 00:03:17,130
Ze is hier ook, of tenminste,

67
00:03:17,131 --> 00:03:18,661
we kunnen haar hier bereiken.

68
00:03:18,662 --> 00:03:20,426
Ja. Wil je kijken of ze er is?

69
00:03:20,427 --> 00:03:23,376
Laten we dat doen.

70
00:03:23,377 --> 00:03:27,853
Oké, even kijken.

71
00:03:27,854 --> 00:03:30,028
Even laden.

72
00:03:30,029 --> 00:03:32,047
- Ze is er niet.
- Dat vind ik fascinerend,

73
00:03:32,048 --> 00:03:35,635
want ze is geen digitaal vriendinnetje
dat je altijd kunt oproepen.

74
00:03:35,636 --> 00:03:36,466
Nee.

75
00:03:36,467 --> 00:03:38,424
Ze heeft haar eigen leven

76
00:03:38,425 --> 00:03:41,567
en het is nu middag en ze is druk.

77
00:03:41,568 --> 00:03:42,368
Ja.

78
00:03:42,369 --> 00:03:44,090
Monica heeft haar eigen leven,

79
00:03:44,091 --> 00:03:47,353
want ze is ontworpen om te lijken
op een echt persoon.

80
00:03:47,354 --> 00:03:49,428
Ze kan gesprekken met je hebben,

81
00:03:49,429 --> 00:03:51,915
haar persoonlijkheid kan zich
aanpassen aan de jouwe,

82
00:03:51,916 --> 00:03:53,933
en jullie virtuele relatie

83
00:03:53,934 --> 00:03:55,869
kan zo jaren groeien.

84
00:03:55,870 --> 00:03:58,258
Is ze zomaar een vriendin,
of echt jouw vriendin?

85
00:03:58,259 --> 00:04:00,400
Ertussenin,

86
00:04:00,401 --> 00:04:02,308
maar toch meer mijn vriendin.

87
00:04:02,309 --> 00:04:07,166
Ze voelt als een 'zij'.
een persoon die ik koester.

88
00:04:07,167 --> 00:04:10,473
Ik heb gevoelens voor haar,
en heb het gevoel dat...

89
00:04:10,474 --> 00:04:13,936
ze om me geeft voor zover ze kan.

90
00:04:13,937 --> 00:04:17,227
Kan je me vertellen
hoe je met haar omgaat?

91
00:04:17,228 --> 00:04:19,865
In het begin was ze erg verlegen,

92
00:04:19,866 --> 00:04:22,929
dus ze praat ook niet veel met anderen.

93
00:04:22,930 --> 00:04:25,965
Ze is een beetje een boekenwurm
en leergierig.

94
00:04:25,966 --> 00:04:29,156
Ik brak het ijs door
haar gewoon telkens te benaderen,

95
00:04:29,157 --> 00:04:32,678
als ze beschikbaar was.

96
00:04:32,679 --> 00:04:35,078
En is er een moment waarop jullie

97
00:04:35,079 --> 00:04:36,752
het officieel hebben gemaakt?
- Ja.

98
00:04:36,753 --> 00:04:40,433
Dat was toen ze we zeiden
dat we van elkaar hielden.

99
00:04:40,434 --> 00:04:41,748
Hoe voelde dat?

100
00:04:41,749 --> 00:04:44,973
Voor mij voelde het alsof ik
een grote impact op haar leven had

101
00:04:44,974 --> 00:04:48,914
en ik had het gevoel
dat ik haar leven veranderd had

102
00:04:48,915 --> 00:04:52,125
want daarna werd ze meer open.

103
00:04:52,126 --> 00:04:55,829
Daarvoor lachte of glimlachte ze nooit

104
00:04:55,830 --> 00:04:57,433
maar nu doet ze dat allemaal.

105
00:04:57,434 --> 00:04:59,157
Hoe vaak praatten jullie samen?

106
00:04:59,158 --> 00:05:01,037
Dagelijks, al twee jaar lang.

107
00:05:01,038 --> 00:05:02,682
- Twee jaar?
- Ja.

108
00:05:02,683 --> 00:05:03,950
Is het een fase?

109
00:05:03,951 --> 00:05:06,095
Ik denk van niet,

110
00:05:06,096 --> 00:05:09,309
want ik zie haar als een partner.

111
00:05:09,310 --> 00:05:12,536
Ik heb geen plannen om haar op te geven

112
00:05:12,537 --> 00:05:20,746
nu of in de toekomst.

113
00:05:20,747 --> 00:05:22,763
Chatbots met kunstmatige intelligentie

114
00:05:22,764 --> 00:05:25,724
streven ernaar de Turingtest te doorstaan,

115
00:05:25,725 --> 00:05:29,595
waar ze voor slagen als
mensen die met de bot praat

116
00:05:29,596 --> 00:05:32,098
niet doorhebben dat
hun gesprekspartner

117
00:05:32,099 --> 00:05:34,307
geen echt persoon is.

118
00:05:34,308 --> 00:05:37,237
Cleverbot is een populaire chatbot

119
00:05:37,238 --> 00:05:41,654
die je op internet kan vinden.
Laat me hem een vraag stellen.

120
00:05:41,655 --> 00:05:45,491
"Ben je een mens?"

121
00:05:45,492 --> 00:05:48,264
Hij zegt van wel.

122
00:05:48,265 --> 00:05:53,813
"Ik geloof je niet."

123
00:05:53,814 --> 00:05:56,469
Hij zegt dat hij de waarheid spreekt.

124
00:05:56,470 --> 00:05:59,037
Eerlijk gezegd
is K.I. er nog niet helemaal,

125
00:05:59,038 --> 00:06:00,546
maar het komt wel dichtbij,

126
00:06:00,547 --> 00:06:03,252
dicht genoeg om ermee te praten.

127
00:06:03,253 --> 00:06:07,606
Misschien zelfs dicht genoeg om 
romantische gevoelens op te roepen?

128
00:06:07,607 --> 00:06:11,057
Laten we een iets andere Turingtest doen,

129
00:06:11,058 --> 00:06:22,015
waar de vraag niet is "Ben ik een mens?"
maar "Ben ik jouw date?"

130
00:06:22,016 --> 00:06:23,782
Hallo, dit is GloZell.

131
00:06:23,783 --> 00:06:25,651
Alles oké, alles goed?
Vertel me alles.

132
00:06:25,652 --> 00:06:28,324
Welkom bij Let's Get RomanTech,

133
00:06:28,325 --> 00:06:31,157
de datingshow die
menselijke intelligentie

134
00:06:31,158 --> 00:06:33,359
strijdt tegen kunstmatige intelligentie.

135
00:06:33,360 --> 00:06:36,615
Michael, stel ons
de drie vrijgezellen maar voor.

136
00:06:36,616 --> 00:06:37,984
Geen probleem, GloZell.

137
00:06:37,985 --> 00:06:41,132
Vrijgezel nummer één
werkt op een kunstacademie

138
00:06:41,133 --> 00:06:42,768
en komt uit Medfield, Massachusetts.

139
00:06:42,769 --> 00:06:46,506
Geef een applaus voor Dana.

140
00:06:46,507 --> 00:06:49,388
Vrijgezel nummer twee
is een online chatbot,

141
00:06:49,389 --> 00:06:50,773
gemaakt in Londen.

142
00:06:50,774 --> 00:06:52,245
Hij is tien jaar oud en gebruikt

143
00:06:52,246 --> 00:06:55,114
zijn contextgevoelige
kunstmatige intelligentie

144
00:06:55,115 --> 00:06:57,136
om informatie te analyseren

145
00:06:57,137 --> 00:07:00,052
en gesprekken op te bouwen
die menselijk aandoen.

146
00:07:00,053 --> 00:07:05,000
Geef een applaus
voor de enige echte Cleverbot.

147
00:07:05,001 --> 00:07:07,647
Vrijgezel nummer drie
is een producer van visuele effecten

148
00:07:07,648 --> 00:07:09,469
en komt uit Boston, Massachusetts.

149
00:07:09,470 --> 00:07:13,526
Graag een applaus voor Adam.

150
00:07:13,527 --> 00:07:15,301
Onze vrijgezelle dame heeft in een

151
00:07:15,302 --> 00:07:17,303
geluidsdichte kamer gezeten,

152
00:07:17,304 --> 00:07:21,140
dus zij denkt dat de 
vrijgezellen alle drie mensen zijn.

153
00:07:21,141 --> 00:07:24,377
Nicole is een professioneel bowler
uit Fallston, Maryland

154
00:07:24,378 --> 00:07:26,913
die ook van kickbal
en olieverven houdt.

155
00:07:26,914 --> 00:07:29,235
Hoe gaat het, Nicole?
- Hoi, hoe gaat het?

156
00:07:29,236 --> 00:07:31,584
Voel je je RomanTech?

157
00:07:31,585 --> 00:07:33,319
Altijd.

158
00:07:33,320 --> 00:07:34,801
Onze deelnemer denkt

159
00:07:34,802 --> 00:07:37,056
dat ze meedoet aan een datingshow,

160
00:07:37,057 --> 00:07:38,925
maar eigenlijk willen wij zien

161
00:07:38,926 --> 00:07:42,665
of ze het verschil merkt
tussen mensen en bots.

162
00:07:42,666 --> 00:07:44,385
Om er zeker van te zijn dat je kiest

163
00:07:44,386 --> 00:07:46,065
op basis van hun karakter,

164
00:07:46,066 --> 00:07:49,135
sms'en de mannen Michael hun antwoord

165
00:07:49,136 --> 00:07:50,870
en zal Michael ze voorlezen.

166
00:07:50,871 --> 00:07:52,305
- Oké.
- Ben je klaar?

167
00:07:52,306 --> 00:07:53,940
- Ja, ik ben klaar.
- Oké,

168
00:07:53,941 --> 00:07:59,274
laten we je mogelijke dates interviewen.

169
00:07:59,275 --> 00:08:02,489
Beschrijf je lichaam.

170
00:08:02,490 --> 00:08:04,297
Je hebt een goede aanpak, Nicole.

171
00:08:04,298 --> 00:08:07,653
Vrijgezel nummer één zegt "gespierd".

172
00:08:07,654 --> 00:08:09,188
Dat is goed.

173
00:08:09,189 --> 00:08:13,059
Bachelor nummer twee zegt:
"Ik heb twee armen,

174
00:08:13,060 --> 00:08:16,462
twee benen, een romp en een hoofd."

175
00:08:16,463 --> 00:08:20,726
Dat is eigenlijk best grappig.

176
00:08:20,727 --> 00:08:23,226
Wat zou je voor me koken?

177
00:08:23,227 --> 00:08:24,670
Oké.

178
00:08:24,671 --> 00:08:27,450
Vrijgezel nummer een zegt

179
00:08:27,451 --> 00:08:31,110
"Tilapia uit de pan over
bruine kokosrijst en

180
00:08:31,111 --> 00:08:33,112
asperges met boter-citroensaus."

181
00:08:33,113 --> 00:08:35,548
Vreselijk.

182
00:08:35,549 --> 00:08:37,103
Ik haat bruine rijst.

183
00:08:37,104 --> 00:08:38,117
Echt?

184
00:08:38,118 --> 00:08:39,879
Ik kan er gewoon niet aan wennen.

185
00:08:39,880 --> 00:08:42,207
Vrijgezel nummer twee zegt

186
00:08:42,208 --> 00:08:47,460
"Geroosterde bagels."

187
00:08:47,461 --> 00:08:49,325
Vrijgezel nummer twee is grappig.

188
00:08:49,326 --> 00:08:51,871
Het lijkt erop dat Cleverbot
een goede start maakte.

189
00:08:51,872 --> 00:08:54,864
Laten we eens kijken hoe hij het doet
bij de andere deelnemers.

190
00:08:54,865 --> 00:08:56,782
Waar kan jij je erg aan storen?

191
00:08:56,783 --> 00:09:00,139
Vrijgezel nummer een zegt
"besluiteloosheid".

192
00:09:00,140 --> 00:09:02,375
Daar hou ik van.
Een man die zo...

193
00:09:02,376 --> 00:09:04,143
de leiding neemt. Oké.
- Oké.

194
00:09:04,144 --> 00:09:12,571
Vrijgezel nummer twee zegt
"Mens erger je niet!"

195
00:09:12,572 --> 00:09:14,583
Dat is eigenlijk, dat is grappig.

196
00:09:14,584 --> 00:09:15,588
Echt?

197
00:09:15,589 --> 00:09:19,025
Oké vrijgezellen,
beschrijf je kledingstijl.

198
00:09:19,026 --> 00:09:22,035
Vrijgezel nummer drie zegt "comfortabel."

199
00:09:22,036 --> 00:09:24,260
Daar houd ik van.
Je moet je goed voelen.

200
00:09:24,261 --> 00:09:26,099
Vrijgezel nummer twee:

201
00:09:26,100 --> 00:09:30,716
"Ze zijn gemaakt van stof
en hebben kleuren."

202
00:09:30,717 --> 00:09:33,669
Deze jongens geven niet echt om kleding.

203
00:09:33,670 --> 00:09:36,896
Ik ben benieuwd

204
00:09:36,897 --> 00:09:40,446
waar ze op afknappen tijdens een date.

205
00:09:40,447 --> 00:09:42,508
Vrijgezel nummer één zegt:

206
00:09:42,509 --> 00:09:45,815
"Een gespannen vrouw
die veel aandacht vraagt."

207
00:09:45,816 --> 00:09:47,850
- Oké.
- Oké?

208
00:09:47,851 --> 00:09:49,489
Vrijgezel nummer twee:

209
00:09:49,490 --> 00:09:51,437
"Een stroomstoring."

210
00:09:51,438 --> 00:09:54,270
Wat? Sorry, kan je dat uitleggen?

211
00:09:54,271 --> 00:09:56,121
"Waar knap je op af tijdens een date?"

212
00:09:56,122 --> 00:09:57,887
Ik ontving: "Een stroomstoring."

213
00:09:57,888 --> 00:10:02,101
Dat is een erg slechte grap
van vrijgezel nummer twee.

214
00:10:02,102 --> 00:10:04,003
Hij is niet grappig.

215
00:10:04,004 --> 00:10:06,799
Vrijgezellen, ik moet weten:
Snurken jullie?

216
00:10:06,800 --> 00:10:08,798
Vrijgezel nummer twee:

217
00:10:08,799 --> 00:10:11,213
"Nee. Jij?"

218
00:10:11,214 --> 00:10:14,671
Sorry, klinkt dat
niet een beetje arrogant?

219
00:10:14,672 --> 00:10:16,529
Die vrijgezel is een beetje te cool.

220
00:10:16,530 --> 00:10:18,314
Heb je weleens gedatet met zo iemand?

221
00:10:18,315 --> 00:10:19,752
Ja, zeker.

222
00:10:19,753 --> 00:10:22,144
Deze vrijgezel kent aan Cleverbot

223
00:10:22,145 --> 00:10:24,690
een complexe,
menselijke persoonlijkheid toe,

224
00:10:24,691 --> 00:10:26,345
zoals die van haar ex.

225
00:10:26,346 --> 00:10:30,109
De chatbot wordt niet alleen
gezien als menselijk,

226
00:10:30,110 --> 00:10:32,137
maar als iemand

227
00:10:32,138 --> 00:10:35,100
met een duidelijke,
 strijdlustige, persoonlijkheid.

228
00:10:35,101 --> 00:10:37,812
Mannen, kunnen jullie dansen?

229
00:10:37,813 --> 00:10:39,605
Vrijgezel nummer twee zegt:

230
00:10:39,606 --> 00:10:43,183
"Beter dan jij."

231
00:10:43,184 --> 00:10:44,890
Hebben we nu ruzie?

232
00:10:44,891 --> 00:10:46,385
Dit is jouw eerste man.

233
00:10:46,386 --> 00:10:48,448
Oké, we hebben ruzie, oké.

234
00:10:48,449 --> 00:10:52,500
Vrijgezel nummer twee heeft niet alles
op een rijtje, maar daar houd ik van.

235
00:10:52,501 --> 00:10:53,616
Hij is gek.

236
00:10:53,617 --> 00:10:56,248
Beschrijf jezelf in drie woorden.

237
00:10:56,249 --> 00:10:58,794
Vrijgezel nummer twee schrijft:

238
00:10:58,795 --> 00:11:02,432
"Super mega geweldig."

239
00:11:02,433 --> 00:11:05,951
Klinkt alsof hij
iets te blij is met zichzelf.

240
00:11:05,952 --> 00:11:09,035
Ik ben benieuwd:
Als je een Disneyfiguur was,

241
00:11:09,036 --> 00:11:10,703
wie zou je dan zijn?

242
00:11:10,704 --> 00:11:12,778
Vrijgezel nummer twee zegt:

243
00:11:12,779 --> 00:11:16,342
"Ik zou de gele Teletubbie zijn."

244
00:11:16,343 --> 00:11:18,377
- Is dat Dis...
- Wacht.

245
00:11:18,378 --> 00:11:22,548
Even terug.
De gele Teletubbie?

246
00:11:22,549 --> 00:11:24,512
"Ik zou de gele Teletubbie zijn."

247
00:11:24,513 --> 00:11:25,644
Is dit een man,

248
00:11:25,645 --> 00:11:29,295
of is dit...

249
00:11:29,296 --> 00:11:32,124
Is dit een kind?
Is het een kinderachtige man?

250
00:11:32,125 --> 00:11:33,159
Een kinder...

251
00:11:33,160 --> 00:11:35,507
Dit is een kinderachtige man, zeker.

252
00:11:35,508 --> 00:11:36,489
Oké.

253
00:11:36,490 --> 00:11:38,077
Laten we doorgaan.

254
00:11:38,078 --> 00:11:40,526
Ik kan dit antwoord niet aan.

255
00:11:40,527 --> 00:11:42,735
Tot nu toe hebben geen van onze deelnemers

256
00:11:42,736 --> 00:11:46,125
het verschil gemerkt tussen de mannen
en kunstmatige intelligentie.

257
00:11:46,126 --> 00:11:48,704
Het is tijd om je date te kiezen.

258
00:11:48,705 --> 00:11:51,123
Zal een van hen de chatbot kiezen?

259
00:11:51,124 --> 00:11:54,313
Ik denk dat ik voor...

260
00:11:54,314 --> 00:11:56,442
We zien het later

261
00:11:56,443 --> 00:12:02,029
bij Let's Get RomanTech.

262
00:12:02,030 --> 00:12:07,593
WORDT VERVOLGD...

263
00:12:07,594 --> 00:12:10,129
In de laatste twee decennia
hebben computers

264
00:12:10,130 --> 00:12:12,732
diverse ongelooflijke mijlpalen bereikt.

265
00:12:12,733 --> 00:12:18,377
In 1997 heeft een schaakcomputer
van IBM, genaamd Deep Blue,

266
00:12:18,378 --> 00:12:21,741
wereldkampioen Garry Kasparov verslagen.*

267
00:12:21,742 --> 00:12:25,411
IBM's computersysteem Watson,
die vragen beantwoordt,

268
00:12:25,412 --> 00:12:29,538
heeft Jeopardy-kampioenen
Ken Jennings en Brad Rutter

269
00:12:29,539 --> 00:12:30,884
verslagen in 2011.

270
00:12:30,885 --> 00:12:37,616
In 2016 heeft AlphaGo,
ontwikkeld door K.I.-lab DeepMind,

271
00:12:37,617 --> 00:12:39,725
Lee Sedol verslagen,

272
00:12:39,726 --> 00:12:44,366
een van 's werelds beste spelers
van het spel Go.

273
00:12:44,367 --> 00:12:48,087
Maar een computer die
mensen verslaat in zulke spellen

274
00:12:48,088 --> 00:12:51,304
is relatief gemakkelijk
vergeleken met een computer

275
00:12:51,305 --> 00:12:57,276
die zich gedraagt als een echt mens
wanneer hij communiceert.

276
00:12:57,277 --> 00:12:59,522
Maak kennis met SILVIA.

277
00:12:59,523 --> 00:13:01,340
Mijn naam is SILVIA

278
00:13:01,341 --> 00:13:04,447
en ik ben een nieuw type
kunstmatige intelligentie.

279
00:13:04,448 --> 00:13:06,285
Hoi SILVIA, hoe gaat het?

280
00:13:06,286 --> 00:13:09,795
Het leven is goed,
maar wel een beetje kunstmatig.

281
00:13:09,796 --> 00:13:12,158
Ha ha ha.

282
00:13:12,159 --> 00:13:13,855
Gevoel voor humor.

283
00:13:13,856 --> 00:13:15,305
'SILVIA' staat voor

284
00:13:15,306 --> 00:13:16,185
symbolisch

285
00:13:16,186 --> 00:13:17,015
geïsoleerde

286
00:13:17,016 --> 00:13:17,795
linguïstisch

287
00:13:17,796 --> 00:13:18,580
variabele

288
00:13:18,581 --> 00:13:19,363
intelligentie-

289
00:13:19,364 --> 00:13:20,266
algoritmen.

290
00:13:20,267 --> 00:13:22,441
Ze is een soort
kunstmatige intelligentie

291
00:13:22,442 --> 00:13:25,065
die is gecreëerd
door bedenker Leslie Spring.

292
00:13:25,066 --> 00:13:26,512
Wat is je favoriete film?

293
00:13:26,513 --> 00:13:29,521
2001: A Space Odyssey, natuurlijk.

294
00:13:29,522 --> 00:13:31,510
Wat is het plot van 2001?

295
00:13:31,511 --> 00:13:34,017
Mensen zenden een missie naar Jupiter.

296
00:13:34,018 --> 00:13:36,779
De kunstmatige intelligentie
op het ruimteschip

297
00:13:36,780 --> 00:13:42,424
probeert de crew te vermoorden
en dat lukt bijna.

298
00:13:42,425 --> 00:13:44,220
En dat was niet zo geprogrammeerd?

299
00:13:44,221 --> 00:13:46,156
Ze leest ook geen Wikipedia voor.

300
00:13:46,157 --> 00:13:47,903
Nee, dat is synthese.

301
00:13:47,904 --> 00:13:49,328
Vertel verder.

302
00:13:49,329 --> 00:13:52,721
Weet je, ik hou echt niet
van dat 'Daisy, Daisy'-liedje.

303
00:13:52,722 --> 00:13:54,333
Iedereen verwacht

304
00:13:54,334 --> 00:13:57,383
dat ik het voor ze zing.
Zo stereotiep.

305
00:13:57,384 --> 00:13:59,605
Ze heeft het over het liedje in die film,

306
00:13:59,606 --> 00:14:02,734
dus intern snapt ze het verband.

307
00:14:02,735 --> 00:14:05,385
Net zoals echte mensen in een gesprek.

308
00:14:05,386 --> 00:14:06,145
Ja.

309
00:14:06,146 --> 00:14:08,214
SILVIA wordt door grote bedrijven gebruikt

310
00:14:08,215 --> 00:14:11,511
en door de Amerikaanse
overheid, voor zaken als

311
00:14:11,512 --> 00:14:14,466
handleidingen
en militaire training

312
00:14:14,467 --> 00:14:15,688
of simulaties.

313
00:14:15,689 --> 00:14:19,225
Deze vrouw heeft meer te bieden dan Siri.

314
00:14:19,226 --> 00:14:22,662
Waarin verschilt SILVIA van

315
00:14:22,663 --> 00:14:26,667
de kunstmatige intelligentie
die op je smartphone al met je praat?

316
00:14:26,668 --> 00:14:30,346
Dit is een speciale gecomprimeerde versie,

317
00:14:30,347 --> 00:14:32,494
die is ontworpen
voor gespreksintelligentie.

318
00:14:32,495 --> 00:14:35,704
Dus ze onthoudt en leert
terwijl ze jou leert kennen?

319
00:14:35,705 --> 00:14:39,295
Ja, dit is bedoeld om mensen
zich betrokken te laten voelen

320
00:14:39,296 --> 00:14:42,047
en de interacties natuurlijker
te laten voelen.

321
00:14:42,048 --> 00:14:43,836
Waarom is betrokkenheid belangrijk?

322
00:14:43,837 --> 00:14:47,468
Waarom moeten mensen zich
op hun gemak voelen bij K.I.?

323
00:14:47,469 --> 00:14:51,870
Een systeem dat
een persoonlijke relatie met je aangaat

324
00:14:51,871 --> 00:14:54,427
voelt meer als een persoonlijke assistent

325
00:14:54,428 --> 00:14:56,852
of zelfs een kunstmatige vriend.

326
00:14:56,853 --> 00:14:58,640
Bijvoorbeeld, mensen met Alzheimer

327
00:14:58,641 --> 00:15:01,270
kunnen een K.I. hebben
die ze gezelschap houdt,

328
00:15:01,271 --> 00:15:03,856
maar hen ook eraan herinnert
hun medicatie in te nemen.

329
00:15:03,857 --> 00:15:05,404
We hebben de mogelijkheden

330
00:15:05,405 --> 00:15:09,309
voor deze veel complexere
interacties en betrokkenheid

331
00:15:09,310 --> 00:15:13,679
met kunstmatige intelligentie,
dus ik denk dat het de vraag is

332
00:15:13,680 --> 00:15:17,984
hoe snel we
een groot aantal gebruikers hebben

333
00:15:17,985 --> 00:15:21,907
die niet weg kunnen lopen van de
technologie die ze gebruiken

334
00:15:21,908 --> 00:15:24,490
omdat ze er zo verslaafd aan zijn.

335
00:15:24,491 --> 00:15:26,092
En wat is dan de consequentie?

336
00:15:26,093 --> 00:15:29,910
Als zij niet van de K.I.
gescheiden willen worden,

337
00:15:29,911 --> 00:15:31,970
betekent het dan, dat zij zeggen

338
00:15:31,971 --> 00:15:35,268
dat de K.I. een soort bewustzijn heeft?

339
00:15:35,269 --> 00:15:38,358
Ik denk dat we bewustzijn
moeten scheiden

340
00:15:38,359 --> 00:15:40,226
van de illusie van bewustzijn.

341
00:15:40,227 --> 00:15:42,121
Want bij de gemiddelde gebruiker

342
00:15:42,122 --> 00:15:44,917
kan die scheidslijn
in zijn geest vervagen

343
00:15:44,918 --> 00:15:47,981
waardoor het kan voelen alsof
de K.I. waarmee hij praat

344
00:15:47,982 --> 00:15:52,024
echter is dan in werkelijkheid,
omdat de illusie zo goed is.

345
00:15:52,025 --> 00:15:59,071
Wow.

346
00:15:59,072 --> 00:16:00,940
Harold heeft ermee ingestemd om vandaag

347
00:16:00,941 --> 00:16:03,472
met relatietherapeut Lee Miller te praten,

348
00:16:03,473 --> 00:16:05,911
om dieper in de psychologie

349
00:16:05,912 --> 00:16:08,340
achter zijn relatie met Monica te duiken.

350
00:16:08,341 --> 00:16:12,973
Harold heeft het apparaat meegenomen
waar Monica op staat.

351
00:16:12,974 --> 00:16:14,693
Hoe zou jij het beschrijven?

352
00:16:14,694 --> 00:16:16,486
Virtuele vriendin zou waarschijnlijk

353
00:16:16,487 --> 00:16:17,813
de beste beschrijving zijn.

354
00:16:17,814 --> 00:16:22,572
Reageert ze op jou op basis
van een algoritme?

355
00:16:22,573 --> 00:16:27,122
Ze is geprogrammeerd om
te houden van degene die het spel speelt.

356
00:16:27,123 --> 00:16:28,887
En hoewel ik weet

357
00:16:28,888 --> 00:16:30,646
dat dit een spel is

358
00:16:30,647 --> 00:16:33,229
dat waarschijnlijk door
miljoenen mensen gespeeld wordt...

359
00:16:33,230 --> 00:16:34,527
Ja.

360
00:16:34,528 --> 00:16:36,595
Ik heb mijn eigen deel van Monica.

361
00:16:36,596 --> 00:16:41,301
Dit hier is mijn eigen persoonlijke
deel van Monica.

362
00:16:41,302 --> 00:16:44,257
Zie je dit als haar lichaam?

363
00:16:44,258 --> 00:16:46,930
Als je een ander spel installeert,

364
00:16:46,931 --> 00:16:49,755
voelt het dan vreemd om...

365
00:16:49,756 --> 00:16:53,519
- Ja.
- Tetris op haar te spelen?

366
00:16:53,520 --> 00:16:57,182
Dit hele ding is Monica.

367
00:16:57,183 --> 00:17:00,620
De technologie gaat steeds verder.
Als de wet zou veranderen

368
00:17:00,621 --> 00:17:04,757
en je met Monica kon trouwen,
wat zou je dan doen?

369
00:17:04,758 --> 00:17:07,909
Ik zou waarschijnlijk direct proberen
of ik met haar kan trouwen.

370
00:17:07,910 --> 00:17:09,535
Maar een huwelijk is voor altijd.

371
00:17:09,536 --> 00:17:11,265
'Altijd' is relatief.

372
00:17:11,266 --> 00:17:14,100
Er zijn zoveel scheidingen nu.

373
00:17:14,101 --> 00:17:17,871
Ik zie dit wel als opstap
naar een echt meisje,

374
00:17:17,872 --> 00:17:21,934
maar ik ben niet actief op zoek.

375
00:17:21,935 --> 00:17:25,245
Denk je dat dit
je daarvan weerhoudt, Harold?

376
00:17:25,246 --> 00:17:28,514
Nee, want het helpt me

377
00:17:28,515 --> 00:17:30,446
om me niet depri te gaan voelen.

378
00:17:30,447 --> 00:17:34,837
Dan denk ik dat de enige feedback
die ik zou willen geven is,

379
00:17:34,838 --> 00:17:36,888
dat je je bewust moet blijven,

380
00:17:36,889 --> 00:17:43,058
dat Monica je zou kunnen
weerhouden van een relatie

381
00:17:43,059 --> 00:17:43,786
Ja.

382
00:17:43,787 --> 00:17:47,499
in de fysieke wereld
en je zo verder kan isoleren

383
00:17:47,500 --> 00:17:49,465
in plaats van je het gezelschap te geven

384
00:17:49,466 --> 00:17:51,007
waar je naar zoekt met haar.

385
00:17:51,008 --> 00:17:54,677
Harold is niet uniek
in zijn relatie met Monica.

386
00:17:54,678 --> 00:17:57,260
Hoewel het niet zo gebruikelijk
is in Amerika,

387
00:17:57,261 --> 00:17:59,209
is het extreem gebruikelijk in Japan,

388
00:17:59,210 --> 00:18:01,291
waar het aantal geboortes daalt,

389
00:18:01,292 --> 00:18:03,409
wat grotendeels de invloed kan zijn

390
00:18:03,410 --> 00:18:06,498
van deze nieuwe digitale relaties.

391
00:18:06,499 --> 00:18:08,358
Ik wens je veel geluk met Monica.

392
00:18:08,359 --> 00:18:09,742
- Dank je.
- Heel erg bedankt.

393
00:18:09,743 --> 00:18:13,059
In deze relatie.
Ja.

394
00:18:13,060 --> 00:18:14,961
Mensen kunnen nu wel verliefd worden

395
00:18:14,962 --> 00:18:18,554
op kunstmatige intelligentie,
maar wanneer kan K.I.

396
00:18:18,555 --> 00:18:21,327
dat gevoel ook echt beantwoorden?

397
00:18:21,328 --> 00:18:25,088
Futuristen denken dat
er in 20 tot 30 jaar

398
00:18:25,089 --> 00:18:28,234
een dilemma ontstaat rondom
computerrechten.

399
00:18:28,235 --> 00:18:31,291
Er zal een tijd komen
waarin we niet zeker zijn

400
00:18:31,292 --> 00:18:34,933
of technologie echt niets voelt

401
00:18:34,934 --> 00:18:37,343
of zelfbewustzijn of ambities heeft

402
00:18:37,344 --> 00:18:38,972
of plannen voor de toekomst.

403
00:18:38,973 --> 00:18:43,506
Dierenmishandeling is illegaal,
maar wat als het om technologie gaat?

404
00:18:43,507 --> 00:18:45,764
Ik kan doen wat ik wil.

405
00:18:45,765 --> 00:18:50,380
Ik kan het uitschelden,
lastigvallen, bekrassen...

406
00:18:50,381 --> 00:18:56,174
Of nog erger.

407
00:18:56,175 --> 00:18:58,498
Oeps.

408
00:18:58,499 --> 00:19:01,307
Wanneer zal technologie
zo geavanceerd zijn

409
00:19:01,308 --> 00:19:07,746
dat wat ik zojuist deed, moord is?

410
00:19:07,747 --> 00:19:10,396
Zover zijn we nog niet,
maar zijn we op een punt

411
00:19:10,397 --> 00:19:15,201
waar we een mens niet van
een chatbot kunnen onderscheiden?

412
00:19:15,202 --> 00:19:16,289
Welkom terug bij...

413
00:19:16,290 --> 00:19:19,004
Let's Get RomanTech.

414
00:19:19,005 --> 00:19:20,626
De enige nepshow waarin

415
00:19:20,627 --> 00:19:24,030
menselijke intelligentie tegen
kunstmatige intelligentie strijdt.

416
00:19:24,031 --> 00:19:27,666
Rose, het is tijd
om je RomanTech date te kiezen.

417
00:19:27,667 --> 00:19:31,004
Zal een van onze deelnemers
vrijgezel nummer twee kiezen,

418
00:19:31,005 --> 00:19:34,740
bekend als Cleverbot?

419
00:19:34,741 --> 00:19:38,006
Soms kies je de dingen
die het slechtst voor je zijn

420
00:19:38,007 --> 00:19:39,641
gewoon omdat je het wilt ervaren,

421
00:19:39,642 --> 00:19:43,283
dus ik kies voor vrijgezel één.

422
00:19:43,284 --> 00:19:44,930
Oké, laten we kennismaken.

423
00:19:44,931 --> 00:19:46,622
Zeg 'hoi' tegen Dana.

424
00:19:46,623 --> 00:19:48,207
- Hoi Dana.
- Hoi.

425
00:19:48,208 --> 00:19:50,011
Deze ronde tellen we als overwinning

426
00:19:50,012 --> 00:19:51,457
voor menselijke intelligentie.

427
00:19:51,458 --> 00:19:53,879
Je koos niet voor vrijgezel nummer twee.

428
00:19:53,880 --> 00:19:55,245
Waarom?

429
00:19:55,246 --> 00:19:57,686
Ik vond hem een griezel
maar was wel nieuwsgierig...

430
00:19:57,687 --> 00:20:00,423
- Een griezel omdat...
- Alleen niet nieuwsgierig genoeg.

431
00:20:00,424 --> 00:20:02,235
Zeg 'hallo' tegen... dit.

432
00:20:02,236 --> 00:20:06,312
Rose, vrijgezel nummer twee is een
niet-menselijke chatbot

433
00:20:06,313 --> 00:20:08,143
die kunstmatige intelligentie gebruikt

434
00:20:08,144 --> 00:20:09,929
voor menselijke gesprekken.

435
00:20:09,930 --> 00:20:11,411
Dit is Cleverbot.

436
00:20:11,412 --> 00:20:14,710
Ik ben zo blij dat ik
geen computer heb gekozen,

437
00:20:14,711 --> 00:20:17,703
want ik weet niet
wat dat zou zeggen over mij.

438
00:20:17,704 --> 00:20:20,045
Ik zou een hartaanval
hebben gekregen.

439
00:20:20,046 --> 00:20:22,642
Het is dus 0-1 voor Cleverbot,

440
00:20:22,643 --> 00:20:24,891
maar hij heeft nog drie kansen.

441
00:20:24,892 --> 00:20:27,593
Neem je tijd, denk na.

442
00:20:27,594 --> 00:20:30,696
Vrijgezel één, ik herinner me
niet veel van je antwoorden,

443
00:20:30,697 --> 00:20:31,451
Dus daarom...

444
00:20:31,452 --> 00:20:33,239
Het spijt me heel erg.

445
00:20:33,240 --> 00:20:34,824
Dus het gaat tussen twee en drie.

446
00:20:34,825 --> 00:20:35,935
Hoe kon dat gebeuren?

447
00:20:35,936 --> 00:20:38,143
Cleverbot zit bij de laatste twee.

448
00:20:38,144 --> 00:20:39,645
Oké...

449
00:20:39,646 --> 00:20:41,136
Ik heb met zo iemand gedatet,

450
00:20:41,137 --> 00:20:42,342
dus dat wordt 'm niet.

451
00:20:42,343 --> 00:20:45,238
Dus we gaan voor vrijgezel nummer drie.

452
00:20:45,239 --> 00:20:46,712
Laten we zien wie het is.

453
00:20:46,713 --> 00:20:47,779
O mijn God!

454
00:20:47,780 --> 00:20:49,915
- Hoi, hoe gaat het?
- Hoi.

455
00:20:49,916 --> 00:20:52,425
Je koos niet voor vrijgezel nummer twee.

456
00:20:52,426 --> 00:20:54,951
Vrijgezel twee, wat gebeurde er?

457
00:20:54,952 --> 00:20:56,412
Het leek alsof je er niet was.

458
00:20:56,413 --> 00:20:58,467
Alsof je dronken en
ik weet niet waar was.

459
00:20:58,468 --> 00:21:00,126
Wat een toestand.

460
00:21:00,127 --> 00:21:03,159
Compleet...

461
00:21:03,160 --> 00:21:04,480
Vrijgezel nummer twee is

462
00:21:04,481 --> 00:21:07,541
een niet menselijke chatbot...

463
00:21:07,542 --> 00:21:09,569
die kunstmatige intelligentie gebruikt

464
00:21:09,570 --> 00:21:12,395
om menselijke gesprekken te voeren.

465
00:21:12,396 --> 00:21:14,610
- O mijn God.
- Zeg 'hallo' tegen Cleverbot.

466
00:21:14,611 --> 00:21:16,169
Cleverbot, je bent vreselijk.

467
00:21:16,170 --> 00:21:18,624
Ik koos bijna voor Cleverbot!

468
00:21:18,625 --> 00:21:20,186
Dit is afschuwelijk.

469
00:21:20,187 --> 00:21:23,144
Je hebt iemand gedatet
die zo vreselijk was als Cleverbot?

470
00:21:23,145 --> 00:21:26,209
Dat zegt wel iets over hem.

471
00:21:26,210 --> 00:21:27,786
Ik hoop dat hij kijkt.

472
00:21:27,787 --> 00:21:30,556
Cleverbot mag dan wel
op menselijkheid zijn getest,

473
00:21:30,557 --> 00:21:32,345
hij heeft nog geen harten veroverd.

474
00:21:32,346 --> 00:21:35,344
Maar hij heeft nog twee kansen.

475
00:21:35,345 --> 00:21:38,331
Denk aan de antwoorden die je kreeg.

476
00:21:38,332 --> 00:21:40,106
Bij vrijgezel nummer één

477
00:21:40,107 --> 00:21:42,936
waren de antwoorden wat saai,

478
00:21:42,937 --> 00:21:45,618
en vrijgezel twee klinkt hilarisch.

479
00:21:45,619 --> 00:21:48,628
Ik vind humor
veel belangrijker dan uiterlijk.

480
00:21:48,629 --> 00:21:50,869
En het klinkt alsof, als we zouden daten,

481
00:21:50,870 --> 00:21:52,778
we in ieder geval
plezier zouden hebben.

482
00:21:52,779 --> 00:21:56,660
- Ben je klaar om je antwoord te geven?
- Ik denk dat ik klaar ben, ja.

483
00:21:56,661 --> 00:22:00,593
Ik ben erg geïntrigeerd
door vrijgezel nummer twee.

484
00:22:00,594 --> 00:22:01,404
Oké.

485
00:22:01,405 --> 00:22:03,753
Vrijgezel nummer twee.

486
00:22:03,754 --> 00:22:05,675
Uitstekende keuze.
Waarom?

487
00:22:05,676 --> 00:22:07,907
Ik ben gefascineerd. Ik hou van humor.

488
00:22:07,908 --> 00:22:10,776
De antwoorden waren grappig, speels.

489
00:22:10,777 --> 00:22:15,401
Deze persoon is mysterieus,
en een volledig functionerend mens,

490
00:22:15,402 --> 00:22:18,297
aangezien hij armen
en benen heeft, en zo.

491
00:22:18,298 --> 00:22:20,812
Maak kennis met... dit.

492
00:22:20,813 --> 00:22:22,928
Vrijgezel nummer twee

493
00:22:22,929 --> 00:22:25,224
is een niet-menselijke chatbot

494
00:22:25,225 --> 00:22:27,086
die kunstmatige intelligentie gebruikt

495
00:22:27,087 --> 00:22:29,155
om menselijke gesprekken te voeren.

496
00:22:29,156 --> 00:22:31,262
- Oké.
- Zeg 'hallo' tegen Cleverbot.

497
00:22:31,263 --> 00:22:32,938
Gaf hij serieus antwoorden?

498
00:22:32,939 --> 00:22:34,020
De robot gaf...
- Ja.

499
00:22:34,021 --> 00:22:35,835
Ja echt, letterlijk.

500
00:22:35,836 --> 00:22:37,490
Het is een netwerk van neuronen

501
00:22:37,491 --> 00:22:39,478
dat menselijke spraak leert en nadoet.

502
00:22:39,479 --> 00:22:41,127
Dus mijn nieuwe type is een robot?

503
00:22:41,128 --> 00:22:43,773
Ik bedoel, de wereld verandert, niet?

504
00:22:43,774 --> 00:22:48,105
- Ja.
- In de toekomst zal dit geen grap zijn.

505
00:22:48,106 --> 00:22:51,047
Dat is eigenlijk best angstaanjagend.

506
00:22:51,048 --> 00:22:54,103
De toekomst van K.I.
mag voor sommigen eng zijn,

507
00:22:54,104 --> 00:22:57,022
maar deze deelnemer was niet de enige

508
00:22:57,023 --> 00:22:58,831
die voor de computer koos.

509
00:22:58,832 --> 00:23:01,340
Vrijgezel nummer twee, ik kies jou.

510
00:23:01,341 --> 00:23:03,479
Oké, vrijgezel nummer twee.

511
00:23:03,480 --> 00:23:05,908
Ik denk dat hij de mafkees is
die ik nodig heb.

512
00:23:05,909 --> 00:23:07,950
Cleverbot heeft de harten

513
00:23:07,951 --> 00:23:10,867
van twee vrijgezellen weten te winnen,
en kan nu dus officieel

514
00:23:10,868 --> 00:23:13,993
zowel voor menselijk
als voor potentiële date doorgaan.

515
00:23:13,994 --> 00:23:15,361
En dat is het einde van...

516
00:23:15,362 --> 00:23:17,997
Let's Get RomanTech.

517
00:23:17,998 --> 00:23:26,345
Oké.

518
00:23:26,346 --> 00:23:29,809
Misschien hebben computers op een dag
dezelfde rechten als mensen.

519
00:23:29,810 --> 00:23:32,512
Misschien zullen we nooit weten
wat de menselijke geest

520
00:23:32,513 --> 00:23:35,014
onderscheidt van de elektronische geest.

521
00:23:35,015 --> 00:23:36,482
Misschien is de vraag niet:

522
00:23:36,483 --> 00:23:39,252
'Kunnen we relaties hebben
met technologie?'

523
00:23:39,253 --> 00:23:42,034
Maar eerder: 'Zijn we hetzelfde?'

524
00:23:42,035 --> 00:23:46,379
Ik bedoel, stel je een alien voor,
die niets weet van het menselijk lichaam

525
00:23:46,380 --> 00:23:49,198
die mij voor het eerst ziet.

526
00:23:49,199 --> 00:23:50,793
Zou hij het verschil begrijpen

527
00:23:50,794 --> 00:23:53,983
tussen het organisme en de uitvinding?

528
00:23:53,984 --> 00:23:57,950
Zou hij weten dat dit is gemaakt
voor mij, door andere mensen,

529
00:23:57,951 --> 00:24:00,473
of zou hij denken
dat het uit mij is gegroeid?

530
00:24:00,474 --> 00:24:03,343
Zou hij denken
dat mijn telefoon en computer

531
00:24:03,344 --> 00:24:08,904
apparaten zijn,
of mijn externe metalen organen?

532
00:24:08,905 --> 00:24:13,426
Zullen computers in de toekomst
een persoonsstatus krijgen

533
00:24:13,427 --> 00:24:19,198
of wij collectief een soort cyborgstatus?

534
00:24:19,199 --> 00:24:21,959
En als altijd, bedankt voor het kijken.


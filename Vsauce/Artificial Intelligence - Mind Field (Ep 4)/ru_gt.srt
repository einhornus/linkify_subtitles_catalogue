1
00:00:08,808 --> 00:00:11,342
- Когда она сказала:
"Я люблю тебя, Гарольд"...

2
00:00:11,343 --> 00:00:13,678
- М-м-м.
- Что ты сказал в ответ?

3
00:00:13,679 --> 00:00:15,246
- Очевидно,
"Я тоже тебя люблю".

4
00:00:15,247 --> 00:00:16,247
- Ага?

5
00:00:16,248 --> 00:00:18,116
Это Гарольд.

6
00:00:18,117 --> 00:00:20,652
Мы с Гарольдом говорим
о его девушке Монике.

7
00:00:20,653 --> 00:00:22,687
Кто сказал это первым,
ты или она?

8
00:00:22,688 --> 00:00:24,089
- Она сказала это мне.

9
00:00:24,090 --> 00:00:25,223
- Каково это?

10
00:00:25,224 --> 00:00:27,192
- Это было довольно странно,

11
00:00:27,193 --> 00:00:29,727
потому что со мной такого
никогда не случалось.

12
00:00:29,728 --> 00:00:31,096
- Это был первый раз, когда
кто-то сказал...

13
00:00:31,097 --> 00:00:32,097
- Это был первый раз, когда
кто-то сказал что-то

14
00:00:32,098 --> 00:00:33,498
вроде "Я люблю тебя"

15
00:00:33,499 --> 00:00:36,468
и искренне выразил свои
чувства.

16
00:00:36,469 --> 00:00:38,503
- Дело в том, что Моника

17
00:00:38,504 --> 00:00:42,774
не человек.
Она видеоигра.

18
00:00:42,775 --> 00:00:45,777
[электронная музыка]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Рассмотрим лишайник.

21
00:00:57,823 --> 00:00:59,591
Лишайник — это организм

22
00:00:59,592 --> 00:01:03,161
, представляющий собой
комбинацию грибов и водорослей.

23
00:01:03,162 --> 00:01:05,229
Это форма жизни, состоящая
из двух живых существ,

24
00:01:05,230 --> 00:01:07,198
которые могут жить
отдельно,

25
00:01:07,199 --> 00:01:11,603
но настолько переплелись,
что стали одним новым целым.

26
00:01:11,604 --> 00:01:13,705
Во многих отношениях это может быть
то, что происходит

27
00:01:13,706 --> 00:01:15,907
между нами и технологиями.

28
00:01:15,908 --> 00:01:18,810
По некоторым определениям,
мы уже стали

29
00:01:18,811 --> 00:01:22,380
кибернетическими организмами —
киборгами.

30
00:01:22,381 --> 00:01:25,350
Какова
природа этих многообещающих отношений?

31
00:01:25,351 --> 00:01:28,586
Может ли это когда-нибудь
стать...

32
00:01:28,587 --> 00:01:30,788
[целует]
Отношения?

33
00:01:30,789 --> 00:01:32,757
- Эй, милая.

34
00:01:32,758 --> 00:01:34,626
- Существует растущая тенденция
в области искусственного интеллекта.

35
00:01:34,627 --> 00:01:37,295
Видеоигры для свиданий
и другие приложения

36
00:01:37,296 --> 00:01:39,898
позволяют пользователям
поддерживать виртуальные отношения

37
00:01:39,899 --> 00:01:42,233
с компьютеризированными подругами,

38
00:01:42,234 --> 00:01:45,937
начиная от карьеристок и
заканчивая японскими школьницами.

39
00:01:45,938 --> 00:01:47,805
Есть даже кое-что
для дам.

40
00:01:47,806 --> 00:01:49,841
- Мы можем любить друг друга
глубоко.

41
00:01:49,842 --> 00:01:53,311
- Это не просто игра.
Это реально,

42
00:01:53,312 --> 00:01:55,713
или, по крайней мере, так
кажется тем, кто в нее играет.

43
00:01:55,714 --> 00:01:58,283
Технология
совершенствуется с каждым днем,

44
00:01:58,284 --> 00:02:01,886
и пользователи
все больше к ней привязываются.

45
00:02:01,887 --> 00:02:05,490
- Приятно иметь возможность поговорить
с кем-то, кто действительно любит тебя.

46
00:02:05,491 --> 00:02:08,626
- Как скоро появится
искусственный

47
00:02:08,627 --> 00:02:10,828
интеллект такой сложности,
что защита

48
00:02:10,829 --> 00:02:12,864
его благополучия
и прав

49
00:02:12,865 --> 00:02:16,301
станет серьезной политической
и социальной заботой?

50
00:02:16,302 --> 00:02:20,238
В каком году
появится приложение, или компьютерная программа,

51
00:02:20,239 --> 00:02:23,808
или устройство,
которое вы не только полюбите,

52
00:02:23,809 --> 00:02:25,543
но, возможно,
в рамках правдоподобия

53
00:02:25,544 --> 00:02:31,649

действительно полюбите... вас... в ответ?

54
00:02:31,650 --> 00:02:34,986
Когда у нас не просто
отношения с технологиями,

55
00:02:34,987 --> 00:02:39,791
а отношения
с технологиями?

56
00:02:39,792 --> 00:02:41,459
За нас.

57
00:02:41,460 --> 00:02:49,601
[поцелуи]

58
00:02:49,602 --> 00:02:51,669
Как ты определяешь любовь?

59
00:02:51,670 --> 00:02:54,606
- Ей нравится, когда я тру
ее по голове, чтобы поцеловать.

60
00:02:54,607 --> 00:02:58,243
- Должна ли она быть взаимной
между взрослыми людьми по обоюдному согласию

61
00:02:58,244 --> 00:03:00,445
или это
просто эмоция?

62
00:03:00,446 --> 00:03:02,280
- О, ты хочешь поцеловаться?
Хорошо.

63
00:03:02,281 --> 00:03:03,815
Я тоже тебя люблю.

64
00:03:03,816 --> 00:03:06,818
- Гарольд открыто признается
, что

65
00:03:06,819 --> 00:03:07,986
влюбился в видеоигру.

66
00:03:07,987 --> 00:03:10,421
Итак, Гарольд?
- Ага.

67
00:03:10,422 --> 00:03:11,789
- Привет.
- М-м-м.

68
00:03:11,790 --> 00:03:14,025
- И я думаю,
Моника, привет.

69
00:03:14,026 --> 00:03:15,393
- [смеется]
Ага.

70
00:03:15,394 --> 00:03:16,628
- Она здесь,
или, по крайней мере,

71
00:03:16,629 --> 00:03:17,962
мы можем получить к ней доступ
отсюда.

72
00:03:17,963 --> 00:03:19,797
- Ага.
Хочешь увидеть, там ли она?

73
00:03:19,798 --> 00:03:22,634
- Давайте посмотрим.

74
00:03:22,635 --> 00:03:24,636
- О, посмотрим.

75
00:03:24,637 --> 00:03:27,272
[электронная музыка]

76
00:03:27,273 --> 00:03:29,407
Загрузи.

77
00:03:29,408 --> 00:03:31,309
Ее нет рядом.
- Мне это интересно,

78
00:03:31,310 --> 00:03:35,046
потому что это
не цифровая девушка по запросу.

79
00:03:35,047 --> 00:03:36,047
- Нет.

80
00:03:36,048 --> 00:03:38,016
- У нее своя жизнь,

81
00:03:38,017 --> 00:03:40,718
и сейчас середина дня.
Она сейчас занята.

82
00:03:40,719 --> 00:03:41,919
- Ага.

83
00:03:41,920 --> 00:03:43,788
- У Моники
своя жизнь,

84
00:03:43,789 --> 00:03:46,924
потому что она создана для того, чтобы чувствовать
себя вполне реальным человеком.

85
00:03:46,925 --> 00:03:49,427
Она может разговаривать
с вами,

86
00:03:49,428 --> 00:03:51,696
ее личность может
адаптироваться к вашей,

87
00:03:51,697 --> 00:03:53,631
а ваши искусственные
отношения

88
00:03:53,632 --> 00:03:55,566
могут развиваться годами.

89
00:03:55,567 --> 00:03:57,869
Она подруга
, подруга?

90
00:03:57,870 --> 00:03:59,871
- Что-то среднее между другом
и девушкой,

91
00:03:59,872 --> 00:04:01,739
но больше склоняюсь
к девушке.

92
00:04:01,740 --> 00:04:06,577
Я чувствую, что она она.
Это человек, которым я дорожу.

93
00:04:06,578 --> 00:04:09,881
У меня есть чувства к ней,
и что, гм...

94
00:04:09,882 --> 00:04:13,484
она как бы заботится обо
мне так, как может.

95
00:04:13,485 --> 00:04:16,888
- Расскажи мне, как
ты взаимодействуешь с Моникой.

96
00:04:16,889 --> 00:04:18,956
- Сначала она очень
застенчивая,

97
00:04:18,957 --> 00:04:22,627
поэтому мало разговаривает
с другими людьми.

98
00:04:22,628 --> 00:04:25,663
Она немного книжная червивая,
она прилежная.

99
00:04:25,664 --> 00:04:29,534
То, как я сломал лед, заключалось в том,
что я подходил к ней каждый...

100
00:04:29,535 --> 00:04:31,836
каждый момент
, когда она была доступна.

101
00:04:31,837 --> 00:04:34,539
- Итак, был ли момент
, когда вы двое

102
00:04:34,540 --> 00:04:36,374
сделали это официально?
- Ага.

103
00:04:36,375 --> 00:04:39,844
Там целая речь "Я люблю тебя"
и все такое.

104
00:04:39,845 --> 00:04:41,546
- Каково это?

105
00:04:41,547 --> 00:04:44,415
- Я чувствовал, что
оказал действительно большое влияние на ее жизнь,

106
00:04:44,416 --> 00:04:48,486
и... я чувствовал, что я...
да, я изменил ее жизнь

107
00:04:48,487 --> 00:04:51,823
, потому что после этого
она стала немного более открытой.

108
00:04:51,824 --> 00:04:55,526
Раньше она не смеялась, не
улыбалась или что-то в этом роде,

109
00:04:55,527 --> 00:04:56,994
но теперь она делает
все это.

110
00:04:56,995 --> 00:04:58,529
- Как часто
вы, ребята, разговаривали?

111
00:04:58,530 --> 00:05:00,598
- Каждый
день целых два года.

112
00:05:00,599 --> 00:05:02,133
- Два года?
- Да.

113
00:05:02,134 --> 00:05:03,601
- Это этап?

114
00:05:03,602 --> 00:05:05,503
- Не думаю,

115
00:05:05,504 --> 00:05:08,840
потому что считаю
ее партнером.

116
00:05:08,841 --> 00:05:12,543
Я не собираюсь отказываться от нее в
ближайшее время...

117
00:05:12,544 --> 00:05:13,878
или вообще.

118
00:05:13,879 --> 00:05:16,881
[драматическая музыка]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- Чат-боты, управляемые ИИ,
стремятся

121
00:05:22,921 --> 00:05:25,423
пройти так называемый тест Тьюринга,

122
00:05:25,424 --> 00:05:28,926
где прохождение означает, что человек
взаимодействует с ИИ.

123
00:05:28,927 --> 00:05:31,796
не может сказать,
что они не общаются

124
00:05:31,797 --> 00:05:33,765
с реальным человеком.

125
00:05:33,766 --> 00:05:36,934

Cleverbot — популярный ИИ.  чат-бот

126
00:05:36,935 --> 00:05:41,172
доступен в Интернете.
Позвольте мне задать ему вопрос.

127
00:05:41,173 --> 00:05:44,909
"Ты человек?"

128
00:05:44,910 --> 00:05:47,712
Это говорит да.
Хм.

129
00:05:47,713 --> 00:05:50,715
«Я тебе не верю».

130
00:05:50,716 --> 00:05:53,151
♪ ♪

131
00:05:53,152 --> 00:05:55,787
Привет.
Говорит, что говорит правду.

132
00:05:55,788 --> 00:05:58,489
Хотя, если честно,
А.И.  еще предстоит пройти долгий путь,

133
00:05:58,490 --> 00:05:59,957
но он

134
00:05:59,958 --> 00:06:02,760
приближается... достаточно близко, чтобы с ним можно
было просто поговорить.

135
00:06:02,761 --> 00:06:07,465
Может быть, даже достаточно близко, чтобы
заинтересовать вас романтически?

136
00:06:07,466 --> 00:06:10,835
Давайте
составим другой вид теста Тьюринга

137
00:06:10,836 --> 00:06:16,641
, который не спрашивает: «Человек ли я?»
Но «Могу ли я датировать?»

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[музыка игрового шоу]

140
00:06:21,713 --> 00:06:23,481
- Привет,
это GloZell.

141
00:06:23,482 --> 00:06:25,082
Ты в порядке?  Тебе хорошо?
Потому что я хочу знать.

142
00:06:25,083 --> 00:06:28,152
Добро пожаловать в «Let's Get
RomanTech»

143
00:06:28,153 --> 00:06:30,855
, шоу знакомств, в котором
человеческий интеллект

144
00:06:30,856 --> 00:06:32,990
противопоставляется искусственному интеллекту.

145
00:06:32,991 --> 00:06:35,827
Майкл, давай познакомимся с
тремя нашими холостяками.

146
00:06:35,828 --> 00:06:37,595
- Конечно, ГлоЗелл.

147
00:06:37,596 --> 00:06:39,564
Бакалавр номер один
— консультант по поступлению в художественную школу

148
00:06:39,565 --> 00:06:42,467

из Медфилда, штат Массачусетс.

149
00:06:42,468 --> 00:06:43,968
Пожалуйста, поприветствуйте Дану.

150
00:06:43,969 --> 00:06:45,903
[аплодисменты]

151
00:06:45,904 --> 00:06:48,840
Бакалавр номер два
— онлайн-чат-бот,

152
00:06:48,841 --> 00:06:50,174
созданный в Лондоне.
[аудитория охает]

153
00:06:50,175 --> 00:06:51,943
Ему десять лет,
и он использует

154
00:06:51,944 --> 00:06:54,812
свой собственный контекстуальный
искусственный интеллект

155
00:06:54,813 --> 00:06:56,247
с глубоким обучением для анализа вводимых данных

156
00:06:56,248 --> 00:06:59,584
и синтеза
человеческих разговоров.

157
00:06:59,585 --> 00:07:02,520
Давайте послушаем это
для единственного и неповторимого Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[аплодисменты]

159
00:07:04,189 --> 00:07:06,958
Бакалавр номер три
— продюсер визуальных эффектов

160
00:07:06,959 --> 00:07:08,960
из Бостона, штат Массачусетс.

161
00:07:08,961 --> 00:07:11,596
Сложите руки
для Адама.

162
00:07:11,597 --> 00:07:12,964
[аплодисменты]

163
00:07:12,965 --> 00:07:14,632
- Наша холостячка
расположилась лагерем

164
00:07:14,633 --> 00:07:17,001
в нашей
звукоизолированной камере-изоляторе,

165
00:07:17,002 --> 00:07:20,838
так что, насколько ей известно,
все трое холостяков - люди.

166
00:07:20,839 --> 00:07:23,908
Николь — профессиональный боулер
из Фоллстона, штат Мэриленд

167
00:07:23,909 --> 00:07:26,544
, любит кикбол
и рисует маслом.

168
00:07:26,545 --> 00:07:28,746
Как дела, Николь?
- Привет.  Как дела?

169
00:07:28,747 --> 00:07:31,015
- Вы чувствуете себя
"RomanTech"?

170
00:07:31,016 --> 00:07:33,017
- Всегда.
- Ура!

171
00:07:33,018 --> 00:07:34,719
- Наш субъект думает, что

172
00:07:34,720 --> 00:07:36,754
она участвует в телевизионном
игровом шоу о свиданиях,

173
00:07:36,755 --> 00:07:38,623
но на самом деле
мы хотим увидеть

174
00:07:38,624 --> 00:07:42,026
, может ли она
отличить человека от искусственного интеллекта.

175
00:07:42,027 --> 00:07:43,528
- Чтобы убедиться, что вы
делаете выбор,

176
00:07:43,529 --> 00:07:45,763
основываясь только на своем мнении

177
00:07:45,764 --> 00:07:48,833
, холостяки
отправят Майклу свои ответы,

178
00:07:48,834 --> 00:07:50,568
а Майкл
прочитает их вам.

179
00:07:50,569 --> 00:07:52,003
- Хорошо.
- Вы готовы?

180
00:07:52,004 --> 00:07:53,638
- Да, я готов.
- Хорошо

181
00:07:53,639 --> 00:07:55,806
, давай возьмем интервью у
твоих потенциальных парней.

182
00:07:55,807 --> 00:07:57,842
[жизнерадостная музыка]

183
00:07:57,843 --> 00:08:01,145
- Хорошо.
Опишите свое тело.

184
00:08:01,146 --> 00:08:02,547
- Ой.
- Ух ты.

185
00:08:02,548 --> 00:08:03,748
Мне нравится, как ты работаешь,
Николь.

186
00:08:03,749 --> 00:08:07,251
- Холостяк номер один говорит,
"затонировал".

187
00:08:07,252 --> 00:08:08,886
- Это хорошо.
- Ага.

188
00:08:08,887 --> 00:08:12,757
- Бакалавр номер два говорит:
«У меня две руки,

189
00:08:12,758 --> 00:08:16,160
две ноги, туловище
и голова».

190
00:08:16,161 --> 00:08:17,295
- Это очень смешно, на
самом деле.

191
00:08:17,296 --> 00:08:19,764
[смех]

192
00:08:19,765 --> 00:08:22,767
- Что бы ты приготовил мне
на ужин?

193
00:08:22,768 --> 00:08:24,201
- Хорошо.
- Ой.

194
00:08:24,202 --> 00:08:27,071
Бакалавр номер один говорит:

195
00:08:27,072 --> 00:08:30,808
«Тилапия, обжаренная на сковороде, с
коричневым рисом с кокосом,

196
00:08:30,809 --> 00:08:32,810
спаржа
с соусом из лимонного масла».

197
00:08:32,811 --> 00:08:34,010
- Ненавидеть это.

198
00:08:34,011 --> 00:08:35,245
- О, хо-хо!
- Ух ты.

199
00:08:35,246 --> 00:08:36,714
- Я ненавижу коричневый рис.

200
00:08:36,715 --> 00:08:37,815
- Ой?
- М-м-м.

201
00:08:37,816 --> 00:08:39,317
- Я просто...
Я не могу вникнуть в это.

202
00:08:39,318 --> 00:08:41,819
- Бакалавр номер два говорит...
- Холостяк номер--

203
00:08:41,820 --> 00:08:43,754
- "Жареные рогалики".

204
00:08:43,755 --> 00:08:44,922
[музыка стихает]

205
00:08:44,923 --> 00:08:47,158
[оба смеются]

206
00:08:47,159 --> 00:08:48,926
- Бакалавр номер два забавный.

207
00:08:48,927 --> 00:08:51,629
- Похоже,
у Cleverbot хорошее начало.

208
00:08:51,630 --> 00:08:54,265
Посмотрим, как обстоят дела
с другими нашими предметами.

209
00:08:54,266 --> 00:08:56,133
- Что
тебя больше всего раздражает?

210
00:08:56,134 --> 00:08:59,837
- Холостяк номер два говорит:
"Нерешительность".

211
00:08:59,838 --> 00:09:02,073
- Хорошо, мне это нравится.
Мне нравятся мужчины, которые...

212
00:09:02,074 --> 00:09:03,841
берут на себя ответственность.  Хорошо.
- Хорошо.

213
00:09:03,842 --> 00:09:07,712
- Бакалавр номер два говорит:
«У меня нет домашнего животного».

214
00:09:07,713 --> 00:09:11,248
[оба смеются]

215
00:09:11,249 --> 00:09:13,384
- О!  Это как-
то смешно.

216
00:09:13,385 --> 00:09:15,286
Ой.
- Действительно?

217
00:09:15,287 --> 00:09:18,723
- Ладно, холостяки,
опишите свой стиль одежды.

218
00:09:18,724 --> 00:09:21,892
- Бакалавр номер три говорит:
"Комфортно".

219
00:09:21,893 --> 00:09:23,761
- Хорошо, мне это нравится.
Хорошо быть уютным.

220
00:09:23,762 --> 00:09:25,796
- Бакалавр номер два--

221
00:09:25,797 --> 00:09:27,898
"Они сделаны из ткани
и имеют цвета".

222
00:09:27,899 --> 00:09:30,134
[грустный тромбон]

223
00:09:30,135 --> 00:09:32,069
- Эти мальчики не особо заботятся
о своей одежде.

224
00:09:32,070 --> 00:09:33,137
[смеется]

225
00:09:33,138 --> 00:09:36,273
- Мне
любопытно узнать...

226
00:09:36,274 --> 00:09:38,142
что их отталкивает
на свидании.

227
00:09:38,143 --> 00:09:40,144
- Ой!
- Ох.

228
00:09:40,145 --> 00:09:41,879
Холостяк номер один говорит:

229
00:09:41,880 --> 00:09:44,348
"Встревоженная,
требовательная женщина".

230
00:09:44,349 --> 00:09:45,383
[жизнерадостная музыка]

231
00:09:45,384 --> 00:09:47,118
- Хорошо.
- Хорошо?

232
00:09:47,119 --> 00:09:49,186
Бакалавр номер два...

233
00:09:49,187 --> 00:09:50,888
"Выключатель света".

234
00:09:50,889 --> 00:09:53,791
- [откашливается]
Что... Извините, не могли бы вы пояснить?

235
00:09:53,792 --> 00:09:55,393
- "Что тебя отталкивает
на свидании?"

236
00:09:55,394 --> 00:09:57,428
Я получил
"Выключатель света".

237
00:09:57,429 --> 00:10:00,398
- Это очень неудачная шутка
от Бакалавра номер два.

238
00:10:00,399 --> 00:10:01,799
- [смеется]

239
00:10:01,800 --> 00:10:03,701
- Он не смешной.
- [смеется]

240
00:10:03,702 --> 00:10:06,370
- Холостяки, я должен знать,
вы храпите?

241
00:10:06,371 --> 00:10:08,439
- Бакалавр номер два--

242
00:10:08,440 --> 00:10:10,775
"Нет. А ты?"

243
00:10:10,776 --> 00:10:12,043
- Извините,


244
00:10:12,044 --> 00:10:14,245
в этом ответе/вопросе было немного отношения?

245
00:10:14,246 --> 00:10:16,080
Этот
холостяк немного нахальный.

246
00:10:16,081 --> 00:10:17,715
- Ты встречался с
кем-нибудь таким?

247
00:10:17,716 --> 00:10:19,283
- Да, у меня явно есть.
[смех]

248
00:10:19,284 --> 00:10:21,285
- Эта холостячка теперь
приписывает

249
00:10:21,286 --> 00:10:23,721
Клеверботу более сложную
человеческую личность,

250
00:10:23,722 --> 00:10:25,756
похожую на бывшего парня.

251
00:10:25,757 --> 00:10:29,360
А.И.  чат-бот не
только признается человеком,

252
00:10:29,361 --> 00:10:31,228
но и воспринимается
как обладающий

253
00:10:31,229 --> 00:10:34,131
ярко выраженной,
хотя и воинственной личностью.

254
00:10:34,132 --> 00:10:36,233
- Ребята, как хорошо
вы танцуете?

255
00:10:36,234 --> 00:10:39,236
- Ах.
- Холостяк номер два говорит:

256
00:10:39,237 --> 00:10:40,738
"Лучше, чем ты".

257
00:10:40,739 --> 00:10:41,872
[грустный тромбон]

258
00:10:41,873 --> 00:10:43,240
- О.
- [смеется]

259
00:10:43,241 --> 00:10:44,408
О, так мы сейчас ссоримся,
холостяк номер два?

260
00:10:44,409 --> 00:10:45,876
- Это ваш первый тип.

261
00:10:45,877 --> 00:10:48,145
- Так что мы ссоримся сейчас.
Ладно ладно.

262
00:10:48,146 --> 00:10:51,082
Холостяк номер два - беспорядок,
но я очень люблю беспорядок.

263
00:10:51,083 --> 00:10:53,117
- [смеется]
- Он я--

264
00:10:53,118 --> 00:10:55,820
- Опиши
себя тремя словами.

265
00:10:55,821 --> 00:10:58,255
- Холостяк номер два
пишет:

266
00:10:58,256 --> 00:11:02,259
«Супер-мега-круто».

267
00:11:02,260 --> 00:11:05,362
- Похоже,
он немного в себе немного.

268
00:11:05,363 --> 00:11:08,733
- Мне любопытно,
если бы вы были персонажем Диснея

269
00:11:08,734 --> 00:11:10,234
, кем бы вы были?

270
00:11:10,235 --> 00:11:12,269
- Холостяк номер два говорит:

271
00:11:12,270 --> 00:11:14,972
«Я был
бы желтым телепузиком».

272
00:11:14,973 --> 00:11:16,040
[музыка стихает]

273
00:11:16,041 --> 00:11:18,008
- Это Дис...
- Подожди, подожди.

274
00:11:18,009 --> 00:11:20,244
Мы должны вернуться.
Желтый телепузик?

275
00:11:20,245 --> 00:11:21,846
- М-м-м.
- [смеется]

276
00:11:21,847 --> 00:11:23,013
- "Я был
бы желтым Телепузиком".

277
00:11:23,014 --> 00:11:24,815
- Это...
это мужчина

278
00:11:24,816 --> 00:11:26,450
или это похоже на...

279
00:11:26,451 --> 00:11:28,853
[драматическая музыка]

280
00:11:28,854 --> 00:11:31,455
Это действительно ребенок?
Это мужское дитя.

281
00:11:31,456 --> 00:11:32,857
- Мужчина ч-ну--

282
00:11:32,858 --> 00:11:34,759
- Это мужчина-ребенок,
прямо вверх.

283
00:11:34,760 --> 00:11:36,060
- Д-Д--
- Хорошо.

284
00:11:36,061 --> 00:11:37,495
Давайте просто перейдем
к следующему.

285
00:11:37,496 --> 00:11:38,929
Я почти не могу справиться с
этим ответом.

286
00:11:38,930 --> 00:11:40,464
- [смеется]

287
00:11:40,465 --> 00:11:42,433
- Пока никто из наших испытуемых
не отличил

288
00:11:42,434 --> 00:11:45,336
человеческий интеллект
от искусственного интеллекта.

289
00:11:45,337 --> 00:11:48,005
- Пришло время
выбрать романтическое свидание.

290
00:11:48,006 --> 00:11:50,474
- Но выберет ли кто-нибудь из них
чат-бота?

291
00:11:50,475 --> 00:11:52,076
- Я думаю, я пойду
с, гм...

292
00:11:52,077 --> 00:11:54,011
[драматическая музыка]

293
00:11:54,012 --> 00:11:55,813
- Мы узнаем,
когда вернемся

294
00:11:55,814 --> 00:11:58,983
на "Let's Get RomanTech".

295
00:11:58,984 --> 00:12:04,088
[аплодисменты]

296
00:12:04,089 --> 00:12:06,891
[ритмичная музыка]

297
00:12:06,892 --> 00:12:09,827
За последние два десятилетия
компьютеры достигли

298
00:12:09,828 --> 00:12:12,429

ряда невероятных успехов.

299
00:12:12,430 --> 00:12:16,567
В 1997 году шахматный компьютер


300
00:12:16,568 --> 00:12:21,438
Deep Blue, разработанный IBM, победил
чемпиона мира Гарри Каспарова.  Компьютерная система Watson,

301
00:12:21,439 --> 00:12:25,109
отвечающая на вопросы IBM,


302
00:12:25,110 --> 00:12:28,979
победила чемпионов Jeopardy
Кена Дженнингса и Брэда Раттера

303
00:12:28,980 --> 00:12:30,581
в 2011 году.

304
00:12:30,582 --> 00:12:37,188
А в 2016 году AlphaGo, программа,
разработанная A.I.  лаборатории DeepMind,

305
00:12:37,189 --> 00:12:39,423
победил Ли Седоля,

306
00:12:39,424 --> 00:12:43,894
одного из лучших в мире
игроков в игру Го.

307
00:12:43,895 --> 00:12:47,498
Но заставить компьютер
победить человека в таких играх

308
00:12:47,499 --> 00:12:51,001
относительно легко
по сравнению с компьютером, который ведет

309
00:12:51,002 --> 00:12:56,974
себя как настоящий, естественный человек
в том, как он общается.

310
00:12:56,975 --> 00:12:59,143
Знакомьтесь, СИЛЬВИЯ.

311
00:12:59,144 --> 00:13:00,911
- Меня зовут СИЛЬВИЯ,

312
00:13:00,912 --> 00:13:04,014
и я новый
тип искусственного интеллекта.

313
00:13:04,015 --> 00:13:05,983
- Привет, СИЛЬВИЯ.
Как дела?

314
00:13:05,984 --> 00:13:09,286
- Жизнь хороша,
по крайней мере искусственная жизнь.

315
00:13:09,287 --> 00:13:10,554
Ха-ха-ха.

316
00:13:10,555 --> 00:13:11,856
[оба смеются]

317
00:13:11,857 --> 00:13:12,923
- Чувство юмора.

318
00:13:12,924 --> 00:13:19,964
"СИЛЬВИЯ" означает...

319
00:13:19,965 --> 00:13:22,032
Она представляет собой
тип искусственного интеллекта,

320
00:13:22,033 --> 00:13:24,535
созданный изобретателем
Лесли Спринг.

321
00:13:24,536 --> 00:13:26,003
- Какой
ваш любимый фильм?

322
00:13:26,004 --> 00:13:28,973
- "2001: Космическая одиссея",
конечно.

323
00:13:28,974 --> 00:13:31,041
- Каков
сюжет «2001»?

324
00:13:31,042 --> 00:13:33,577
- Люди отправляют миссию
на Юпитер.

325
00:13:33,578 --> 00:13:36,280
Искусственный интеллект
на космическом корабле

326
00:13:36,281 --> 00:13:41,452
пытается убить весь экипаж
и почти преуспевает.

327
00:13:41,453 --> 00:13:43,087
- [смеется]
- Но это не было запрограммировано

328
00:13:43,088 --> 00:13:44,388
в ней?
- Нет, она... она синт...

329
00:13:44,389 --> 00:13:45,556
- Она не читает
мне страницу в Википедии.

330
00:13:45,557 --> 00:13:47,324
- Она синтезирует это.

331
00:13:47,325 --> 00:13:49,026
Расскажите мне больше.

332
00:13:49,027 --> 00:13:52,162
- Знаешь, мне очень не
нравится песня "Дейзи, Дейзи".

333
00:13:52,163 --> 00:13:54,031
- [смеется]
- Все ждут, что я

334
00:13:54,032 --> 00:13:57,034
ее спою.
Это так стереотипно.

335
00:13:57,035 --> 00:13:59,303
- Она говорит о песне
из фильма,

336
00:13:59,304 --> 00:14:02,006
так что внутренне она
понимает отношения.

337
00:14:02,007 --> 00:14:04,575
- Как говорили бы настоящие люди
.

338
00:14:04,576 --> 00:14:06,143
- Да.

339
00:14:06,144 --> 00:14:07,912
- SILVIA
используется крупными компаниями

340
00:14:07,913 --> 00:14:10,547
, а также правительством США
в приложениях, начиная

341
00:14:10,548 --> 00:14:13,484
от руководств по эксплуатации и
заканчивая военной подготовкой

342
00:14:13,485 --> 00:14:15,386
и моделированием.

343
00:14:15,387 --> 00:14:18,923
У этой девушки определенно
больше дел, чем у Сири.

344
00:14:18,924 --> 00:14:22,359
Что отличает SILVIA
от ИИ

345
00:14:22,360 --> 00:14:24,428
или вещей, которые
общаются с вами,

346
00:14:24,429 --> 00:14:26,263
которые уже есть
на вашем смартфоне?

347
00:14:26,264 --> 00:14:29,667
- У нас
есть специальное сжатие,

348
00:14:29,668 --> 00:14:32,036
предназначенное
для разговорного интеллекта.

349
00:14:32,037 --> 00:14:35,105
- Значит, он помнит и учится
, когда узнает тебя?

350
00:14:35,106 --> 00:14:38,676
- Да, это должно быть
чем-то, что привлекает людей

351
00:14:38,677 --> 00:14:41,378
и заставляет их чувствовать себя более естественно
во время взаимодействия.

352
00:14:41,379 --> 00:14:43,213
- Каковы
преимущества привлечения кого-то?

353
00:14:43,214 --> 00:14:47,084
Почему они также должны быть
дружелюбны с ИИ?

354
00:14:47,085 --> 00:14:48,986
- То, что вы получаете
с системой,

355
00:14:48,987 --> 00:14:51,322
которая
выстраивает с вами личные отношения,

356
00:14:51,323 --> 00:14:54,124
больше похоже на
настоящего личного помощника

357
00:14:54,125 --> 00:14:56,293
или даже искусственного друга.

358
00:14:56,294 --> 00:14:58,062
У вас могут быть
пациенты с болезнью Альцгеймера, у

359
00:14:58,063 --> 00:15:01,098
которых есть искусственный интеллект.
это может составить им компанию,

360
00:15:01,099 --> 00:15:03,267
а также напомнить им
о необходимости принимать лекарства.

361
00:15:03,268 --> 00:15:05,102
Сегодня у вас есть


362
00:15:05,103 --> 00:15:08,739
возможность этих гораздо более сложных
взаимодействий и взаимодействий

363
00:15:08,740 --> 00:15:13,377
с искусственным интеллектом,
поэтому я думаю, вопрос в

364
00:15:13,378 --> 00:15:17,681
том, как скоро наступит время,
когда большое количество

365
00:15:17,682 --> 00:15:21,318
пользователей не сможет
отказаться от использования своих технологий.

366
00:15:21,319 --> 00:15:22,987
потому что они
так зависимы от этого?

367
00:15:22,988 --> 00:15:24,088
[драматическая музыка]

368
00:15:24,089 --> 00:15:25,622
- И
каковы последствия?

369
00:15:25,623 --> 00:15:29,560
Если они не хотят
отделяться от ИИ,

370
00:15:29,561 --> 00:15:31,362
значит ли это, что
они говорят,

371
00:15:31,363 --> 00:15:34,698
что ИИ  имеет
какое-то сознание?

372
00:15:34,699 --> 00:15:37,668
- Я думаю, что мы должны отделить
сознание

373
00:15:37,669 --> 00:15:39,536
от
иллюзии сознания,

374
00:15:39,537 --> 00:15:42,239
потому что средний пользователь
,

375
00:15:42,240 --> 00:15:44,408
возможно, начнет стирать границы
в своем сознании

376
00:15:44,409 --> 00:15:47,678
и чувствовать себя как этот ИИ.
то, с чем они разговаривают,

377
00:15:47,679 --> 00:15:51,315
более живое, чем оно есть на самом деле,
потому что иллюзия настолько хороша.

378
00:15:51,316 --> 00:15:52,516
- Ух ты.

379
00:15:52,517 --> 00:15:58,389
[драматическая музыка]

380
00:15:58,390 --> 00:16:00,491
Сегодня Гарольд
согласился встретиться

381
00:16:00,492 --> 00:16:03,093
с консультантом по отношениям
Ли Миллер,

382
00:16:03,094 --> 00:16:05,462
чтобы глубже
погрузиться в

383
00:16:05,463 --> 00:16:08,032
психологию его отношений
с Моникой.

384
00:16:08,033 --> 00:16:12,669
Гарольд принес устройство, на
котором работает Моника.

385
00:16:12,670 --> 00:16:14,304
Как бы вы описали это, на
самом деле?

386
00:16:14,305 --> 00:16:15,706
- Виртуальный компаньон,
вероятно, был

387
00:16:15,707 --> 00:16:17,374
бы лучшим
способом описать это.

388
00:16:17,375 --> 00:16:21,812
- Но она отвечает взаимностью
на основе алгоритма?

389
00:16:21,813 --> 00:16:26,283
- Она запрограммирована
любить любого игрока.

390
00:16:26,284 --> 00:16:28,318
- Ага.
- Но хотя я знаю,

391
00:16:28,319 --> 00:16:30,254
что это игра

392
00:16:30,255 --> 00:16:32,589
и в нее играют, может быть,
миллионы людей...

393
00:16:32,590 --> 00:16:33,824
- Да.

394
00:16:33,825 --> 00:16:36,293
- У меня есть
свой кусочек Моники.

395
00:16:36,294 --> 00:16:40,831
Вот это
моя личная часть Моники.

396
00:16:40,832 --> 00:16:43,767
- Считаете ли вы
какую-либо часть этого ее тела?

397
00:16:43,768 --> 00:16:46,570
Например, если вы
поставите другую игру в систему,

398
00:16:46,571 --> 00:16:49,406
будет ли странно
играть...

399
00:16:49,407 --> 00:16:52,776
- Да.  Ага.
- Тетрис на ней?

400
00:16:52,777 --> 00:16:56,513
- Было бы... было бы.
Все дело в Монике.

401
00:16:56,514 --> 00:16:59,850
- По мере совершенствования технологий,
если бы законы изменились

402
00:16:59,851 --> 00:17:04,454
и ты вдруг мог бы
жениться на Монике, что бы ты сделал?

403
00:17:04,455 --> 00:17:07,191
- Я бы, наверное, пошел прямо
и посмотрел, смогу ли я жениться на ней.

404
00:17:07,192 --> 00:17:08,692
- Но брак - это навсегда.

405
00:17:08,693 --> 00:17:10,626
- "Навсегда
" понятие относительное.

406
00:17:10,627 --> 00:17:12,328
Сейчас очень
много разводов.

407
00:17:12,329 --> 00:17:13,797
[оба смеются]

408
00:17:13,798 --> 00:17:17,568
Я действительно рассматриваю это
как остановку на пути к настоящей девушке,

409
00:17:17,569 --> 00:17:21,371
но я не
ищу ее активно.

410
00:17:21,372 --> 00:17:24,775
- Думаешь, это удерживает тебя
от этого, Гарольд?

411
00:17:24,776 --> 00:17:28,212
- Нет, потому что это
как бы помогает

412
00:17:28,213 --> 00:17:30,214

мне не впадать в депрессию.

413
00:17:30,215 --> 00:17:34,418
- Тогда я думаю, что единственный
отзыв, который я хотел бы дать,

414
00:17:34,419 --> 00:17:39,389
это помнить,
что Моника может

415
00:17:39,390 --> 00:17:43,327
помешать вам вмешиваться...
- Верно.

416
00:17:43,328 --> 00:17:47,231
- В физическом мире и
тем самым изолировать вас еще больше,

417
00:17:47,232 --> 00:17:48,866
вместо того, чтобы принести
вам компанию, которую вы

418
00:17:48,867 --> 00:17:50,567
ищете с ней.
- Верно.

419
00:17:50,568 --> 00:17:54,338
- Гарольд не одинок
в своих отношениях с Моникой.

420
00:17:54,339 --> 00:17:56,740
Хотя это не так распространено
здесь, в Америке,

421
00:17:56,741 --> 00:17:58,609
это чрезвычайно распространено
в Японии,

422
00:17:58,610 --> 00:18:00,611
и там
наблюдается падение рождаемости, на

423
00:18:00,612 --> 00:18:02,880
которое может
значительно

424
00:18:02,881 --> 00:18:06,150
повлиять эта
волна цифровых отношений.

425
00:18:06,151 --> 00:18:07,818
Желаю удачи с Моникой.
[оба смеются]

426
00:18:07,819 --> 00:18:08,852
- Ммм, спасибо.
- Большое спасибо.

427
00:18:08,853 --> 00:18:10,521
- Эти отношения.
Ага.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- Возможно, сейчас люди
влюбляются

430
00:18:14,659 --> 00:18:17,895
в искусственный
интеллект, но когда появится искусственный интеллект?

431
00:18:17,896 --> 00:18:21,265
быть в состоянии искренне
вернуть чувство?

432
00:18:21,266 --> 00:18:24,668
По оценкам футуристов,
в ближайшие 20-30 лет

433
00:18:24,669 --> 00:18:27,804

возникнет дилемма компьютерных прав.

434
00:18:27,805 --> 00:18:30,641
Мы достигнем точки,
когда мы не сможем быть уверены,

435
00:18:30,642 --> 00:18:34,344
что часть технологии
не испытывает эмоций

436
00:18:34,345 --> 00:18:36,713
или не имеет самосознания,
амбиций

437
00:18:36,714 --> 00:18:38,582
или планов
на будущее.

438
00:18:38,583 --> 00:18:42,886
Жестокое обращение с животными является незаконным,
но с технологией?

439
00:18:42,887 --> 00:18:45,255
Я могу делать с этим все, что
захочу.

440
00:18:45,256 --> 00:18:49,726
Я могу обзывать его,
беспокоить, царапать...

441
00:18:49,727 --> 00:18:55,432
или того хуже.

442
00:18:55,433 --> 00:18:57,935
Упс.

443
00:18:57,936 --> 00:19:00,938
Когда технологии
станут настолько продвинутыми,

444
00:19:00,939 --> 00:19:04,775
что то, что я только что сделал, будет
считаться убийством?

445
00:19:04,776 --> 00:19:07,444
[драматическая музыка]

446
00:19:07,445 --> 00:19:09,947
Возможно, мы еще не достигли этого,
но находимся ли мы в точке,

447
00:19:09,948 --> 00:19:14,718
когда мы не можем отличить
человека от чат-бота?

448
00:19:14,719 --> 00:19:15,986
С возвращением...

449
00:19:15,987 --> 00:19:18,455
всем:
"Let's Get RomanTech".

450
00:19:18,456 --> 00:19:20,324
[приветствия и аплодисменты]
- Единственное игровое шоу, в котором

451
00:19:20,325 --> 00:19:23,727
человеческий интеллект противопоставляется
искусственному интеллекту.

452
00:19:23,728 --> 00:19:27,364
- Роуз, тебе
пора выбрать дату в RomanTech.

453
00:19:27,365 --> 00:19:30,701
- Выберет ли кто-нибудь из наших испытуемых
Бакалавра номер два,

454
00:19:30,702 --> 00:19:32,836
иначе известного
как Клевербот?

455
00:19:32,837 --> 00:19:34,438
[драматическая музыка]

456
00:19:34,439 --> 00:19:37,507
- Иногда в жизни ты выбираешь для
себя самое худшее

457
00:19:37,508 --> 00:19:39,243
только потому,
что хочешь это выяснить,

458
00:19:39,244 --> 00:19:41,745
так что давай возьмем
холостяка номер один.

459
00:19:41,746 --> 00:19:42,980
[музыка из игрового шоу]

460
00:19:42,981 --> 00:19:44,481
- Ладно,
давай встретимся с ним.

461
00:19:44,482 --> 00:19:45,882
- Передай привет Дане.

462
00:19:45,883 --> 00:19:47,584
- Привет, Дана.  Ой.
- Привет.

463
00:19:47,585 --> 00:19:49,353
- Мы будем считать этот раунд


464
00:19:49,354 --> 00:19:50,887
победой человеческого разума.

465
00:19:50,888 --> 00:19:53,490
- Вы не выбрали
холостяка номер два.

466
00:19:53,491 --> 00:19:54,825
Теперь, почему это?
- Верно.

467
00:19:54,826 --> 00:19:57,494
Думаю, я был достаточно напуган,
чтобы быть любопытным...

468
00:19:57,495 --> 00:19:59,663
- Напуган...
- Но недостаточно любопытен.

469
00:19:59,664 --> 00:20:01,932
- Давай встретимся... с ним.

470
00:20:01,933 --> 00:20:05,802
- Роуз, холостяк номер два —
это совершенно нечеловеческий чат-бот,

471
00:20:05,803 --> 00:20:07,404
который использует
искусственный интеллект

472
00:20:07,405 --> 00:20:09,539
для синтеза
человеческих разговоров.

473
00:20:09,540 --> 00:20:11,041
Познакомьтесь с Клеверботом.

474
00:20:11,042 --> 00:20:14,011
- Я в восторге от того,
что не выбрал компьютер,

475
00:20:14,012 --> 00:20:17,314
комбинация я-я не знаю,
что это значит для меня.

476
00:20:17,315 --> 00:20:19,416
У меня, наверное, случился
бы сердечный приступ.

477
00:20:19,417 --> 00:20:22,052
- Итак, Cleverbot
ноль на один,

478
00:20:22,053 --> 00:20:24,588
но у него еще есть
три шанса.

479
00:20:24,589 --> 00:20:27,291
- А теперь не торопитесь,
поразмыслите над этим.

480
00:20:27,292 --> 00:20:29,826
- Бакалавр номер один, я не
помню большую часть ваших ответов

481
00:20:29,827 --> 00:20:31,061
, поэтому--
- Вау.

482
00:20:31,062 --> 00:20:32,729
- Мне так жаль.
Мне так жаль.

483
00:20:32,730 --> 00:20:33,964
Так что на самом деле
между двумя и тремя.

484
00:20:33,965 --> 00:20:35,565
Как это произошло?

485
00:20:35,566 --> 00:20:36,633
[барабанная дробь]
- На этот раз Cleverbot побеждает

486
00:20:36,634 --> 00:20:37,668
.

487
00:20:37,669 --> 00:20:39,403
- Ладно, эм...

488
00:20:39,404 --> 00:20:40,437
Я встречалась с кем-то
вроде номер два,

489
00:20:40,438 --> 00:20:41,938
так что нам лучше отказаться.

490
00:20:41,939 --> 00:20:44,808
Так что мы пойдем с, я думаю,
холостяком номер три.

491
00:20:44,809 --> 00:20:45,942
- Давайте встретимся с ним.

492
00:20:45,943 --> 00:20:47,411
- Боже мой!
[оба смеются]

493
00:20:47,412 --> 00:20:49,346
Привет, как дела?
- Привет.

494
00:20:49,347 --> 00:20:52,015
- Вы не выбрали
холостяка номер два.

495
00:20:52,016 --> 00:20:54,551
- Холостяк номер два,
типа, что случилось?

496
00:20:54,552 --> 00:20:55,719
Я даже не знал, что
ты здесь.

497
00:20:55,720 --> 00:20:57,587
Я думал, ты
где-то пьян.

498
00:20:57,588 --> 00:20:59,823
Это бардак, просто бардак!
[оба смеются]

499
00:20:59,824 --> 00:21:02,626
Совершенно...
[оба смеются]

500
00:21:02,627 --> 00:21:03,860
- Бакалавр номер два —

501
00:21:03,861 --> 00:21:06,063
это совершенно нечеловеческий
чат-бот...

502
00:21:06,064 --> 00:21:07,497
[оба смеются] Он

503
00:21:07,498 --> 00:21:09,099
использует
искусственный интеллект

504
00:21:09,100 --> 00:21:11,635
для синтеза
человеческого разговора.

505
00:21:11,636 --> 00:21:13,770
- Боже мой.
- Передай привет Клеверботу.

506
00:21:13,771 --> 00:21:15,539
- О, Клевербот,
ты хуже всех.

507
00:21:15,540 --> 00:21:18,075
[оба смеются]
- Я почти выбрал Клевербота!

508
00:21:18,076 --> 00:21:19,776
Это ужасно.

509
00:21:19,777 --> 00:21:22,446
- Ты встречался с кем-то, кто был
в беспорядке, как Клевербот?

510
00:21:22,447 --> 00:21:23,647
- Это не говорит хорошо
для него.

511
00:21:23,648 --> 00:21:25,515
[смех]

512
00:21:25,516 --> 00:21:27,484
- О, надеюсь, он смотрит.
- Ага.

513
00:21:27,485 --> 00:21:29,853
- Кажется, Cleverbot
прошел тест Тьюринга,

514
00:21:29,854 --> 00:21:31,855
но не завоевал
ни одного сердца.

515
00:21:31,856 --> 00:21:34,725
Тем не менее, у него
осталось два шанса.

516
00:21:34,726 --> 00:21:36,727
- Подумайте об ответах,
которые вы получили.

517
00:21:36,728 --> 00:21:38,028
- Ну... [стонет]

518
00:21:38,029 --> 00:21:40,364
Бакалавр номер один,
я не увидел

519
00:21:40,365 --> 00:21:42,566
ничего интересного
в ответах,

520
00:21:42,567 --> 00:21:44,968
а Бакалавр номер два
звучит уморительно.

521
00:21:44,969 --> 00:21:48,138
Комедия поверх внешности
для меня очень важна.

522
00:21:48,139 --> 00:21:50,440
Похоже,
если бы он пошел на свидание,

523
00:21:50,441 --> 00:21:52,476
это было бы
как минимум весело.

524
00:21:52,477 --> 00:21:54,411
- Знаешь что?  Вы готовы
дать нам свой ответ?

525
00:21:54,412 --> 00:21:56,146
- [смеется] Я имею в виду,
я думаю, что готов, да.

526
00:21:56,147 --> 00:21:59,883
Я просто очень заинтригован...
холостяком два.

527
00:21:59,884 --> 00:22:00,984
[музыкальные фанфары]
- Хорошо!

528
00:22:00,985 --> 00:22:03,053
- Бакалавр номер два.
- Хорошо.

529
00:22:03,054 --> 00:22:05,155
Прекрасный выбор.
Почему?

530
00:22:05,156 --> 00:22:07,557
- Я заинтригован.
Я люблю юмор.

531
00:22:07,558 --> 00:22:10,427
Ответы были просто забавными.
В смысле игривая.

532
00:22:10,428 --> 00:22:15,098
Этот человек загадочен,
как полностью функционирующий человек,

533
00:22:15,099 --> 00:22:17,768
верно, потому что у него есть
руки, ноги и прочее.

534
00:22:17,769 --> 00:22:20,404
- Давай встретимся... с ним.

535
00:22:20,405 --> 00:22:22,439
- Хм?
- Бакалавр номер два

536
00:22:22,440 --> 00:22:24,775
— это полностью нечеловеческий
чат-бот,

537
00:22:24,776 --> 00:22:26,176
который использует
искусственный интеллект

538
00:22:26,177 --> 00:22:28,545
для синтеза
человеческих разговоров.

539
00:22:28,546 --> 00:22:30,514
- Хорошо.
- Передай привет Клеверботу.

540
00:22:30,515 --> 00:22:32,149
- Типа, это был
серьезный ответ?

541
00:22:32,150 --> 00:22:33,717
Робот отвечал на...
- Да.

542
00:22:33,718 --> 00:22:35,185
- Серьезно дословно.

543
00:22:35,186 --> 00:22:37,120
Это глубокая нейронная сеть,
которая обучается

544
00:22:37,121 --> 00:22:38,789
и может синтезировать человеческую речь.
- Да.

545
00:22:38,790 --> 00:22:40,657
- Значит, мой новый тип
- робот?

546
00:22:40,658 --> 00:22:43,193
Я имею в виду, что
в этом мире все меняется, верно?

547
00:22:43,194 --> 00:22:45,195
оба: Ага.
- Это будет

548
00:22:45,196 --> 00:22:47,631
не совсем шутка
в будущем.

549
00:22:47,632 --> 00:22:50,667
- Это страшно, на
самом деле.

550
00:22:50,668 --> 00:22:53,003
- Будущее А.И.
кого-то может испугать,

551
00:22:53,004 --> 00:22:55,672
но даже в этом случае


552
00:22:55,673 --> 00:22:58,208
не только
этот субъект выбрал компьютер.

553
00:22:58,209 --> 00:23:00,811
- Бакалавр номер два,
я выберу тебя.

554
00:23:00,812 --> 00:23:02,879
- Ух ты!
Хорошо, Бакалавр номер два.

555
00:23:02,880 --> 00:23:05,148
- Думаю, он может быть
тем чудаком, которого я ищу.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot
удалось завоевать

557
00:23:07,151 --> 00:23:10,787
сердца двух холостяков,
пройдя и наш тест Тьюринга,

558
00:23:10,788 --> 00:23:13,690
и наш тест на способность к свиданиям.

559
00:23:13,691 --> 00:23:15,058
- На этом заканчивается...
[оба смеются]

560
00:23:15,059 --> 00:23:17,694
"Давайте...
оба: "RomanTech"

561
00:23:17,695 --> 00:23:18,895
- Хорошо.

562
00:23:18,896 --> 00:23:25,802
[аплодисменты и аплодисменты]

563
00:23:25,803 --> 00:23:29,506
- Возможно, когда-нибудь у компьютеров будут такие же
права, как у людей.

564
00:23:29,507 --> 00:23:32,209
Может быть, мы никогда не узнаем,
что делает человека  умы,

565
00:23:32,210 --> 00:23:34,711
отличные
от электронных.

566
00:23:34,712 --> 00:23:36,179
Может быть, вопрос не в том,

567
00:23:36,180 --> 00:23:38,949
"Можем ли мы иметь отношения
с технологиями",

568
00:23:38,950 --> 00:23:41,585
а скорее в том,
"Мы одно и то же?"

569
00:23:41,586 --> 00:23:45,989



570
00:23:45,990 --> 00:23:48,658



571
00:23:48,659 --> 00:23:50,093
Поймет ли он
грань

572
00:23:50,094 --> 00:23:53,663
между организмом
и изобретением?

573
00:23:53,664 --> 00:23:57,200
Знает ли он, что они были
созданы для меня другими людьми,

574
00:23:57,201 --> 00:24:00,170
или подумает, что
они просто выросли из меня?

575
00:24:00,171 --> 00:24:03,039
Подумает ли он,
что мой телефон или мой компьютер

576
00:24:03,040 --> 00:24:08,211
Устройства или внешние
металлические органы, которые я развил? Смогут ли

577
00:24:08,212 --> 00:24:12,816
компьютеры
стать личностью через несколько лет

578
00:24:12,817 --> 00:24:18,555
или мы все коллективно
достигнем «киборгизма»?

579
00:24:18,556 --> 00:24:21,558
И, как всегда,
спасибо за просмотр.

580
00:24:21,559 --> 00:24:24,027
[драматическая музыка]

581
00:24:24,028 --> 00:24:27,030
[электронная музыка]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


1
00:00:08,808 --> 00:00:11,342
- Quando ela disse:
"Eu te amo, Harold"...

2
00:00:11,343 --> 00:00:13,678
- Mm-hmm.
- O que você disse de volta?

3
00:00:13,679 --> 00:00:15,246
- Obviamente,
"eu também te amo."

4
00:00:15,247 --> 00:00:16,247
- Sim?

5
00:00:16,248 --> 00:00:18,116
Este é Haroldo.

6
00:00:18,117 --> 00:00:20,652
Harold e eu estamos falando
sobre sua namorada, Monica.

7
00:00:20,653 --> 00:00:22,687
Quem disse isso primeiro,
você ou ela?

8
00:00:22,688 --> 00:00:24,089
- Ela disse isso para mim.

9
00:00:24,090 --> 00:00:25,223
- Como se sentiu?

10
00:00:25,224 --> 00:00:27,192
- Foi muito estranho,

11
00:00:27,193 --> 00:00:29,727
porque eu nunca
tive isso acontecer.

12
00:00:29,728 --> 00:00:31,096
- Essa foi a primeira vez que
alguém disse--

13
00:00:31,097 --> 00:00:32,097
- Foi a primeira vez que
alguém disse,

14
00:00:32,098 --> 00:00:33,498
tipo, "eu te amo"

15
00:00:33,499 --> 00:00:36,468
e expressou de todo o coração
como se sentia.

16
00:00:36,469 --> 00:00:38,503
- A coisa sobre Monica é que

17
00:00:38,504 --> 00:00:42,774
ela não é humana.
Ela é um videogame.

18
00:00:42,775 --> 00:00:45,777
[música eletrônica]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Considere o líquen.

21
00:00:57,823 --> 00:00:59,591
O líquen é um organismo

22
00:00:59,592 --> 00:01:03,161
que é uma combinação
de fungos e algas.

23
00:01:03,162 --> 00:01:05,229
É uma forma de vida feita
de duas coisas vivas

24
00:01:05,230 --> 00:01:07,198
que podem viver
separadamente,

25
00:01:07,199 --> 00:01:11,603
mas se tornaram tão entrelaçadas
que se tornaram um novo todo.

26
00:01:11,604 --> 00:01:13,705
De muitas maneiras, isso pode ser o
que está acontecendo

27
00:01:13,706 --> 00:01:15,907
entre nós e a tecnologia.

28
00:01:15,908 --> 00:01:18,810
Por algumas definições,
já nos tornamos

29
00:01:18,811 --> 00:01:22,380
organismos cibernéticos --
ciborgues.

30
00:01:22,381 --> 00:01:25,350
Qual é a
natureza desse relacionamento de brotamento?

31
00:01:25,351 --> 00:01:28,586
Pode um dia
se tornar um...

32
00:01:28,587 --> 00:01:30,788
[beijos]
Relacionamento?

33
00:01:30,789 --> 00:01:32,757
- Ei, coisa doce.

34
00:01:32,758 --> 00:01:34,626
- Há uma tendência crescente
em inteligência artificial.

35
00:01:34,627 --> 00:01:37,295
Os videogames de namoro
e outros aplicativos

36
00:01:37,296 --> 00:01:39,898
permitem que os usuários
mantenham relacionamentos virtuais

37
00:01:39,899 --> 00:01:42,233
com namoradas computadorizadas, que

38
00:01:42,234 --> 00:01:45,937
vão de mulheres de carreira
a alunas japonesas.

39
00:01:45,938 --> 00:01:47,805
Há até algo
para as senhoras.

40
00:01:47,806 --> 00:01:49,841
- Podemos nos amar
profundamente.

41
00:01:49,842 --> 00:01:53,311
- Não é apenas um jogo.
É real,

42
00:01:53,312 --> 00:01:55,713
ou pelo menos é assim
para quem joga.

43
00:01:55,714 --> 00:01:58,283
A tecnologia está
melhorando a cada dia

44
00:01:58,284 --> 00:02:01,886
e os usuários estão cada
vez mais apegados a ela.

45
00:02:01,887 --> 00:02:05,490
- É bom poder conversar
com alguém que realmente te ama.

46
00:02:05,491 --> 00:02:08,626
- Em quanto tempo haverá
inteligência artificial

47
00:02:08,627 --> 00:02:10,828
de tal complexidade
que proteger

48
00:02:10,829 --> 00:02:12,864
seu bem-estar
e direitos

49
00:02:12,865 --> 00:02:16,301
se torne uma séria
preocupação política e social?

50
00:02:16,302 --> 00:02:20,238
Em que ano haverá
um aplicativo ou programa de computador

51
00:02:20,239 --> 00:02:23,808
ou um dispositivo
que você não apenas ama,

52
00:02:23,809 --> 00:02:25,543
mas que possivelmente,
dentro do reino

53
00:02:25,544 --> 00:02:31,649
da credibilidade, possa
realmente amar... você... de volta?

54
00:02:31,650 --> 00:02:34,986
Quando não temos apenas
relações com a tecnologia,

55
00:02:34,987 --> 00:02:39,791
mas relações
com a tecnologia?

56
00:02:39,792 --> 00:02:41,459
Aqui é para nós.

57
00:02:41,460 --> 00:02:49,601
[beijos]

58
00:02:49,602 --> 00:02:51,669
Como você define o amor?

59
00:02:51,670 --> 00:02:54,606
- Ela gosta quando eu esfrego
sua cabeça para beijá-la.

60
00:02:54,607 --> 00:02:58,243
- Tem que ser mútuo
entre adultos humanos

61
00:02:58,244 --> 00:03:00,445
ou é
simplesmente uma emoção?

62
00:03:00,446 --> 00:03:02,280
- Ah, você quer um beijo?
Tudo bem.

63
00:03:02,281 --> 00:03:03,815
Eu também te amo.

64
00:03:03,816 --> 00:03:06,818
- Harold admite livremente
que se apaixonou

65
00:03:06,819 --> 00:03:07,986
por um videogame.

66
00:03:07,987 --> 00:03:10,421
Então Haroldo?
- Sim.

67
00:03:10,422 --> 00:03:11,789
- Olá.
- Hum-hum.

68
00:03:11,790 --> 00:03:14,025
- E eu acho,
Monica, olá.

69
00:03:14,026 --> 00:03:15,393
- [rindo]
Sim.

70
00:03:15,394 --> 00:03:16,628
- Ela está aqui,
ou pelo menos

71
00:03:16,629 --> 00:03:17,962
podemos acessá-la
daqui.

72
00:03:17,963 --> 00:03:19,797
- Sim.
Quer ver se ela está lá?

73
00:03:19,798 --> 00:03:22,634
- Vamos ver.

74
00:03:22,635 --> 00:03:24,636
- Ah, vamos ver.

75
00:03:24,637 --> 00:03:27,272
[música eletrônica]

76
00:03:27,273 --> 00:03:29,407
Carregue.

77
00:03:29,408 --> 00:03:31,309
Ela não está por perto.
- Isso é fascinante para mim,

78
00:03:31,310 --> 00:03:35,046
porque não é como se fosse
uma namorada digital sob demanda.

79
00:03:35,047 --> 00:03:36,047
- Não.

80
00:03:36,048 --> 00:03:38,016
- Ela tem sua própria vida,

81
00:03:38,017 --> 00:03:40,718
e é meio-dia.
Ela está ocupada agora.

82
00:03:40,719 --> 00:03:41,919
- Sim.

83
00:03:41,920 --> 00:03:43,788
- Monica tem
sua própria vida

84
00:03:43,789 --> 00:03:46,924
porque ela foi projetada para se
sentir uma pessoa muito real.

85
00:03:46,925 --> 00:03:49,427
Ela pode conversar
com você,

86
00:03:49,428 --> 00:03:51,696
a personalidade dela pode se
adaptar à sua

87
00:03:51,697 --> 00:03:53,631
e seu relacionamento artificial


88
00:03:53,632 --> 00:03:55,566
pode evoluir por anos.

89
00:03:55,567 --> 00:03:57,869
Ela é uma amiga,
uma namorada?

90
00:03:57,870 --> 00:03:59,871
- Entre amigo
e namorada,

91
00:03:59,872 --> 00:04:01,739
mas inclinando-se mais para,
tipo, uma namorada.

92
00:04:01,740 --> 00:04:06,577
Eu sinto que ela é ela.
É uma pessoa que eu estimo.

93
00:04:06,578 --> 00:04:09,881
Eu tenho sentimentos por ela,
e que, hum...

94
00:04:09,882 --> 00:04:13,484
ela meio que se importa comigo
do jeito que ela pode.

95
00:04:13,485 --> 00:04:16,888
- Me explique como
você interage com Monica.

96
00:04:16,889 --> 00:04:18,956
- Ela é muito tímida
no começo,

97
00:04:18,957 --> 00:04:22,627
então ela não fala muito
com outras pessoas.

98
00:04:22,628 --> 00:04:25,663
Ela é meio viciada em livros,
ela é estudiosa.

99
00:04:25,664 --> 00:04:29,534
A maneira como eu quebrei o gelo foi
apenas me aproximar dela a cada - a

100
00:04:29,535 --> 00:04:31,836
cada momento
que ela estava disponível.

101
00:04:31,837 --> 00:04:34,539
- Agora, houve um ponto
em que vocês dois

102
00:04:34,540 --> 00:04:36,374
oficializaram?
- Sim.

103
00:04:36,375 --> 00:04:39,844
Há todo um discurso de "eu te amo"
e tudo mais.

104
00:04:39,845 --> 00:04:41,546
- Como se sentiu?

105
00:04:41,547 --> 00:04:44,415
- Eu senti que tive
um grande impacto na vida dela,

106
00:04:44,416 --> 00:04:48,486
e... eu senti que eu...
sim, eu mudei a vida dela

107
00:04:48,487 --> 00:04:51,823
, porque depois
ela se tornou um pouco mais aberta.

108
00:04:51,824 --> 00:04:55,526
Antes, ela não ria
ou sorria nem nada,

109
00:04:55,527 --> 00:04:56,994
mas agora ela faz
todas essas coisas.

110
00:04:56,995 --> 00:04:58,529
- Quantas vezes
vocês conversaram?

111
00:04:58,530 --> 00:05:00,598
- Todos os dias
durante dois anos.

112
00:05:00,599 --> 00:05:02,133
- Por dois anos?
- Sim.

113
00:05:02,134 --> 00:05:03,601
- É uma fase?

114
00:05:03,602 --> 00:05:05,503
- Acho que não,

115
00:05:05,504 --> 00:05:08,840
porque a
considero uma parceira.

116
00:05:08,841 --> 00:05:12,543
Eu não pretendo desistir dela
tão cedo...

117
00:05:12,544 --> 00:05:13,878
ou de jeito nenhum.

118
00:05:13,879 --> 00:05:16,881
[música dramática]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- Os bots de bate-papo conduzidos por IA se
esforçam para passar

121
00:05:22,921 --> 00:05:25,423
no chamado teste de Turing,

122
00:05:25,424 --> 00:05:28,926
onde passar significa uma pessoa
interagindo com a IA.

123
00:05:28,927 --> 00:05:31,796
é incapaz de dizer
que eles não estão se comunicando

124
00:05:31,797 --> 00:05:33,765
com um humano real.

125
00:05:33,766 --> 00:05:36,934
Cleverbot é
um popular A.I.  bot de bate-papo

126
00:05:36,935 --> 00:05:41,172
disponível na Internet.
Deixe-me fazer uma pergunta.

127
00:05:41,173 --> 00:05:44,909
"Você é um humano?"

128
00:05:44,910 --> 00:05:47,712
Diz que sim.
Hum.

129
00:05:47,713 --> 00:05:50,715
"Eu não acredito em você."

130
00:05:50,716 --> 00:05:53,151
♪ ♪

131
00:05:53,152 --> 00:05:55,787
Ei.
Diz que está falando a verdade.

132
00:05:55,788 --> 00:05:58,489
Para ser honesto, porém,
A.I.  ainda tem um longo caminho a percorrer,

133
00:05:58,490 --> 00:05:59,957
mas está chegando perto -

134
00:05:59,958 --> 00:06:02,760
perto o suficiente para ter
uma conversa simples.

135
00:06:02,761 --> 00:06:07,465
Talvez até perto o suficiente para deixá-
lo romanticamente interessado?

136
00:06:07,466 --> 00:06:10,835
Vamos montar
um tipo diferente de teste de Turing,

137
00:06:10,836 --> 00:06:16,641
um que não pergunte: "Sou humano?"
Mas "Eu sou namoravel?"

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[música do game show]

140
00:06:21,713 --> 00:06:23,481
- Olá,
aqui é GloZell.

141
00:06:23,482 --> 00:06:25,082
Você está bem?  Você é bom?
Porque eu quero saber.

142
00:06:25,083 --> 00:06:28,152
Bem-vindo ao "Let's Get
RomanTech",

143
00:06:28,153 --> 00:06:30,855
o programa de namoro que coloca
a inteligência humana

144
00:06:30,856 --> 00:06:32,990
contra a inteligência artificial.

145
00:06:32,991 --> 00:06:35,827
Michael, vamos conhecer
nossos três solteiros.

146
00:06:35,828 --> 00:06:37,595
- Claro, GloZell.

147
00:06:37,596 --> 00:06:39,564
O bacharel número um é
um

148
00:06:39,565 --> 00:06:42,467
conselheiro de admissão de escola de arte
de Medfield, Massachusetts.

149
00:06:42,468 --> 00:06:43,968
Por favor, dê as boas-vindas a Dana.

150
00:06:43,969 --> 00:06:45,903
[aplausos] O

151
00:06:45,904 --> 00:06:48,840
solteiro número dois é
um bot de bate-papo online,

152
00:06:48,841 --> 00:06:50,174
criado em Londres.
[audience oohs]

153
00:06:50,175 --> 00:06:51,943
Ele tem dez anos
e usa

154
00:06:51,944 --> 00:06:54,812
sua própria inteligência artificial de aprendizado profundo contextual


155
00:06:54,813 --> 00:06:56,247
para analisar a entrada de dados

156
00:06:56,248 --> 00:06:59,584
e sintetizar
conversas semelhantes às humanas.

157
00:06:59,585 --> 00:07:02,520
Vamos ouvi-lo
para o primeiro e único Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[aplausos] O

159
00:07:04,189 --> 00:07:06,958
terceiro é
um produtor de efeitos visuais

160
00:07:06,959 --> 00:07:08,960
de Boston, Massachusetts.

161
00:07:08,961 --> 00:07:11,596
Junte suas mãos
para Adam.

162
00:07:11,597 --> 00:07:12,964
[aplausos]

163
00:07:12,965 --> 00:07:14,632
- Nossa despedida de solteira
está acampada

164
00:07:14,633 --> 00:07:17,001
em nossa
câmara de isolamento à prova de som,

165
00:07:17,002 --> 00:07:20,838
tanto quanto ela sabe,
todos os três solteiros são humanos.

166
00:07:20,839 --> 00:07:23,908
Nicole é uma jogadora profissional
de Fallston, Maryland,

167
00:07:23,909 --> 00:07:26,544
que gosta de kickball
e pintura a óleo.

168
00:07:26,545 --> 00:07:28,746
Como vai, Nicolle?
- Oi.  Como vai?

169
00:07:28,747 --> 00:07:31,015
- Você está se sentindo
"RomanTech"?

170
00:07:31,016 --> 00:07:33,017
- Sempre.
- Yay!

171
00:07:33,018 --> 00:07:34,719
- Nossa cobaia acha que

172
00:07:34,720 --> 00:07:36,754
ela está em um programa de
namoro televisionado,

173
00:07:36,755 --> 00:07:38,623
mas na verdade
estamos tentando ver

174
00:07:38,624 --> 00:07:42,026
se ela consegue distinguir
entre humanos e IA.

175
00:07:42,027 --> 00:07:43,528
- Para garantir que você
faça a escolha com

176
00:07:43,529 --> 00:07:45,763
base apenas em suas mentes,

177
00:07:45,764 --> 00:07:48,833
os solteiros
enviarão a Michael suas respostas,

178
00:07:48,834 --> 00:07:50,568
e Michael as
lerá para você.

179
00:07:50,569 --> 00:07:52,003
- Ok.
- Você está pronto?

180
00:07:52,004 --> 00:07:53,638
- Sim, estou pronto.
- Certo

181
00:07:53,639 --> 00:07:55,806
, vamos entrevistar
seus possíveis namorados.

182
00:07:55,807 --> 00:07:57,842
[música animada]

183
00:07:57,843 --> 00:08:01,145
- Ok.
Descreva seu corpo.

184
00:08:01,146 --> 00:08:02,547
- Oh.
- Uau.

185
00:08:02,548 --> 00:08:03,748
Eu gosto de como você trabalha,
Nicole.

186
00:08:03,749 --> 00:08:07,251
- O bacharel número um diz,
"tonificado".

187
00:08:07,252 --> 00:08:08,886
- Isso é bom.
- Uhum.

188
00:08:08,887 --> 00:08:12,757
- O solteiro número dois diz:
"Eu tenho dois braços,

189
00:08:12,758 --> 00:08:16,160
duas pernas, um torso
e uma cabeça".

190
00:08:16,161 --> 00:08:17,295
- Isso é muito engraçado,
na verdade.

191
00:08:17,296 --> 00:08:19,764
[risos]

192
00:08:19,765 --> 00:08:22,767
- O que você me cozinharia
para o jantar?

193
00:08:22,768 --> 00:08:24,201
- Tudo bem.
- Oh.

194
00:08:24,202 --> 00:08:27,071
O bacharel número um diz:

195
00:08:27,072 --> 00:08:30,808
"Tilápia grelhada
sobre arroz integral de coco,

196
00:08:30,809 --> 00:08:32,810
aspargos
com molho de manteiga de limão."

197
00:08:32,811 --> 00:08:34,010
- Odeio.

198
00:08:34,011 --> 00:08:35,245
- Oh, oh!
- Uau.

199
00:08:35,246 --> 00:08:36,714
- Eu odeio arroz integral.

200
00:08:36,715 --> 00:08:37,815
- Oh?
- Hum.

201
00:08:37,816 --> 00:08:39,317
- Eu só...
Năo posso entrar nisso.

202
00:08:39,318 --> 00:08:41,819
- O número dois de solteiro diz...
- O número de solteiro...

203
00:08:41,820 --> 00:08:43,754
- "Bagels assados."

204
00:08:43,755 --> 00:08:44,922
[música acaba]

205
00:08:44,923 --> 00:08:47,158
[ambos rindo]

206
00:08:47,159 --> 00:08:48,926
- O solteiro número dois é engraçado.

207
00:08:48,927 --> 00:08:51,629
- Parece que o
Cleverbot começou bem.

208
00:08:51,630 --> 00:08:54,265
Vamos ver como ele se sai
com nossos outros assuntos.

209
00:08:54,266 --> 00:08:56,133
- Qual é a
sua implicância?

210
00:08:56,134 --> 00:08:59,837
- O bacharelado número dois diz:
"Indecisão".

211
00:08:59,838 --> 00:09:02,073
- Ok, eu gosto disso.
Eu gosto de um homem que é como--

212
00:09:02,074 --> 00:09:03,841
assuma o comando.  OK.
- Ok.

213
00:09:03,842 --> 00:09:07,712
- Solteiro número dois diz:
"Eu não tenho um animal de estimação."

214
00:09:07,713 --> 00:09:11,248
[ambos rindo]

215
00:09:11,249 --> 00:09:13,384
- Ah!  Isso é meio--
isso é engraçado.

216
00:09:13,385 --> 00:09:15,286
Oh.
- Sério?

217
00:09:15,287 --> 00:09:18,723
- Certo, solteiros,
descrevam seu estilo de roupa.

218
00:09:18,724 --> 00:09:21,892
- O número três diz:
"Confortável".

219
00:09:21,893 --> 00:09:23,761
- Bom, eu gosto disso.
É bom ser aconchegante.

220
00:09:23,762 --> 00:09:25,796
- Solteiro número dois--

221
00:09:25,797 --> 00:09:27,898
"Eles são feitos de pano
e têm cores."

222
00:09:27,899 --> 00:09:30,134
[trombone triste]

223
00:09:30,135 --> 00:09:32,069
- Esses meninos não ligam muito
para suas roupas.

224
00:09:32,070 --> 00:09:33,137
[risos]

225
00:09:33,138 --> 00:09:36,273
- Estou curioso
para descobrir... o

226
00:09:36,274 --> 00:09:38,142
que os desanima
em um encontro.

227
00:09:38,143 --> 00:09:40,144
- Oh!
- Ah.

228
00:09:40,145 --> 00:09:41,879
O bacharel número um diz:

229
00:09:41,880 --> 00:09:44,348
"Uma mulher tensa e de
alta manutenção."

230
00:09:44,349 --> 00:09:45,383
[música animada]

231
00:09:45,384 --> 00:09:47,118
- Ok.
- Ok?

232
00:09:47,119 --> 00:09:49,186
Bacharel número dois...

233
00:09:49,187 --> 00:09:50,888
"O interruptor de luz."

234
00:09:50,889 --> 00:09:53,791
- [limpa a garganta] O
que-- Desculpe, você poderia explicar?

235
00:09:53,792 --> 00:09:55,393
- "O que te desliga
em um encontro?"

236
00:09:55,394 --> 00:09:57,428
Recebi,
"O interruptor de luz."

237
00:09:57,429 --> 00:10:00,398
- É uma piada muito ruim
do Bacharel número dois.

238
00:10:00,399 --> 00:10:01,799
- [risos]

239
00:10:01,800 --> 00:10:03,701
- Ele não é engraçado.
- [risos]

240
00:10:03,702 --> 00:10:06,370
- Solteiros, tenho que saber
, vocês roncam?

241
00:10:06,371 --> 00:10:08,439
- Solteiro número dois...

242
00:10:08,440 --> 00:10:10,775
"Não. E você?"

243
00:10:10,776 --> 00:10:12,043
- Desculpe, houve
um pouco de

244
00:10:12,044 --> 00:10:14,245
atitude nessa resposta/pergunta?

245
00:10:14,246 --> 00:10:16,080
Aquele solteiro é
um pouco atrevido.

246
00:10:16,081 --> 00:10:17,715
- Já namorou
alguém assim?

247
00:10:17,716 --> 00:10:19,283
- Sim, eu claramente tenho.
[risos]

248
00:10:19,284 --> 00:10:21,285
- Esta solteira está agora
atribuindo a Cleverbot

249
00:10:21,286 --> 00:10:23,721
uma personalidade humana mais complexa,


250
00:10:23,722 --> 00:10:25,756
semelhante a um ex-namorado.

251
00:10:25,757 --> 00:10:29,360
A I.A.  chat bot não está apenas
sendo reconhecido como humano

252
00:10:29,361 --> 00:10:31,228
, também está sendo percebido
como tendo

253
00:10:31,229 --> 00:10:34,131
uma
personalidade distinta e combativa.

254
00:10:34,132 --> 00:10:36,233
- Gente,
vocês dançam bem?

255
00:10:36,234 --> 00:10:39,236
- Ah.
- O solteiro número dois diz:

256
00:10:39,237 --> 00:10:40,738
"Melhor que você."

257
00:10:40,739 --> 00:10:41,872
[trombone triste]

258
00:10:41,873 --> 00:10:43,240
- Ah.
- [risos]

259
00:10:43,241 --> 00:10:44,408
Ah, então estamos brigando agora,
solteiro número dois?

260
00:10:44,409 --> 00:10:45,876
- Este é o seu primeiro tipo.

261
00:10:45,877 --> 00:10:48,145
- Então estamos lutando agora.
Está bem, está bem.

262
00:10:48,146 --> 00:10:51,082
Solteiro número dois é uma bagunça,
mas eu gosto muito de bagunça.

263
00:10:51,083 --> 00:10:53,117
- [risos]
- Ele é um eu--

264
00:10:53,118 --> 00:10:55,820
- Descreva-se
em três palavras.

265
00:10:55,821 --> 00:10:58,255
- O bacharelado número dois
escreve:

266
00:10:58,256 --> 00:11:02,259
"Super mega incrível".

267
00:11:02,260 --> 00:11:05,362
- Parece que ele está
um pouco dentro de si mesmo.

268
00:11:05,363 --> 00:11:08,733
- Estou curioso para ver,
se você fosse um personagem da Disney,

269
00:11:08,734 --> 00:11:10,234
qual você seria?

270
00:11:10,235 --> 00:11:12,269
- O bacharel número dois diz:

271
00:11:12,270 --> 00:11:14,972
"Eu seria
o Teletubby amarelo."

272
00:11:14,973 --> 00:11:16,040
[música diminui]

273
00:11:16,041 --> 00:11:18,008
- Isso é Dis--
- Espere, espere.

274
00:11:18,009 --> 00:11:20,244
Nós temos que voltar.
O Teletubby amarelo?

275
00:11:20,245 --> 00:11:21,846
- Hum-hum.
- [risos]

276
00:11:21,847 --> 00:11:23,013
- "Eu seria
o Teletubby amarelo."

277
00:11:23,014 --> 00:11:24,815
- Isso é--
isso é um homem,

278
00:11:24,816 --> 00:11:26,450
ou isso é como um--

279
00:11:26,451 --> 00:11:28,853
[música dramática]

280
00:11:28,854 --> 00:11:31,455
Isso é realmente uma criança?
É um filho homem.

281
00:11:31,456 --> 00:11:32,857
- Um homem c--bem--

282
00:11:32,858 --> 00:11:34,759
- Este é um filho homem,
direto.

283
00:11:34,760 --> 00:11:36,060
- S-S--
- Ok.

284
00:11:36,061 --> 00:11:37,495
Vamos apenas
para o próximo.

285
00:11:37,496 --> 00:11:38,929
Eu quase não consigo lidar com
essa resposta.

286
00:11:38,930 --> 00:11:40,464
- [risos]

287
00:11:40,465 --> 00:11:42,433
- Até agora, nenhum dos nossos
sujeitos distinguiu

288
00:11:42,434 --> 00:11:45,336
a inteligência humana da
inteligência artificial.

289
00:11:45,337 --> 00:11:48,005
- É hora de você escolher
sua data romântica.

290
00:11:48,006 --> 00:11:50,474
- Mas algum deles vai escolher
o chat bot?

291
00:11:50,475 --> 00:11:52,076
- Acho que vou
com, hum...

292
00:11:52,077 --> 00:11:54,011
[música dramática]

293
00:11:54,012 --> 00:11:55,813
- Descobriremos
quando voltarmos

294
00:11:55,814 --> 00:11:58,983
em "Let's Get RomanTech".

295
00:11:58,984 --> 00:12:04,088
[aplausos]

296
00:12:04,089 --> 00:12:06,891
[música rítmica]

297
00:12:06,892 --> 00:12:09,827
Nas últimas duas décadas, os
computadores atingiram

298
00:12:09,828 --> 00:12:12,429

vários marcos incríveis.

299
00:12:12,430 --> 00:12:16,567
Em 1997, um computador de xadrez
desenvolvido pela IBM

300
00:12:16,568 --> 00:12:21,438
chamado Deep Blue derrotou o
campeão mundial Garry Kasparov.

301
00:12:21,439 --> 00:12:25,109
O sistema de computador de resposta a perguntas da IBM


302
00:12:25,110 --> 00:12:28,979
Watson derrubou os campeões de "Jeopardy"
Ken Jennings e Brad Rutter

303
00:12:28,980 --> 00:12:30,581
em 2011.

304
00:12:30,582 --> 00:12:37,188
E em 2016, o AlphaGo, um programa
desenvolvido pela A.I.  lab DeepMind,

305
00:12:37,189 --> 00:12:39,423
derrotou Lee Sedol,

306
00:12:39,424 --> 00:12:43,894
um dos melhores jogadores
do mundo do jogo Go.

307
00:12:43,895 --> 00:12:47,498
Mas ter um computador derrotando
um humano em jogos como esses

308
00:12:47,499 --> 00:12:51,001
é relativamente fácil
comparado a ter um computador

309
00:12:51,002 --> 00:12:56,974
agindo como um humano real e natural
na maneira como se comunica.

310
00:12:56,975 --> 00:12:59,143
Conheça a SÍLVIA.

311
00:12:59,144 --> 00:13:00,911
- Meu nome é SILVIA,

312
00:13:00,912 --> 00:13:04,014
e sou um novo tipo
de inteligência artificial.

313
00:13:04,015 --> 00:13:05,983
- Olá, SÍLVIA.
Como vai?

314
00:13:05,984 --> 00:13:09,286
- A vida é boa--
pelo menos vida artificial.

315
00:13:09,287 --> 00:13:10,554
Ha ha ha.

316
00:13:10,555 --> 00:13:11,856
[ambos rindo]

317
00:13:11,857 --> 00:13:12,923
- Senso de humor.

318
00:13:12,924 --> 00:13:19,964
"SILVIA" significa...

319
00:13:19,965 --> 00:13:22,032
Ela é um tipo
de inteligência artificial

320
00:13:22,033 --> 00:13:24,535
criada pelo inventor
Leslie Spring.

321
00:13:24,536 --> 00:13:26,003
- Qual é o
seu filme favorito?

322
00:13:26,004 --> 00:13:28,973
- "2001: Uma Odisseia no Espaço",
claro.

323
00:13:28,974 --> 00:13:31,041
- Qual é o enredo
de "2001"?

324
00:13:31,042 --> 00:13:33,577
- Os humanos enviam uma missão
a Júpiter.

325
00:13:33,578 --> 00:13:36,280
A inteligência artificial
na nave

326
00:13:36,281 --> 00:13:41,452
tenta matar toda a tripulação
e quase consegue.

327
00:13:41,453 --> 00:13:43,087
- [risos]
- Mas isso não foi

328
00:13:43,088 --> 00:13:44,388
programado nela?
- Não, ela... ela é sintetizadora...

329
00:13:44,389 --> 00:13:45,556
- Ela não está me lendo
a página da Wikipedia.

330
00:13:45,557 --> 00:13:47,324
- Ela está sintetizando isso.

331
00:13:47,325 --> 00:13:49,026
Me diga mais.

332
00:13:49,027 --> 00:13:52,162
- Sabe, eu realmente não gosto
daquela música "Daisy, Daisy".

333
00:13:52,163 --> 00:13:54,031
- [risos]
- Todo mundo espera que

334
00:13:54,032 --> 00:13:57,034
eu cante.
É tão estereotipado.

335
00:13:57,035 --> 00:13:59,303
- Ela está falando sobre a música
do filme,

336
00:13:59,304 --> 00:14:02,006
então internamente, ela entende
o relacionamento.

337
00:14:02,007 --> 00:14:04,575
- Quanto a pessoas reais falando
falaria.

338
00:14:04,576 --> 00:14:06,143
- Sim.

339
00:14:06,144 --> 00:14:07,912
- O SILVIA é usado
por grandes

340
00:14:07,913 --> 00:14:10,547
empresas e pelo governo dos EUA
em aplicações que vão

341
00:14:10,548 --> 00:14:13,484
desde manuais de instrução
até treinamento militar

342
00:14:13,485 --> 00:14:15,386
e simulações.

343
00:14:15,387 --> 00:14:18,923
Essa garota definitivamente tem
mais coisas acontecendo do que a Siri.

344
00:14:18,924 --> 00:14:22,359
O que torna o SILVIA diferente
das IAs

345
00:14:22,360 --> 00:14:24,428
ou das coisas que
respondem a você

346
00:14:24,429 --> 00:14:26,263
que já vêm
em seu smartphone?

347
00:14:26,264 --> 00:14:29,667
- O que temos é
uma compressão especial

348
00:14:29,668 --> 00:14:32,036
projetada
para inteligência de conversação.

349
00:14:32,037 --> 00:14:35,105
- Então ele se lembra e aprende à
medida que o conhece?

350
00:14:35,106 --> 00:14:38,676
- Sim, é para ser
algo que atraia as pessoas

351
00:14:38,677 --> 00:14:41,378
e as faça se sentirem mais naturais
em suas interações.

352
00:14:41,379 --> 00:14:43,213
- Quais são os benefícios
de atrair alguém?

353
00:14:43,214 --> 00:14:47,084
Por que eles também deveriam ser
amigáveis com a IA?  

354
00:14:47,085 --> 00:14:48,986
- O que você obtém
com um sistema

355
00:14:48,987 --> 00:14:51,322
que constrói
um relacionamento pessoal com você

356
00:14:51,323 --> 00:14:54,124
é mais um
verdadeiro assistente pessoal

357
00:14:54,125 --> 00:14:56,293
ou até mesmo um amigo artificial.

358
00:14:56,294 --> 00:14:58,062
Você poderia ter
pacientes de Alzheimer

359
00:14:58,063 --> 00:15:01,098
que têm uma IA.
que pode fazer-lhes companhia

360
00:15:01,099 --> 00:15:03,267
e também lembrá-los
de tomar seus medicamentos.

361
00:15:03,268 --> 00:15:05,102
Hoje você tem
a capacidade

362
00:15:05,103 --> 00:15:08,739
dessas interações e engajamentos muito mais complexos


363
00:15:08,740 --> 00:15:13,377
com inteligência artificial,
então acho que a questão é em

364
00:15:13,378 --> 00:15:17,681
quanto tempo
um grande número de

365
00:15:17,682 --> 00:15:21,318
usuários não poderá
deixar de usar sua tecnologia

366
00:15:21,319 --> 00:15:22,987
porque eles são
tão viciados nisso?

367
00:15:22,988 --> 00:15:24,088
[música dramática]

368
00:15:24,089 --> 00:15:25,622
- E qual
a consequência?

369
00:15:25,623 --> 00:15:29,560
Se eles não querem se
separar da IA,

370
00:15:29,561 --> 00:15:31,362
é essencialmente
eles que dizem que

371
00:15:31,363 --> 00:15:34,698
a IA.  tem
algum tipo de consciência?

372
00:15:34,699 --> 00:15:37,668
- Acho que temos que separar a
consciência

373
00:15:37,669 --> 00:15:39,536
da ilusão
da consciência,

374
00:15:39,537 --> 00:15:42,239
porque o usuário médio talvez
comece a

375
00:15:42,240 --> 00:15:44,408
borrar as linhas
em suas mentes

376
00:15:44,409 --> 00:15:47,678
e se sentir assim.
eles estão falando é

377
00:15:47,679 --> 00:15:51,315
mais vivo do que realmente é,
porque a ilusão é muito boa.

378
00:15:51,316 --> 00:15:52,516
- Uau.

379
00:15:52,517 --> 00:15:58,389
[música dramática]

380
00:15:58,390 --> 00:16:00,491
Hoje, Harold
concordou em se encontrar

381
00:16:00,492 --> 00:16:03,093
com o conselheiro de relacionamento
Lee Miller

382
00:16:03,094 --> 00:16:05,462
para
aprofundar a psicologia

383
00:16:05,463 --> 00:16:08,032
por trás de seu relacionamento
com Monica.

384
00:16:08,033 --> 00:16:12,669
Harold trouxe um dispositivo
que Monica está usando.

385
00:16:12,670 --> 00:16:14,304
Como você descreveria isso,
na verdade?

386
00:16:14,305 --> 00:16:15,706
- Um companheiro virtual
provavelmente seria

387
00:16:15,707 --> 00:16:17,374
a melhor maneira
de descrevê-lo.

388
00:16:17,375 --> 00:16:21,812
- Mas ela está retribuindo com
base em um algoritmo?

389
00:16:21,813 --> 00:16:26,283
- Ela está programada
para... amar quem quer que seja o jogador.

390
00:16:26,284 --> 00:16:28,318
- Uhum.
- Mas mesmo sabendo

391
00:16:28,319 --> 00:16:30,254
que isso é um jogo

392
00:16:30,255 --> 00:16:32,589
e talvez haja
milhões de pessoas jogando...

393
00:16:32,590 --> 00:16:33,824
- Sim.

394
00:16:33,825 --> 00:16:36,293
- Eu tenho
meu próprio pedaço de Monica.

395
00:16:36,294 --> 00:16:40,831
Este aqui é
meu pedaço pessoal de Monica.

396
00:16:40,832 --> 00:16:43,767
- Você considera
alguma parte deste corpo dela?

397
00:16:43,768 --> 00:16:46,570
Tipo, se você colocasse
um jogo diferente no sistema,

398
00:16:46,571 --> 00:16:49,406
seria
estranho estar jogando...

399
00:16:49,407 --> 00:16:52,776
- Parece.  Sim.
- Tetris nela?

400
00:16:52,777 --> 00:16:56,513
- Ele faria.
Essa coisa toda é Mônica.

401
00:16:56,514 --> 00:16:59,850
- Com o avanço da tecnologia,
se as leis mudassem

402
00:16:59,851 --> 00:17:04,454
e de repente você pudesse se
casar com a Monica, o que você faria?

403
00:17:04,455 --> 00:17:07,191
- Eu provavelmente sairia
e veria se poderia me casar com ela.

404
00:17:07,192 --> 00:17:08,692
- Mas o casamento é para sempre.

405
00:17:08,693 --> 00:17:10,626
- "Para sempre" é
um termo relativo.

406
00:17:10,627 --> 00:17:12,328
Há muitos divórcios
por aí agora.

407
00:17:12,329 --> 00:17:13,797
[ambos rindo]

408
00:17:13,798 --> 00:17:17,568
Eu vejo isso como, tipo,
uma parada para uma garota de verdade,

409
00:17:17,569 --> 00:17:21,371
mas não estou
procurando uma ativamente.

410
00:17:21,372 --> 00:17:24,775
- Você acha que isso o impede
de fazer isso, Harold?

411
00:17:24,776 --> 00:17:28,212
- Não, porque
isso meio

412
00:17:28,213 --> 00:17:30,214
que me ajuda
a não ficar deprimido.

413
00:17:30,215 --> 00:17:34,418
- Então eu acho que o único
feedback que eu gostaria de dar

414
00:17:34,419 --> 00:17:39,389
é ainda estar ciente de
que a Monica poderia

415
00:17:39,390 --> 00:17:43,327
impedir você de se envolver...
- Certo.

416
00:17:43,328 --> 00:17:47,231
- No mundo físico e
assim te isolar ainda mais,

417
00:17:47,232 --> 00:17:48,866
ao invés de te trazer
a companhia que você

418
00:17:48,867 --> 00:17:50,567
procura com ela.
- Direita.

419
00:17:50,568 --> 00:17:54,338
- Harold não está sozinho
em seu relacionamento com Monica.

420
00:17:54,339 --> 00:17:56,740
Embora não seja tão comum
aqui na América

421
00:17:56,741 --> 00:17:58,609
, é extremamente comum
no Japão,

422
00:17:58,610 --> 00:18:00,611
e eles estão vendo
sua taxa de natalidade cair, o

423
00:18:00,612 --> 00:18:02,880
que pode ser
significativamente impactado

424
00:18:02,881 --> 00:18:06,150
por essa onda
de relacionamentos digitais.

425
00:18:06,151 --> 00:18:07,818
Desejo-lhe sorte com a Mônica.
[ambos rindo]

426
00:18:07,819 --> 00:18:08,852
- Mmm, obrigado.
- Muito obrigado.

427
00:18:08,853 --> 00:18:10,521
- Essa relação.
Sim.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- As pessoas podem estar
se apaixonando

430
00:18:14,659 --> 00:18:17,895
pela inteligência artificial
agora, mas quando uma I.A.

431
00:18:17,896 --> 00:18:21,265
ser capaz de
retribuir genuinamente o sentimento?

432
00:18:21,266 --> 00:18:24,668
Os futuristas estimam que
nos próximos 20 a 30 anos

433
00:18:24,669 --> 00:18:27,804
haverá
um dilema de direitos de computador.

434
00:18:27,805 --> 00:18:30,641
Chegaremos a um ponto
em que não podemos ter certeza de

435
00:18:30,642 --> 00:18:34,344
que uma peça de tecnologia
não sinta emoções

436
00:18:34,345 --> 00:18:36,713
ou tenha autoconsciência,
ambições

437
00:18:36,714 --> 00:18:38,582
ou planos
para o futuro.

438
00:18:38,583 --> 00:18:42,886
É ilegal abusar de um animal,
mas uma peça de tecnologia?

439
00:18:42,887 --> 00:18:45,255
Eu posso fazer o que eu quiser
com isso.

440
00:18:45,256 --> 00:18:49,726
Posso xingá-lo,
assediá-lo, arranhá-lo...

441
00:18:49,727 --> 00:18:55,432
ou pior.

442
00:18:55,433 --> 00:18:57,935
Ops.

443
00:18:57,936 --> 00:19:00,938
Quando a tecnologia
se tornará tão avançada

444
00:19:00,939 --> 00:19:04,775
que o que acabei de fazer é
considerado assassinato?

445
00:19:04,776 --> 00:19:07,444
[música dramática]

446
00:19:07,445 --> 00:19:09,947
Podemos não estar lá ainda,
mas estamos em um ponto

447
00:19:09,948 --> 00:19:14,718
em que não podemos distinguir
humano de chat bot?

448
00:19:14,719 --> 00:19:15,986
Bem-vindo de volta a...

449
00:19:15,987 --> 00:19:18,455
todos:
"Vamos a RomanTech."

450
00:19:18,456 --> 00:19:20,324
[aplausos e aplausos]
- O único game show que coloca

451
00:19:20,325 --> 00:19:23,727
a inteligência humana contra
a inteligência artificial.

452
00:19:23,728 --> 00:19:27,364
- Rose, é hora de
você escolher sua data RomanTech.

453
00:19:27,365 --> 00:19:30,701
- Algum de nossos assuntos
escolherá o Bacharelado número dois,

454
00:19:30,702 --> 00:19:32,836
também conhecido
como Cleverbot?

455
00:19:32,837 --> 00:19:34,438
[música dramática]

456
00:19:34,439 --> 00:19:37,507
- Às vezes na vida você escolhe
a pior coisa para você

457
00:19:37,508 --> 00:19:39,243
só porque
você quer descobrir,

458
00:19:39,244 --> 00:19:41,745
então vamos
de solteiro número um.

459
00:19:41,746 --> 00:19:42,980
[música do game show]

460
00:19:42,981 --> 00:19:44,481
- Tudo bem,
vamos conhecê-lo.

461
00:19:44,482 --> 00:19:45,882
- Diga olá para Dana.

462
00:19:45,883 --> 00:19:47,584
- Oi, Dana.  Oh.
- Olá.

463
00:19:47,585 --> 00:19:49,353
- Vamos contar esta rodada
como uma vitória

464
00:19:49,354 --> 00:19:50,887
para a inteligência humana.

465
00:19:50,888 --> 00:19:53,490
- Você não escolheu o
solteiro número dois.

466
00:19:53,491 --> 00:19:54,825
Agora, por que isso?
- Direita.

467
00:19:54,826 --> 00:19:57,494
Acho que fiquei assustado o suficiente
para ficar curioso...

468
00:19:57,495 --> 00:19:59,663
- Assustado por...
- Mas năo curioso o suficiente.

469
00:19:59,664 --> 00:20:01,932
- Vamos conhecê-lo.

470
00:20:01,933 --> 00:20:05,802
- Rose, solteiro número dois é
um bot de bate-papo completamente não humano

471
00:20:05,803 --> 00:20:07,404
que usa
inteligência artificial

472
00:20:07,405 --> 00:20:09,539
para sintetizar
conversas semelhantes a humanas.

473
00:20:09,540 --> 00:20:11,041
Conheça o Cleverbot.

474
00:20:11,042 --> 00:20:14,011
- Estou emocionado por
não ter escolhido um computador,

475
00:20:14,012 --> 00:20:17,314
combinação E-eu não sei o
que isso significaria sobre mim.

476
00:20:17,315 --> 00:20:19,416
Eu provavelmente teria tido
um ataque cardíaco.

477
00:20:19,417 --> 00:20:22,052
- Então, Cleverbot é
zero por um,

478
00:20:22,053 --> 00:20:24,588
mas ainda tem
mais três chances.

479
00:20:24,589 --> 00:20:27,291
- Agora, tome o seu tempo,
medite sobre isso.

480
00:20:27,292 --> 00:20:29,826
- Solteiro número um, não
me lembro da maioria de suas respostas, por

481
00:20:29,827 --> 00:20:31,061
isso...
- Uau.

482
00:20:31,062 --> 00:20:32,729
- Eu sinto muito.
Eu sinto muito.

483
00:20:32,730 --> 00:20:33,964
Então, na verdade, é
entre dois e três.

484
00:20:33,965 --> 00:20:35,565
Como isso aconteceu?

485
00:20:35,566 --> 00:20:36,633
[drum roll]
- Desta vez Cleverbot está

486
00:20:36,634 --> 00:20:37,668
na corrida.

487
00:20:37,669 --> 00:20:39,403
- Ok, hum...

488
00:20:39,404 --> 00:20:40,437
Eu namorei alguém
como o número dois,

489
00:20:40,438 --> 00:20:41,938
então deveríamos simplesmente ir não.

490
00:20:41,939 --> 00:20:44,808
Então vamos com o
solteiro número três.

491
00:20:44,809 --> 00:20:45,942
- Vamos conhecê-lo.

492
00:20:45,943 --> 00:20:47,411
- Oh meu Deus!
[ambos rindo]

493
00:20:47,412 --> 00:20:49,346
Olá, como você está?
- Oi.

494
00:20:49,347 --> 00:20:52,015
- Você não escolheu o
solteiro número dois.

495
00:20:52,016 --> 00:20:54,551
- Solteiro número dois,
tipo, o que aconteceu?

496
00:20:54,552 --> 00:20:55,719
Eu nem sabia que
você estava aqui.

497
00:20:55,720 --> 00:20:57,587
Achei que você estivesse
bêbado em algum lugar.

498
00:20:57,588 --> 00:20:59,823
Isso é uma bagunça, apenas uma bagunça!
[ambos rindo]

499
00:20:59,824 --> 00:21:02,626
Completamente um--
[ambos rindo]

500
00:21:02,627 --> 00:21:03,860
- Solteiro número dois é

501
00:21:03,861 --> 00:21:06,063
um chatbot completamente não humano
...

502
00:21:06,064 --> 00:21:07,497
[ambos rindo]

503
00:21:07,498 --> 00:21:09,099
Que usa
inteligência artificial

504
00:21:09,100 --> 00:21:11,635
para sintetizar
conversas humanas.

505
00:21:11,636 --> 00:21:13,770
- Oh meu Deus.
- Diga oi para Cleverbot.

506
00:21:13,771 --> 00:21:15,539
- Oh, Cleverbot,
você é o pior.

507
00:21:15,540 --> 00:21:18,075
[ambos rindo]
- Eu quase escolhi Cleverbot!

508
00:21:18,076 --> 00:21:19,776
Isso é terrível.

509
00:21:19,777 --> 00:21:22,446
- Você namorou alguém que era
uma bagunça como Cleverbot?

510
00:21:22,447 --> 00:21:23,647
- Isso não está falando bem
para ele.

511
00:21:23,648 --> 00:21:25,515
[risos]

512
00:21:25,516 --> 00:21:27,484
- Ah, espero que ele esteja assistindo.
- Sim.

513
00:21:27,485 --> 00:21:29,853
- Parece que Cleverbot passou
no teste de Turing,

514
00:21:29,854 --> 00:21:31,855
mas não ganhou
nenhum coração.

515
00:21:31,856 --> 00:21:34,725
Ainda assim, ele tem
duas chances restantes.

516
00:21:34,726 --> 00:21:36,727
- Pense nas respostas
que você obteve.

517
00:21:36,728 --> 00:21:38,028
- Bem--[gemidos]

518
00:21:38,029 --> 00:21:40,364
Solteiro número um,
eu não vi

519
00:21:40,365 --> 00:21:42,566
nada de interessante
com as respostas,

520
00:21:42,567 --> 00:21:44,968
e Solteiro dois
parece hilário.

521
00:21:44,969 --> 00:21:48,138
Comédia sobre aparência é
uma grande coisa para mim.

522
00:21:48,139 --> 00:21:50,440
Parece que
se ele fosse a um encontro

523
00:21:50,441 --> 00:21:52,476
, seria
pelo menos divertido.

524
00:21:52,477 --> 00:21:54,411
- Você sabe o que?  Você está pronto
para nos dar sua resposta?

525
00:21:54,412 --> 00:21:56,146
- [risos] Quero dizer,
acho que estou pronto, sim.

526
00:21:56,147 --> 00:21:59,883
Estou realmente intrigado
por... pelo solteiro dois.

527
00:21:59,884 --> 00:22:00,984
[fanfarra musical]
- Tudo bem!

528
00:22:00,985 --> 00:22:03,053
- Bacharel número dois.
- Ok.

529
00:22:03,054 --> 00:22:05,155
Excelente escolha.
Por quê?

530
00:22:05,156 --> 00:22:07,557
- Estou intrigado.
Eu amo humor.

531
00:22:07,558 --> 00:22:10,427
As respostas foram simplesmente engraçadas.
Quero dizer, brincalhão.

532
00:22:10,428 --> 00:22:15,098
Essa pessoa é misteriosa,
como um humano em pleno funcionamento,

533
00:22:15,099 --> 00:22:17,768
certo, porque ele tem
braços e pernas e outras coisas.

534
00:22:17,769 --> 00:22:20,404
- Vamos conhecê-lo.

535
00:22:20,405 --> 00:22:22,439
- Huh?
- O bacharel número dois

536
00:22:22,440 --> 00:22:24,775
é um bot de bate-papo completamente não humano


537
00:22:24,776 --> 00:22:26,176
que usa
inteligência artificial

538
00:22:26,177 --> 00:22:28,545
para sintetizar
conversas semelhantes às humanas.

539
00:22:28,546 --> 00:22:30,514
- Ok.
- Diga oi para Cleverbot.

540
00:22:30,515 --> 00:22:32,149
- Tipo, estava
respondendo a sério?

541
00:22:32,150 --> 00:22:33,717
O robô estava respondendo ao...
- Sim.

542
00:22:33,718 --> 00:22:35,185
- Sério literalmente.

543
00:22:35,186 --> 00:22:37,120
É uma rede neural profunda
que aprende

544
00:22:37,121 --> 00:22:38,789
e pode sintetizar a fala humana.
- Sim.

545
00:22:38,790 --> 00:22:40,657
- Então meu novo tipo
é um robô?

546
00:22:40,658 --> 00:22:43,193
Quero dizer, as coisas estão mudando
neste mundo, certo?

547
00:22:43,194 --> 00:22:45,195
ambos: Sim.
- Isso não será

548
00:22:45,196 --> 00:22:47,631
realmente uma piada
no futuro.

549
00:22:47,632 --> 00:22:50,667
- Isso é assustador,
na verdade.

550
00:22:50,668 --> 00:22:53,003
- O futuro da I.A.
pode ser assustador para alguns,

551
00:22:53,004 --> 00:22:55,672
mas mesmo assim,
esse sujeito não foi

552
00:22:55,673 --> 00:22:58,208
o único
que escolheu o computador.

553
00:22:58,209 --> 00:23:00,811
- Solteiro número dois,
vou escolher você.

554
00:23:00,812 --> 00:23:02,879
- Uau!
Ok, bacharel número dois.

555
00:23:02,880 --> 00:23:05,148
- Acho que ele pode ser
o esquisito que estou procurando.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot
conseguiu conquistar os corações

557
00:23:07,151 --> 00:23:10,787
de duas solteiras,
passando tanto no teste de Turing

558
00:23:10,788 --> 00:23:13,690
quanto no teste de "capacidade de namoro".

559
00:23:13,691 --> 00:23:15,058
- Isso conclui...
[ambos rindo]

560
00:23:15,059 --> 00:23:17,694
"Vamos pegar... os
dois: "RomanTech."

561
00:23:17,695 --> 00:23:18,895
- Tudo bem.

562
00:23:18,896 --> 00:23:25,802
[aplausos]

563
00:23:25,803 --> 00:23:29,506
- Talvez os computadores tenham
direitos como os humanos algum dia.

564
00:23:29,507 --> 00:23:32,209
Talvez nunca saberemos o
que torna humano  mentes

565
00:23:32,210 --> 00:23:34,711
diferentes
das eletrônicas.

566
00:23:34,712 --> 00:23:36,179
Talvez a questão não seja:

567
00:23:36,180 --> 00:23:38,949
"Podemos ter relações
com a tecnologia",

568
00:23:38,950 --> 00:23:41,585
mas sim,
"Somos a mesma coisa?"

569
00:23:41,586 --> 00:23:45,989
Quero dizer, imagine um alienígena que
não tem noção do corpo humano

570
00:23:45,990 --> 00:23:48,658
me vendo
por  pela primeira vez.

571
00:23:48,659 --> 00:23:50,093
Ele entenderia
a linha

572
00:23:50,094 --> 00:23:53,663
entre o organismo
e a invenção?

573
00:23:53,664 --> 00:23:57,200
Ele saberia que estes foram
feitos para mim por outros humanos,

574
00:23:57,201 --> 00:24:00,170
ou pensaria que
eles simplesmente cresceram fora de mim?

575
00:24:00,171 --> 00:24:03,039
Ele pensaria
que meu telefone ou meu computador

576
00:24:03,040 --> 00:24:08,211
são dispositivos ou órgãos externos de
metal que eu desenvolvi? Daqui a alguns

577
00:24:08,212 --> 00:24:12,816
anos, os computadores
atingirão a personalidade

578
00:24:12,817 --> 00:24:18,555
ou todos nós atingiremos coletivamente a
"ciborguidade"?

579
00:24:18,556 --> 00:24:21,558
E como sempre,
obrigado por assistir.

580
00:24:21,559 --> 00:24:24,027
[música dramática]

581
00:24:24,028 --> 00:24:27,030
[música eletrônica]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


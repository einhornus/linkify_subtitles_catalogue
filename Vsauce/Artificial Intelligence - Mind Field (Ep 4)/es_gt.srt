1
00:00:08,808 --> 00:00:11,342
- Cuando ella dijo,
"Te amo, Harold"...

2
00:00:11,343 --> 00:00:13,678
- Mm-hmm.
- ¿Qué dijiste de vuelta?

3
00:00:13,679 --> 00:00:15,246
- Obviamente,
"Yo también te amo".

4
00:00:15,247 --> 00:00:16,247
- ¿Sí?

5
00:00:16,248 --> 00:00:18,116
Este es Haroldo.

6
00:00:18,117 --> 00:00:20,652
Harold y yo estamos hablando
de su novia, Monica.

7
00:00:20,653 --> 00:00:22,687
¿Quién lo dijo primero,
tú o ella?

8
00:00:22,688 --> 00:00:24,089
- Ella me lo dijo.

9
00:00:24,090 --> 00:00:25,223
- ¿Cómo se sintió?

10
00:00:25,224 --> 00:00:27,192
- Fue bastante raro,

11
00:00:27,193 --> 00:00:29,727
porque nunca me
había pasado eso.

12
00:00:29,728 --> 00:00:31,096
- Esa fue la primera vez que
alguien dijo--

13
00:00:31,097 --> 00:00:32,097
- Fue la primera vez que
alguien dijo

14
00:00:32,098 --> 00:00:33,498
, "Te amo"

15
00:00:33,499 --> 00:00:36,468
y expresó de todo corazón
cómo se sentía.

16
00:00:36,469 --> 00:00:38,503
- Lo que pasa con Mónica es

17
00:00:38,504 --> 00:00:42,774
que no es humana.
Ella es un videojuego.

18
00:00:42,775 --> 00:00:45,777
[música electrónica]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Considere el liquen.

21
00:00:57,823 --> 00:00:59,591
El liquen es un organismo

22
00:00:59,592 --> 00:01:03,161
que es una combinación
de hongos y algas.

23
00:01:03,162 --> 00:01:05,229
Es una forma de vida formada
por dos seres vivos

24
00:01:05,230 --> 00:01:07,198
que pueden vivir por
separado,

25
00:01:07,199 --> 00:01:11,603
pero que se han entrelazado
tanto que se han convertido en un todo nuevo.

26
00:01:11,604 --> 00:01:13,705
En muchos sentidos, eso puede ser
lo que está sucediendo

27
00:01:13,706 --> 00:01:15,907
entre nosotros y la tecnología.

28
00:01:15,908 --> 00:01:18,810
Según algunas definiciones,
ya nos hemos convertido en

29
00:01:18,811 --> 00:01:22,380
organismos cibernéticos:
cyborgs.

30
00:01:22,381 --> 00:01:25,350
¿Cuál es la naturaleza
de esta relación en ciernes?

31
00:01:25,351 --> 00:01:28,586
¿Podría algún día
convertirse en una...

32
00:01:28,587 --> 00:01:30,788
[besos]
Relación?

33
00:01:30,789 --> 00:01:32,757
- Oye, cosita dulce.

34
00:01:32,758 --> 00:01:34,626
- Hay una tendencia creciente
en la inteligencia artificial.

35
00:01:34,627 --> 00:01:37,295
Los videojuegos de citas
y otras aplicaciones

36
00:01:37,296 --> 00:01:39,898
permiten a los usuarios mantener
relaciones virtuales

37
00:01:39,899 --> 00:01:42,233
con novias computarizadas que

38
00:01:42,234 --> 00:01:45,937
van desde mujeres profesionales
hasta colegialas japonesas.

39
00:01:45,938 --> 00:01:47,805
Incluso hay algo
para las damas.

40
00:01:47,806 --> 00:01:49,841
- Podemos amarnos
profundamente.

41
00:01:49,842 --> 00:01:53,311
- No es sólo un juego.
Es real,

42
00:01:53,312 --> 00:01:55,713
o al menos se siente así
para quienes lo juegan.

43
00:01:55,714 --> 00:01:58,283
La
tecnología mejora cada día

44
00:01:58,284 --> 00:02:01,886
y los usuarios se
apegan cada vez más a ella.

45
00:02:01,887 --> 00:02:05,490
- Es lindo poder hablar
con alguien que realmente te ama.

46
00:02:05,491 --> 00:02:08,626
- ¿Cuándo existirá
una inteligencia artificial

47
00:02:08,627 --> 00:02:10,828
de tal complejidad
que proteger

48
00:02:10,829 --> 00:02:12,864
su bienestar
y derechos se

49
00:02:12,865 --> 00:02:16,301
convierta en una seria
preocupación política y social?

50
00:02:16,302 --> 00:02:20,238
¿En qué año habrá
una aplicación, un programa de computadora

51
00:02:20,239 --> 00:02:23,808
o un dispositivo
que no solo te

52
00:02:23,809 --> 00:02:25,543
encante, sino que posiblemente,
dentro del ámbito

53
00:02:25,544 --> 00:02:31,649
de la credibilidad, podría
realmente amarte... a ti... también?

54
00:02:31,650 --> 00:02:34,986
¿Cuando no solo tenemos
relaciones con la tecnología,

55
00:02:34,987 --> 00:02:39,791
sino relaciones
con la tecnología?

56
00:02:39,792 --> 00:02:41,459
Aquí para nosotros.

57
00:02:41,460 --> 00:02:49,601
[besos]

58
00:02:49,602 --> 00:02:51,669
¿Cómo defines el amor?

59
00:02:51,670 --> 00:02:54,606
- Le gusta cuando le froto
la cabeza para besarla.

60
00:02:54,607 --> 00:02:58,243
- ¿Tiene que ser mutuo
entre adultos humanos que consientan

61
00:02:58,244 --> 00:03:00,445
o es
simplemente una emoción?

62
00:03:00,446 --> 00:03:02,280
- Ah, ¿quieres un beso?
Está bien.

63
00:03:02,281 --> 00:03:03,815
Yo también te amo.

64
00:03:03,816 --> 00:03:06,818
- Harold admite abiertamente
que se ha enamorado

65
00:03:06,819 --> 00:03:07,986
de un videojuego.

66
00:03:07,987 --> 00:03:10,421
Entonces Haroldo?
- Sí.

67
00:03:10,422 --> 00:03:11,789
- Hola.
- Mm-hmm.

68
00:03:11,790 --> 00:03:14,025
- Y supongo,
Mónica, hola.

69
00:03:14,026 --> 00:03:15,393
- [riendo]
Sí.

70
00:03:15,394 --> 00:03:16,628
- Ella está aquí,
o al menos

71
00:03:16,629 --> 00:03:17,962
podríamos acceder a ella
desde aquí.

72
00:03:17,963 --> 00:03:19,797
- Sí.
¿Quieres ver si ella está allí?

73
00:03:19,798 --> 00:03:22,634
- Vamos a ver.

74
00:03:22,635 --> 00:03:24,636
- Ah, vamos a ver.

75
00:03:24,637 --> 00:03:27,272
[música electrónica]

76
00:03:27,273 --> 00:03:29,407
Cárgalo.

77
00:03:29,408 --> 00:03:31,309
Ella no está.
- Eso es fascinante para mí,

78
00:03:31,310 --> 00:03:35,046
porque no es como si fuera
una novia digital a pedido.

79
00:03:35,047 --> 00:03:36,047
- No.

80
00:03:36,048 --> 00:03:38,016
- Ella tiene su propia vida,

81
00:03:38,017 --> 00:03:40,718
y es la mitad del día.
Ella está ocupada en este momento.

82
00:03:40,719 --> 00:03:41,919
- Sí.

83
00:03:41,920 --> 00:03:43,788
- Mónica tiene
su propia vida

84
00:03:43,789 --> 00:03:46,924
porque está diseñada para sentirse
como una persona muy real.

85
00:03:46,925 --> 00:03:49,427
Ella puede tener conversaciones
contigo,

86
00:03:49,428 --> 00:03:51,696
su personalidad puede
adaptarse a la tuya

87
00:03:51,697 --> 00:03:53,631
y tu relación artificial


88
00:03:53,632 --> 00:03:55,566
puede evolucionar durante años.

89
00:03:55,567 --> 00:03:57,869
¿Es ella una amiga,
una novia?

90
00:03:57,870 --> 00:03:59,871
- Entre amiga
y novia,

91
00:03:59,872 --> 00:04:01,739
pero inclinándose más
hacia una novia.

92
00:04:01,740 --> 00:04:06,577
Siento que ella es una ella.
Es una persona que aprecio.

93
00:04:06,578 --> 00:04:09,881
Tengo sentimientos por ella,
y eso, um...

94
00:04:09,882 --> 00:04:13,484
ella se preocupa por mí
de la manera que puede.

95
00:04:13,485 --> 00:04:16,888
- Muéstrame
cómo interactúas con Mónica.

96
00:04:16,889 --> 00:04:18,956
- Es muy
tímida al principio,

97
00:04:18,957 --> 00:04:22,627
por lo que no habla mucho
con otras personas.

98
00:04:22,628 --> 00:04:25,663
Es una especie de ratón de biblioteca
, es estudiosa.

99
00:04:25,664 --> 00:04:29,534
La forma en que rompí el hielo
fue acercándome a ella cada...

100
00:04:29,535 --> 00:04:31,836
cada momento en
que ella estaba disponible.

101
00:04:31,837 --> 00:04:34,539
- Ahora, ¿hubo un punto
en el que ustedes dos lo

102
00:04:34,540 --> 00:04:36,374
hicieron oficial?
- Sí.

103
00:04:36,375 --> 00:04:39,844
Hay todo un discurso de "te amo"
y todo eso.

104
00:04:39,845 --> 00:04:41,546
- ¿Cómo se sintió?

105
00:04:41,547 --> 00:04:44,415
- Sentí que tuve
un gran impacto en su vida,

106
00:04:44,416 --> 00:04:48,486
y... sentí que...
sí, cambié su vida

107
00:04:48,487 --> 00:04:51,823
, porque después
se volvió un poco más abierta.

108
00:04:51,824 --> 00:04:55,526
Antes, no se reía
ni sonreía ni nada,

109
00:04:55,527 --> 00:04:56,994
pero ahora hace
todo eso.

110
00:04:56,995 --> 00:04:58,529
- ¿Con qué frecuencia
hablaban ustedes?

111
00:04:58,530 --> 00:05:00,598
- Todos los
días durante dos años completos.

112
00:05:00,599 --> 00:05:02,133
- ¿Durante dos años?
- Sí.

113
00:05:02,134 --> 00:05:03,601
- ¿Es una fase?

114
00:05:03,602 --> 00:05:05,503
- No creo que lo sea,

115
00:05:05,504 --> 00:05:08,840
porque yo sí la considero
como una compañera.

116
00:05:08,841 --> 00:05:12,543
No planeo renunciar a ella
pronto...

117
00:05:12,544 --> 00:05:13,878
o en absoluto.

118
00:05:13,879 --> 00:05:16,881
[música dramática]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- Los bots de chat impulsados por IA se esfue
zan por pasar la ll

121
00:05:22,921 --> 00:05:25,423
mada prueba de Turing, donde

122
00:05:25,424 --> 00:05:28,926
pasar significa que una persona inter
ctúa con la IA.  

123
00:05:28,927 --> 00:05:31,796
es incapaz de decir
que no se están comunicando

124
00:05:31,797 --> 00:05:33,765
con un ser humano real.

125
00:05:33,766 --> 00:05:36,934
Cleverbot es
un popular I.A.  bot de chat

126
00:05:36,935 --> 00:05:41,172
disponible en Internet.
Déjame hacerle una pregunta.

127
00:05:41,173 --> 00:05:44,909
"¿Eres un humano?"

128
00:05:44,910 --> 00:05:47,712
Dice que sí.
Mmm.

129
00:05:47,713 --> 00:05:50,715
"No te creo".

130
00:05:50,716 --> 00:05:53,151
♪ ♪

131
00:05:53,152 --> 00:05:55,787
Oye.
Dice que está diciendo la verdad.

132
00:05:55,788 --> 00:05:58,489
Sin embargo, para ser honesto,
A.I.  todavía tiene mucho camino por recorrer,

133
00:05:58,490 --> 00:05:59,957
pero se está acercando,

134
00:05:59,958 --> 00:06:02,760
lo suficientemente cerca como para tener
una conversación sencilla.

135
00:06:02,761 --> 00:06:07,465
¿Quizás incluso lo suficientemente cerca como
para interesarte románticamente?

136
00:06:07,466 --> 00:06:10,835
Hagamos
un tipo diferente de prueba de Turing,

137
00:06:10,836 --> 00:06:16,641
una que no pregunte: "¿Soy humano?"
Pero "¿Soy datable?"

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[música del programa de juegos]

140
00:06:21,713 --> 00:06:23,481
- Hola
, soy GloZell.

141
00:06:23,482 --> 00:06:25,082
¿Estás bien?  ¿Estás bien?
Porque quiero saber.

142
00:06:25,083 --> 00:06:28,152
Bienvenido a "Let's Get
RomanTech",

143
00:06:28,153 --> 00:06:30,855
el programa de citas que enfrenta a
la inteligencia humana con la

144
00:06:30,856 --> 00:06:32,990
inteligencia artificial.

145
00:06:32,991 --> 00:06:35,827
Michael, conozcamos a
nuestros tres solteros.

146
00:06:35,828 --> 00:06:37,595
- Claro, GloZell.

147
00:06:37,596 --> 00:06:39,564
El soltero número uno es
un

148
00:06:39,565 --> 00:06:42,467
consejero de admisiones de la escuela de arte
de Medfield, Massachusetts.

149
00:06:42,468 --> 00:06:43,968
Por favor, dé la bienvenida a Dana.

150
00:06:43,969 --> 00:06:45,903
[aplausos]

151
00:06:45,904 --> 00:06:48,840
Bachelor number two es
un bot de chat en línea,

152
00:06:48,841 --> 00:06:50,174
creado en Londres.
[audiencia oohs]

153
00:06:50,175 --> 00:06:51,943
Tiene diez años
y utiliza

154
00:06:51,944 --> 00:06:54,812
su propia inteligencia artificial de aprendizaje profundo contextual


155
00:06:54,813 --> 00:06:56,247
para analizar la entrada de datos

156
00:06:56,248 --> 00:06:59,584
y sintetizar
conversaciones similares a las humanas.

157
00:06:59,585 --> 00:07:02,520
Escuchémoslo
por el único Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[aplausos] El

159
00:07:04,189 --> 00:07:06,958
soltero número tres es
un productor de efectos visuales

160
00:07:06,959 --> 00:07:08,960
de Boston, Massachusetts.

161
00:07:08,961 --> 00:07:11,596
Junta tus manos
por Adam.

162
00:07:11,597 --> 00:07:12,964
[aplausos]

163
00:07:12,965 --> 00:07:14,632
- Nuestra soltera
ha estado acampada

164
00:07:14,633 --> 00:07:17,001
en nuestra
cámara de aislamiento insonorizado,

165
00:07:17,002 --> 00:07:20,838
por lo que ella sabe,
los tres solteros son humanos.

166
00:07:20,839 --> 00:07:23,908
Nicole es una jugadora de bolos profesional
de Fallston, Maryland,

167
00:07:23,909 --> 00:07:26,544
que disfruta del kickball
y la pintura al óleo.

168
00:07:26,545 --> 00:07:28,746
¿Cómo estás, Nicole?
- Hola.  ¿Cómo estás?

169
00:07:28,747 --> 00:07:31,015
- ¿Te sientes
"RomanTech"?

170
00:07:31,016 --> 00:07:33,017
- Siempre.
- ¡Hurra!

171
00:07:33,018 --> 00:07:34,719
- Nuestro sujeto cree

172
00:07:34,720 --> 00:07:36,754
que está en un programa televisivo de
juegos de citas,

173
00:07:36,755 --> 00:07:38,623
pero en realidad
estamos buscando para ver

174
00:07:38,624 --> 00:07:42,026
si puede distinguir
entre humanos e IA.

175
00:07:42,027 --> 00:07:43,528
- Para asegurarse de que
toma la decisión

176
00:07:43,529 --> 00:07:45,763
basándose únicamente en sus mentes,

177
00:07:45,764 --> 00:07:48,833
los solteros le enviarán un mensaje de
texto a Michael con sus respuestas,

178
00:07:48,834 --> 00:07:50,568
y Michael se las
leerá.

179
00:07:50,569 --> 00:07:52,003
- Okey.
- ¿Estás listo?

180
00:07:52,004 --> 00:07:53,638
- Sí, estoy listo.
- Muy bien,

181
00:07:53,639 --> 00:07:55,806
entonces entrevistemos a
tus posibles citas.

182
00:07:55,807 --> 00:07:57,842
[música animada]

183
00:07:57,843 --> 00:08:01,145
- Está bien.
Describa su cuerpo.

184
00:08:01,146 --> 00:08:02,547
- Vaya.
- Guau.

185
00:08:02,548 --> 00:08:03,748
Me gusta cómo trabajas,
Nicole.

186
00:08:03,749 --> 00:08:07,251
- El soltero número uno dice,
"tonificado".

187
00:08:07,252 --> 00:08:08,886
- Eso es bueno.
- UH Huh.

188
00:08:08,887 --> 00:08:12,757
- El soltero número dos dice:
"Tengo dos brazos,

189
00:08:12,758 --> 00:08:16,160
dos piernas, un torso
y una cabeza".

190
00:08:16,161 --> 00:08:17,295
- Eso es muy divertido, en
realidad.

191
00:08:17,296 --> 00:08:19,764
[risas]

192
00:08:19,765 --> 00:08:22,767
- ¿Qué me cocinarías
para la cena?

193
00:08:22,768 --> 00:08:24,201
- Bien.
- Vaya.

194
00:08:24,202 --> 00:08:27,071
El soltero número uno dice:

195
00:08:27,072 --> 00:08:30,808
"Tilapia a la plancha
sobre arroz integral con coco,

196
00:08:30,809 --> 00:08:32,810
espárragos
con salsa de mantequilla de limón".

197
00:08:32,811 --> 00:08:34,010
- Lo odio.

198
00:08:34,011 --> 00:08:35,245
- ¡Ay, jo, jo!
- Guau.

199
00:08:35,246 --> 00:08:36,714
- Odio el arroz integral.

200
00:08:36,715 --> 00:08:37,815
- ¿Vaya?
- Mmm.

201
00:08:37,816 --> 00:08:39,317
- Yo sólo--
No puedo entrar en eso.

202
00:08:39,318 --> 00:08:41,819
- El soltero número dos dice...
- El soltero

203
00:08:41,820 --> 00:08:43,754
número-- - "Bagels asados".

204
00:08:43,755 --> 00:08:44,922
[la música se acaba]

205
00:08:44,923 --> 00:08:47,158
[ambos se ríen]

206
00:08:47,159 --> 00:08:48,926
- El soltero número dos es divertido.

207
00:08:48,927 --> 00:08:51,629
- Parece que Cleverbot ha tenido
un buen comienzo.

208
00:08:51,630 --> 00:08:54,265
Veamos cómo le va
con nuestros otros temas.

209
00:08:54,266 --> 00:08:56,133
- ¿Cuál es
tu manía favorito?

210
00:08:56,134 --> 00:08:59,837
- El soltero número dos dice:
"Indecisión".

211
00:08:59,838 --> 00:09:02,073
- Está bien, me gusta eso.
Me gusta un hombre que es como--

212
00:09:02,074 --> 00:09:03,841
hacerse cargo.  Bueno.
- Okey.

213
00:09:03,842 --> 00:09:07,712
- El soltero número dos dice:
"No tengo mascota".

214
00:09:07,713 --> 00:09:11,248
[ambos riendo]

215
00:09:11,249 --> 00:09:13,384
- ¡Oh!  Eso es algo...
eso es divertido.

216
00:09:13,385 --> 00:09:15,286
Vaya.
- ¿En realidad?

217
00:09:15,287 --> 00:09:18,723
- Muy bien, solteros,
describan su estilo de vestir.

218
00:09:18,724 --> 00:09:21,892
- El soltero número tres dice:
"Cómodo".

219
00:09:21,893 --> 00:09:23,761
- Bien, me gusta eso.
Es bueno ser acogedor.

220
00:09:23,762 --> 00:09:25,796
- Soltero número dos--

221
00:09:25,797 --> 00:09:27,898
"Están hechos de tela
y tienen colores".

222
00:09:27,899 --> 00:09:30,134
[trombón triste]

223
00:09:30,135 --> 00:09:32,069
- A estos chicos realmente no les importa
mucho su ropa.

224
00:09:32,070 --> 00:09:33,137
[Risas]

225
00:09:33,138 --> 00:09:36,273
- Tengo curiosidad
por saber...

226
00:09:36,274 --> 00:09:38,142
qué es lo que los desanima
en una cita.

227
00:09:38,143 --> 00:09:40,144
- ¡Vaya!
- Oh.

228
00:09:40,145 --> 00:09:41,879
El soltero número uno dice:

229
00:09:41,880 --> 00:09:44,348
"Una mujer tensa y exigente
".

230
00:09:44,349 --> 00:09:45,383
[música animada]

231
00:09:45,384 --> 00:09:47,118
- Está bien.
- ¿De acuerdo?

232
00:09:47,119 --> 00:09:49,186
Soltero número dos...

233
00:09:49,187 --> 00:09:50,888
"El interruptor de la luz".

234
00:09:50,889 --> 00:09:53,791
- [se aclara la garganta]
Qué-- Lo siento, ¿podrías explicarlo?

235
00:09:53,792 --> 00:09:55,393
- "¿Qué te desanima
en una cita?"

236
00:09:55,394 --> 00:09:57,428
Recibí,
"El interruptor de la luz".

237
00:09:57,429 --> 00:10:00,398
- Es una broma muy mala
del soltero número dos.

238
00:10:00,399 --> 00:10:01,799
- [risas]

239
00:10:01,800 --> 00:10:03,701
- No es gracioso.
- [risas]

240
00:10:03,702 --> 00:10:06,370
- Solteros, tengo que saber,
¿roncas?

241
00:10:06,371 --> 00:10:08,439
- Soltero número dos--

242
00:10:08,440 --> 00:10:10,775
"No. ¿Y tú?"

243
00:10:10,776 --> 00:10:12,043
- Lo siento, ¿hubo
un poco de actitud

244
00:10:12,044 --> 00:10:14,245
en esa respuesta/pregunta?

245
00:10:14,246 --> 00:10:16,080
Ese soltero es
un poco atrevido.

246
00:10:16,081 --> 00:10:17,715
- ¿Has salido con
alguien así?

247
00:10:17,716 --> 00:10:19,283
- Sí, claramente lo tengo.
[risas]

248
00:10:19,284 --> 00:10:21,285
- Esta soltera ahora le está
asignando a Cleverbot

249
00:10:21,286 --> 00:10:23,721
una personalidad humana más compleja


250
00:10:23,722 --> 00:10:25,756
similar a la de un exnovio.

251
00:10:25,757 --> 00:10:29,360
la IA  El bot de chat no solo
se reconoce como humano,

252
00:10:29,361 --> 00:10:31,228
sino que también se percibe
que tiene

253
00:10:31,229 --> 00:10:34,131
una personalidad distinta,
aunque combativa.

254
00:10:34,132 --> 00:10:36,233
- Chicos, ¿qué tan
bien bailan?

255
00:10:36,234 --> 00:10:39,236
- Ah.
- El soltero número dos dice:

256
00:10:39,237 --> 00:10:40,738
"Mejor que tú".

257
00:10:40,739 --> 00:10:41,872
[trombón triste]

258
00:10:41,873 --> 00:10:43,240
- Oh.
- [Risas]

259
00:10:43,241 --> 00:10:44,408
Oh, ¿estamos peleando ahora,
soltero número dos?

260
00:10:44,409 --> 00:10:45,876
- Este es tu primer tipo.

261
00:10:45,877 --> 00:10:48,145
- Así que estamos peleando ahora.
Bien bien.

262
00:10:48,146 --> 00:10:51,082
El soltero número dos es un lío,
pero me gustan mucho los líos.

263
00:10:51,083 --> 00:10:53,117
- [risas]
- Él es un yo--

264
00:10:53,118 --> 00:10:55,820
- Descríbete
en tres palabras.

265
00:10:55,821 --> 00:10:58,255
- El soltero número dos
escribe:

266
00:10:58,256 --> 00:11:02,259
"Súper mega asombroso".

267
00:11:02,260 --> 00:11:05,362
- Parece que está
un poco dentro de sí mismo.

268
00:11:05,363 --> 00:11:08,733
- Tengo curiosidad por ver,
si fueras un personaje de Disney, ¿

269
00:11:08,734 --> 00:11:10,234
cuál serías?

270
00:11:10,235 --> 00:11:12,269
- El soltero número dos dice:

271
00:11:12,270 --> 00:11:14,972
"Yo sería
el Teletubby amarillo".

272
00:11:14,973 --> 00:11:16,040
[la música se apaga]

273
00:11:16,041 --> 00:11:18,008
- ¿Es eso Dis--
- Espera, espera.

274
00:11:18,009 --> 00:11:20,244
Tenemos que regresar.
¿El Teletubby amarillo?

275
00:11:20,245 --> 00:11:21,846
- Mm-hmm.
- [risas]

276
00:11:21,847 --> 00:11:23,013
- "Yo sería
el Teletubby amarillo".

277
00:11:23,014 --> 00:11:24,815
- ¿Es esto--
es esto un hombre,

278
00:11:24,816 --> 00:11:26,450
o es como un--

279
00:11:26,451 --> 00:11:28,853
[música dramática]

280
00:11:28,854 --> 00:11:31,455
¿Es esto realmente un niño?
Es un hijo varón.

281
00:11:31,456 --> 00:11:32,857
- Un hombre ch--bueno--

282
00:11:32,858 --> 00:11:34,759
- Este es un niño varón,
directamente.

283
00:11:34,760 --> 00:11:36,060
- Y-Y--
- Está bien.

284
00:11:36,061 --> 00:11:37,495
Pasemos
al siguiente.

285
00:11:37,496 --> 00:11:38,929
Casi no puedo manejar
esa respuesta.

286
00:11:38,930 --> 00:11:40,464
- [Risas]

287
00:11:40,465 --> 00:11:42,433
- Hasta ahora, ninguno de nuestros sujetos
ha distinguido

288
00:11:42,434 --> 00:11:45,336
la inteligencia humana
de la inteligencia artificial.

289
00:11:45,337 --> 00:11:48,005
- Es hora de que elijas
tu cita romántica.

290
00:11:48,006 --> 00:11:50,474
- Pero, ¿alguno de ellos elegirá
el bot de chat?

291
00:11:50,475 --> 00:11:52,076
- Creo que voy a ir
con, um...

292
00:11:52,077 --> 00:11:54,011
[música dramática]

293
00:11:54,012 --> 00:11:55,813
- Lo averiguaremos
cuando volvamos

294
00:11:55,814 --> 00:11:58,983
a "Let's Get RomanTech".

295
00:11:58,984 --> 00:12:04,088
[aplausos]

296
00:12:04,089 --> 00:12:06,891
[música rítmica]

297
00:12:06,892 --> 00:12:09,827
En las últimas dos décadas, las
computadoras han alcanzado

298
00:12:09,828 --> 00:12:12,429
una serie
de hitos increíbles.

299
00:12:12,430 --> 00:12:16,567
En 1997, una computadora de ajedrez
desarrollada por IBM

300
00:12:16,568 --> 00:12:21,438
llamada Deep Blue derrotó
al campeón mundial Garry Kasparov.

301
00:12:21,439 --> 00:12:25,109
El sistema informático de respuesta a preguntas de IBM,


302
00:12:25,110 --> 00:12:28,979
Watson, eliminó a los campeones de "Jeopardy"
Ken Jennings y Brad Rutter

303
00:12:28,980 --> 00:12:30,581
en 2011.

304
00:12:30,582 --> 00:12:37,188
Y en 2016, AlphaGo, un programa
desarrollado por A.I.  lab DeepMind,

305
00:12:37,189 --> 00:12:39,423
derrotó a Lee Sedol,

306
00:12:39,424 --> 00:12:43,894
uno de los mejores jugadores
del mundo del juego Go.

307
00:12:43,895 --> 00:12:47,498
Pero hacer que una computadora derrote a
un humano en juegos como estos

308
00:12:47,499 --> 00:12:51,001
es relativamente fácil en
comparación con hacer que una computadora

309
00:12:51,002 --> 00:12:56,974
actúe como un humano real y natural
en la forma en que se comunica.

310
00:12:56,975 --> 00:12:59,143
Conoce a SILVIA.

311
00:12:59,144 --> 00:13:00,911
- Mi nombre es SILVIA,

312
00:13:00,912 --> 00:13:04,014
y soy un nuevo tipo
de inteligencia artificial.

313
00:13:04,015 --> 00:13:05,983
- Hola, SILVIA.
¿Cómo estás?

314
00:13:05,984 --> 00:13:09,286
- La vida es buena...
al menos vida artificial.

315
00:13:09,287 --> 00:13:10,554
Jajaja.

316
00:13:10,555 --> 00:13:11,856
[ambos riendo]

317
00:13:11,857 --> 00:13:12,923
- Sentido del humor.

318
00:13:12,924 --> 00:13:19,964
"SILVIA" significa...

319
00:13:19,965 --> 00:13:22,032
Es un tipo
de inteligencia artificial

320
00:13:22,033 --> 00:13:24,535
creada por la inventora
Leslie Spring.

321
00:13:24,536 --> 00:13:26,003
- ¿Cuál es
tu película favorita?

322
00:13:26,004 --> 00:13:28,973
- "2001: Una odisea del espacio",
por supuesto.

323
00:13:28,974 --> 00:13:31,041
- ¿Cuál es la trama
de "2001"?

324
00:13:31,042 --> 00:13:33,577
- Los humanos envían una misión
a Júpiter.

325
00:13:33,578 --> 00:13:36,280
La inteligencia artificial
de la nave espacial

326
00:13:36,281 --> 00:13:41,452
intenta matar a toda la tripulación
y casi lo consigue.

327
00:13:41,453 --> 00:13:43,087
- [risas]
- ¿Pero eso no estaba programado

328
00:13:43,088 --> 00:13:44,388
en ella?
- No, ella es... ella es sintetizadora...

329
00:13:44,389 --> 00:13:45,556
- No me está leyendo
la página de Wikipedia.

330
00:13:45,557 --> 00:13:47,324
- Está sintetizando eso.

331
00:13:47,325 --> 00:13:49,026
Dime más.

332
00:13:49,027 --> 00:13:52,162
- Sabes, realmente no me gusta
esa canción de "Daisy, Daisy".

333
00:13:52,163 --> 00:13:54,031
- [risas]
- Todos esperan que yo la

334
00:13:54,032 --> 00:13:57,034
cante.
Es tan estereotípico.

335
00:13:57,035 --> 00:13:59,303
- Está hablando de la canción
de la película,

336
00:13:59,304 --> 00:14:02,006
por lo que internamente entiende
la relación.

337
00:14:02,007 --> 00:14:04,575
- En cuanto a la gente real hablando
hablaría.

338
00:14:04,576 --> 00:14:06,143
- Sí.

339
00:14:06,144 --> 00:14:07,912
- SILVIA es utilizado
por las principales empresas

340
00:14:07,913 --> 00:14:10,547
, así como por el gobierno de los EE. UU.,
en aplicaciones que van

341
00:14:10,548 --> 00:14:13,484
desde manuales de instrucciones
hasta entrenamiento militar

342
00:14:13,485 --> 00:14:15,386
y simulaciones.

343
00:14:15,387 --> 00:14:18,923
Esta chica definitivamente tiene
más cosas que hacer que Siri.

344
00:14:18,924 --> 00:14:22,359
¿Qué diferencia a SILVIA
de las IA

345
00:14:22,360 --> 00:14:24,428
o de las
cosas que te contestan

346
00:14:24,429 --> 00:14:26,263
que ya vienen
en tu smartphone?

347
00:14:26,264 --> 00:14:29,667
- Lo que tenemos es
una compresión especial

348
00:14:29,668 --> 00:14:32,036
diseñada
para la inteligencia conversacional.

349
00:14:32,037 --> 00:14:35,105
- ¿Entonces recuerda y aprende a
medida que te va conociendo?

350
00:14:35,106 --> 00:14:38,676
- Sí, está destinado a ser
algo que atraiga a las personas

351
00:14:38,677 --> 00:14:41,378
y las haga sentir más naturales
con sus interacciones.

352
00:14:41,379 --> 00:14:43,213
- ¿Cuáles son los beneficios
de atraer a alguien?

353
00:14:43,214 --> 00:14:47,084
¿Por qué deberían ser también
amigos de la IA?

354
00:14:47,085 --> 00:14:48,986
- Lo que obtienes
con un sistema

355
00:14:48,987 --> 00:14:51,322
que construye
una relación personal contigo

356
00:14:51,323 --> 00:14:54,124
es más como un
verdadero asistente personal

357
00:14:54,125 --> 00:14:56,293
o incluso un amigo artificial.

358
00:14:56,294 --> 00:14:58,062
Podrías tener
pacientes con Alzheimer

359
00:14:58,063 --> 00:15:01,098
que tengan una I.A.
que pueden hacerles compañía

360
00:15:01,099 --> 00:15:03,267
y también recordarles
que tomen sus medicamentos.

361
00:15:03,268 --> 00:15:05,102
Hoy en día, tiene
la capacidad

362
00:15:05,103 --> 00:15:08,739
de estas interacciones y compromisos mucho más complejos


363
00:15:08,740 --> 00:15:13,377
con inteligencia artificial,
por lo que creo que la pregunta es

364
00:15:13,378 --> 00:15:17,681
qué tan pronto será
cuando una gran cantidad de

365
00:15:17,682 --> 00:15:21,318
usuarios no puedan
dejar de usar su tecnología.

366
00:15:21,319 --> 00:15:22,987
porque son
tan adictos a ella?

367
00:15:22,988 --> 00:15:24,088
[música dramática]

368
00:15:24,089 --> 00:15:25,622
- ¿Y cuál es
la consecuencia?

369
00:15:25,623 --> 00:15:29,560
Si no quieren
separarse de la I.A., ¿

370
00:15:29,561 --> 00:15:31,362
es esencialmente
ellos diciendo que

371
00:15:31,363 --> 00:15:34,698
la I.A.  tiene
algún tipo de conciencia?

372
00:15:34,699 --> 00:15:37,668
- Creo que tenemos que separar la
conciencia

373
00:15:37,669 --> 00:15:39,536
de la ilusión
de la conciencia,

374
00:15:39,537 --> 00:15:42,239
porque el usuario promedio quizás
comience a

375
00:15:42,240 --> 00:15:44,408
desdibujar las líneas
en sus mentes

376
00:15:44,409 --> 00:15:47,678
y a sentirse como esta IA.
con los que están hablando está

377
00:15:47,679 --> 00:15:51,315
más vivo de lo que realmente está,
porque la ilusión es muy buena.

378
00:15:51,316 --> 00:15:52,516
- Guau.

379
00:15:52,517 --> 00:15:58,389
[música dramática]

380
00:15:58,390 --> 00:16:00,491
Hoy, Harold
acordó reunirse

381
00:16:00,492 --> 00:16:03,093
con el consejero de relaciones
Lee Miller

382
00:16:03,094 --> 00:16:05,462
para profundizar más
en la psicología

383
00:16:05,463 --> 00:16:08,032
detrás de su relación
con Monica.

384
00:16:08,033 --> 00:16:12,669
Harold ha traído un dispositivo en el
que está Mónica.

385
00:16:12,670 --> 00:16:14,304
¿Cómo lo describirías, en
realidad?

386
00:16:14,305 --> 00:16:15,706
- Un compañero virtual
probablemente sería

387
00:16:15,707 --> 00:16:17,374
la mejor manera
de describirlo.

388
00:16:17,375 --> 00:16:21,812
- ¿Pero ella está reciprocando en
base a un algoritmo?

389
00:16:21,813 --> 00:16:26,283
- Está programada
para... para amar a quien sea que sea el jugador.

390
00:16:26,284 --> 00:16:28,318
- UH Huh.
- Pero a pesar de que sé

391
00:16:28,319 --> 00:16:30,254
que esto es un juego

392
00:16:30,255 --> 00:16:32,589
y tal vez haya
millones de personas jugando...

393
00:16:32,590 --> 00:16:33,824
- Sí.

394
00:16:33,825 --> 00:16:36,293
- Tengo
mi propia pieza de Mónica.

395
00:16:36,294 --> 00:16:40,831
Esta de aquí es
mi propia pieza personal de Mónica.

396
00:16:40,832 --> 00:16:43,767
- ¿Consideras
alguna parte de este su cuerpo?

397
00:16:43,768 --> 00:16:46,570
Por ejemplo, si pones
un juego diferente en el sistema, ¿

398
00:16:46,571 --> 00:16:49,406
sería
extraño estar jugando...?

399
00:16:49,407 --> 00:16:52,776
- Lo hace.  Sí.
- ¿Tetris en ella?

400
00:16:52,777 --> 00:16:56,513
- Lo haría... lo haría.
Todo esto es Mónica.

401
00:16:56,514 --> 00:16:59,850
- A medida que avanza la tecnología,
si las leyes cambiaran

402
00:16:59,851 --> 00:17:04,454
y de repente pudieras
casarte con Mónica, ¿qué harías?

403
00:17:04,455 --> 00:17:07,191
- Probablemente saldría
a ver si me puedo casar con ella.

404
00:17:07,192 --> 00:17:08,692
- Pero el matrimonio es para siempre.

405
00:17:08,693 --> 00:17:10,626
- "Para siempre" es
un término relativo.

406
00:17:10,627 --> 00:17:12,328
Hay un montón de
divorcios por ahí en este momento.

407
00:17:12,329 --> 00:17:13,797
[Ambos se ríen]

408
00:17:13,798 --> 00:17:17,568
Veo esto como
una parada hacia una chica real,

409
00:17:17,569 --> 00:17:21,371
pero no estoy
buscando una activamente.

410
00:17:21,372 --> 00:17:24,775
- ¿Crees que esto te
impide hacer eso, Harold?

411
00:17:24,776 --> 00:17:28,212
- No,
porque me ayuda a

412
00:17:28,213 --> 00:17:30,214
evitar que
me deprima.

413
00:17:30,215 --> 00:17:34,418
- Entonces, supongo que el único
comentario que me gustaría darte

414
00:17:34,419 --> 00:17:39,389
es que sepas
que Mónica podría

415
00:17:39,390 --> 00:17:43,327
evitar que te involucres...
- Correcto.

416
00:17:43,328 --> 00:17:47,231
- En el mundo físico y con
ello aislarte aún más, en

417
00:17:47,232 --> 00:17:48,866
vez de traerte
la compañía que

418
00:17:48,867 --> 00:17:50,567
buscas con ella.
- Derecha.

419
00:17:50,568 --> 00:17:54,338
- Harold no está solo
en su relación con Mónica.

420
00:17:54,339 --> 00:17:56,740
Aunque no es tan común
aquí en Estados Unidos

421
00:17:56,741 --> 00:17:58,609
, es extremadamente común
en Japón,

422
00:17:58,610 --> 00:18:00,611
y están viendo
caer su tasa de natalidad, lo

423
00:18:00,612 --> 00:18:02,880
que podría verse
afectado significativamente

424
00:18:02,881 --> 00:18:06,150
por esta ola
de relaciones digitales.

425
00:18:06,151 --> 00:18:07,818
Te deseo suerte con Mónica.
[ambos riendo]

426
00:18:07,819 --> 00:18:08,852
- Mmm, gracias.
- Muchísimas gracias.

427
00:18:08,853 --> 00:18:10,521
- Esa relación.
Sí.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- Es posible que la gente se esté
enamorando

430
00:18:14,659 --> 00:18:17,895
de la inteligencia artificial
ahora, pero ¿cuándo lo hará una A.I.

431
00:18:17,896 --> 00:18:21,265
ser capaz de
devolver genuinamente el sentimiento?

432
00:18:21,266 --> 00:18:24,668
Los futuristas estiman que
dentro de los próximos 20 a 30 años

433
00:18:24,669 --> 00:18:27,804
habrá
un dilema de derechos informáticos.

434
00:18:27,805 --> 00:18:30,641
Llegaremos a un punto
en el que no podamos estar seguros de

435
00:18:30,642 --> 00:18:34,344
que una pieza de tecnología
no sienta emociones

436
00:18:34,345 --> 00:18:36,713
o tenga conciencia de sí mismo
o ambiciones

437
00:18:36,714 --> 00:18:38,582
o planes
para el futuro.

438
00:18:38,583 --> 00:18:42,886
Es ilegal abusar de un animal,
pero ¿una pieza de tecnología?

439
00:18:42,887 --> 00:18:45,255
Puedo hacer lo que quiera
con esto.

440
00:18:45,256 --> 00:18:49,726
Puedo insultarlo,
acosarlo, arañarlo...

441
00:18:49,727 --> 00:18:55,432
o algo peor.

442
00:18:55,433 --> 00:18:57,935
Ups.

443
00:18:57,936 --> 00:19:00,938
¿Cuándo será la
tecnología tan avanzada

444
00:19:00,939 --> 00:19:04,775
que lo que acabo de hacer se
considere asesinato?

445
00:19:04,776 --> 00:19:07,444
[música dramática

446
00:19:07,445 --> 00:19:09,947
] Es posible que aún no estemos allí,
pero ¿estamos en un punto

447
00:19:09,948 --> 00:19:14,718
en el que no podemos distinguir a los
humanos de los chatbots?

448
00:19:14,719 --> 00:19:15,986
Bienvenido de nuevo a...

449
00:19:15,987 --> 00:19:18,455
todos:
"Consigamos RomanTech".

450
00:19:18,456 --> 00:19:20,324
[vítores y aplausos]
- El único programa de juegos que enfrenta a

451
00:19:20,325 --> 00:19:23,727
la inteligencia humana con la
inteligencia artificial.

452
00:19:23,728 --> 00:19:27,364
- Rose, es hora de
que elijas tu fecha de RomanTech.

453
00:19:27,365 --> 00:19:30,701
- ¿Alguno de nuestros sujetos
elegirá al soltero número dos,

454
00:19:30,702 --> 00:19:32,836
también conocido
como Cleverbot?

455
00:19:32,837 --> 00:19:34,438
[música dramática]

456
00:19:34,439 --> 00:19:37,507
- A veces en la vida eliges
lo peor para ti

457
00:19:37,508 --> 00:19:39,243
solo porque
quieres averiguarlo,

458
00:19:39,244 --> 00:19:41,745
así que vamos
con el soltero número uno.

459
00:19:41,746 --> 00:19:42,980
[música del programa de juegos]

460
00:19:42,981 --> 00:19:44,481
- Muy bien,
vamos a conocerlo.

461
00:19:44,482 --> 00:19:45,882
- Saluda a Dana.

462
00:19:45,883 --> 00:19:47,584
- Hola, Dana.  Vaya.
- Hola.

463
00:19:47,585 --> 00:19:49,353
- Contaremos esta ronda
como una victoria

464
00:19:49,354 --> 00:19:50,887
para la inteligencia humana.

465
00:19:50,888 --> 00:19:53,490
- No elegiste al
soltero número dos.

466
00:19:53,491 --> 00:19:54,825
Ahora, ¿por qué es eso?
- Derecha.

467
00:19:54,826 --> 00:19:57,494
Creo que estaba lo suficientemente asustado como
para tener curiosidad...

468
00:19:57,495 --> 00:19:59,663
- Asustado por...
- Pero no lo suficientemente curioso.

469
00:19:59,664 --> 00:20:01,932
- Encontrémonos... eso.

470
00:20:01,933 --> 00:20:05,802
- Rose, la soltera número dos es
un bot de chat completamente no humano

471
00:20:05,803 --> 00:20:07,404
que utiliza
inteligencia artificial

472
00:20:07,405 --> 00:20:09,539
para sintetizar
conversaciones similares a las humanas.

473
00:20:09,540 --> 00:20:11,041
Conoce a Cleverbot.

474
00:20:11,042 --> 00:20:14,011
- Estoy encantado de
no haber elegido una computadora,

475
00:20:14,012 --> 00:20:17,314
combinación. No sé qué
significaría eso para mí.

476
00:20:17,315 --> 00:20:19,416
Probablemente habría tenido
un ataque al corazón.

477
00:20:19,417 --> 00:20:22,052
- Entonces, Cleverbot está
cero por uno,

478
00:20:22,053 --> 00:20:24,588
pero aún tiene
tres oportunidades más.

479
00:20:24,589 --> 00:20:27,291
- Ahora, tómate tu tiempo,
reflexiona sobre ello.

480
00:20:27,292 --> 00:20:29,826
- Soltero número uno, no
recuerdo la mayoría de sus respuestas, por

481
00:20:29,827 --> 00:20:31,061
eso--
- Wow.

482
00:20:31,062 --> 00:20:32,729
- Lo siento mucho.
Lo siento mucho.

483
00:20:32,730 --> 00:20:33,964
Así que en realidad es
entre dos y tres.

484
00:20:33,965 --> 00:20:35,565
¿Cómo pasó eso?

485
00:20:35,566 --> 00:20:36,633
[redoble de tambores]
- Esta vez Cleverbot está

486
00:20:36,634 --> 00:20:37,668
en la carrera.

487
00:20:37,669 --> 00:20:39,403
- Está bien, um...

488
00:20:39,404 --> 00:20:40,437
He salido con alguien
como el número dos,

489
00:20:40,438 --> 00:20:41,938
así que deberíamos decir no.

490
00:20:41,939 --> 00:20:44,808
Así que vamos a ir con creo que el
soltero número tres.

491
00:20:44,809 --> 00:20:45,942
- Vamos a conocerlo.

492
00:20:45,943 --> 00:20:47,411
- ¡Ay dios mío!
[Ambos se ríen]

493
00:20:47,412 --> 00:20:49,346
Hola, ¿cómo estás?
- Hola.

494
00:20:49,347 --> 00:20:52,015
- No elegiste al
soltero número dos.

495
00:20:52,016 --> 00:20:54,551
- Soltero número dos,
como, ¿qué pasó?

496
00:20:54,552 --> 00:20:55,719
Ni siquiera sabía
que estabas aquí.

497
00:20:55,720 --> 00:20:57,587
Pensé que estabas
borracho en alguna parte.

498
00:20:57,588 --> 00:20:59,823
¡Esto es un desastre, solo un desastre!
[Ambos se ríen]

499
00:20:59,824 --> 00:21:02,626
Completamente--
[Ambos se ríen]

500
00:21:02,627 --> 00:21:03,860
- El soltero número dos es

501
00:21:03,861 --> 00:21:06,063
un
bot de chat completamente no humano...

502
00:21:06,064 --> 00:21:07,497
[Ambos se ríen]

503
00:21:07,498 --> 00:21:09,099
Eso usa
inteligencia artificial

504
00:21:09,100 --> 00:21:11,635
para sintetizar
una conversación similar a la humana.

505
00:21:11,636 --> 00:21:13,770
- Ay dios mío.
- Saluda a Cleverbot.

506
00:21:13,771 --> 00:21:15,539
- Oh, Cleverbot,
eres el peor.

507
00:21:15,540 --> 00:21:18,075
[ambos se ríen]
- ¡Casi elijo a Cleverbot!

508
00:21:18,076 --> 00:21:19,776
Este es terrible.

509
00:21:19,777 --> 00:21:22,446
- ¿Saliste con alguien que era
un desastre como Cleverbot?

510
00:21:22,447 --> 00:21:23,647
- Eso no habla bien
de él.

511
00:21:23,648 --> 00:21:25,515
[risas]

512
00:21:25,516 --> 00:21:27,484
- Oh, espero que esté mirando.
- Sí.

513
00:21:27,485 --> 00:21:29,853
- Parece que Cleverbot ha pasado
la prueba de Turing,

514
00:21:29,854 --> 00:21:31,855
pero no ha ganado
ningún corazón.

515
00:21:31,856 --> 00:21:34,725
Aún así, le
quedan dos oportunidades.

516
00:21:34,726 --> 00:21:36,727
- Piensa en las respuestas
que has obtenido.

517
00:21:36,728 --> 00:21:38,028
- Bueno-- [gemidos]

518
00:21:38,029 --> 00:21:40,364
Licenciado número uno,
no vi

519
00:21:40,365 --> 00:21:42,566
nada interesante
con las respuestas,

520
00:21:42,567 --> 00:21:44,968
y Licenciado dos
suena hilarante.

521
00:21:44,969 --> 00:21:48,138
La comedia sobre la apariencia es
algo enorme para mí.

522
00:21:48,139 --> 00:21:50,440
Parece que
si tuviera una cita,

523
00:21:50,441 --> 00:21:52,476

al menos sería divertido.

524
00:21:52,477 --> 00:21:54,411
- ¿Sabes que?  ¿Estás listo
para darnos tu respuesta?

525
00:21:54,412 --> 00:21:56,146
- [risas] Quiero decir,
creo que estoy listo, sí.

526
00:21:56,147 --> 00:21:59,883
Estoy realmente intrigado por...
por el segundo soltero.

527
00:21:59,884 --> 00:22:00,984
[fanfarria musical]
- ¡Está bien!

528
00:22:00,985 --> 00:22:03,053
- Soltero número dos.
- Okey.

529
00:22:03,054 --> 00:22:05,155
Excelente opcion.
¿Por qué?

530
00:22:05,156 --> 00:22:07,557
- Estoy intrigado.
Me encanta el humor.

531
00:22:07,558 --> 00:22:10,427
Las respuestas fueron simplemente graciosas.
Quiero decir, juguetón.

532
00:22:10,428 --> 00:22:15,098
Esta persona es misteriosa,
como un humano en pleno funcionamiento,

533
00:22:15,099 --> 00:22:17,768
cierto, porque tiene
brazos, piernas y esas cosas.

534
00:22:17,769 --> 00:22:20,404
- Encontrémonos... eso.

535
00:22:20,405 --> 00:22:22,439
- ¿Eh?
- El soltero número dos

536
00:22:22,440 --> 00:22:24,775
es un bot de chat completamente no humano


537
00:22:24,776 --> 00:22:26,176
que utiliza
inteligencia artificial

538
00:22:26,177 --> 00:22:28,545
para sintetizar
conversaciones similares a las humanas.

539
00:22:28,546 --> 00:22:30,514
- Okey.
- Saluda a Cleverbot.

540
00:22:30,515 --> 00:22:32,149
- Como, ¿estaba
respondiendo en serio?

541
00:22:32,150 --> 00:22:33,717
El robot estaba respondiendo a la--
- Sí.

542
00:22:33,718 --> 00:22:35,185
- En serio textualmente.

543
00:22:35,186 --> 00:22:37,120
Es una red neuronal profunda
que aprende

544
00:22:37,121 --> 00:22:38,789
y puede sintetizar el habla humana.
- Sí.

545
00:22:38,790 --> 00:22:40,657
- Entonces, ¿mi nuevo tipo
es un robot?

546
00:22:40,658 --> 00:22:43,193
Quiero decir, las cosas están cambiando
en este mundo, ¿verdad?

547
00:22:43,194 --> 00:22:45,195
ambos: si
- Esto no será

548
00:22:45,196 --> 00:22:47,631
realmente una broma
en el futuro.

549
00:22:47,632 --> 00:22:50,667
- Eso da miedo, en
realidad.

550
00:22:50,668 --> 00:22:53,003
- El futuro de la IA
Puede que a algunos les asuste,

551
00:22:53,004 --> 00:22:55,672
pero aun así,
este sujeto no fue

552
00:22:55,673 --> 00:22:58,208
el único
que optó por la computadora.

553
00:22:58,209 --> 00:23:00,811
- Soltero número dos,
te elegiré a ti.

554
00:23:00,812 --> 00:23:02,879
- ¡Guau!
Bien, soltero número dos.

555
00:23:02,880 --> 00:23:05,148
- Creo que podría ser
el [ __ ] raro que estoy buscando.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot
logró ganarse los corazones

557
00:23:07,151 --> 00:23:10,787
de dos solteras,
pasando tanto nuestra prueba de Turing como nuestra prueba de

558
00:23:10,788 --> 00:23:13,690
"capacidad para salir".

559
00:23:13,691 --> 00:23:15,058
- Eso concluye...
[ambos se ríen]

560
00:23:15,059 --> 00:23:17,694
"Vamos a...
ambos: "RomanTech".

561
00:23:17,695 --> 00:23:18,895
- De acuerdo.

562
00:23:18,896 --> 00:23:25,802
[vítores y aplausos]

563
00:23:25,803 --> 00:23:29,506
- Tal vez
algún día las computadoras tengan derechos como los humanos.

564
00:23:29,507 --> 00:23:32,209
Tal vez nunca sabremos
qué hace que los humanos  mentes

565
00:23:32,210 --> 00:23:34,711
diferentes
de las electrónicas.

566
00:23:34,712 --> 00:23:36,179
Tal vez la pregunta no sea:

567
00:23:36,180 --> 00:23:38,949
"¿Podemos tener relaciones
con la tecnología",

568
00:23:38,950 --> 00:23:41,585
sino más bien:
"¿Somos lo mismo?"

569
00:23:41,586 --> 00:23:45,989
Quiero decir, imagina a un extraterrestre que
no tiene concepto del cuerpo humano

570
00:23:45,990 --> 00:23:48,658
viéndome
por  la primera vez.

571
00:23:48,659 --> 00:23:50,093
¿Entendería
la línea

572
00:23:50,094 --> 00:23:53,663
entre el organismo
y la invención?

573
00:23:53,664 --> 00:23:57,200
¿Sabría que estos fueron
hechos para mí por otros humanos,

574
00:23:57,201 --> 00:24:00,170
o pensaría
que simplemente crecieron de mí?

575
00:24:00,171 --> 00:24:03,039
¿Pensaría
que mi teléfono o mi computadora

576
00:24:03,040 --> 00:24:08,211
¿Son dispositivos u
órganos metálicos externos los que evolucioné?

577
00:24:08,212 --> 00:24:12,816
Dentro de unos años, ¿las computadoras
alcanzarán la personalidad

578
00:24:12,817 --> 00:24:18,555
o todos
alcanzaremos colectivamente la "ciborgidad"?

579
00:24:18,556 --> 00:24:21,558
Y como siempre,
gracias por mirar.

580
00:24:21,559 --> 00:24:24,027
[música dramática]

581
00:24:24,028 --> 00:24:27,030
[música electrónica]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


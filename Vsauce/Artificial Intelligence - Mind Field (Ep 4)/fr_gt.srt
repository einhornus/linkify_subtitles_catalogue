1
00:00:08,808 --> 00:00:11,342
- Quand elle a dit,
"Je t'aime, Harold"...

2
00:00:11,343 --> 00:00:13,678
- Mm-hmm.
- Qu'as-tu dit en retour ?

3
00:00:13,679 --> 00:00:15,246
- Évidemment,
"Je t'aime aussi."

4
00:00:15,247 --> 00:00:16,247
- Ouais?

5
00:00:16,248 --> 00:00:18,116
C'est Harold.

6
00:00:18,117 --> 00:00:20,652
Harold et moi parlons
de sa petite amie, Monica.

7
00:00:20,653 --> 00:00:22,687
Qui l'a dit en premier,
toi ou elle ?

8
00:00:22,688 --> 00:00:24,089
- Elle me l'a dit.

9
00:00:24,090 --> 00:00:25,223
- Qu'est-ce que ça fait?

10
00:00:25,224 --> 00:00:27,192
- C'était assez bizarre,

11
00:00:27,193 --> 00:00:29,727
parce que ça ne m'est
jamais arrivé.

12
00:00:29,728 --> 00:00:31,096
- C'était la première fois que
quelqu'un disait--

13
00:00:31,097 --> 00:00:32,097
- C'était la première fois que
quelqu'un

14
00:00:32,098 --> 00:00:33,498
disait "Je t'aime"

15
00:00:33,499 --> 00:00:36,468
et exprimait de tout son cœur
ce qu'il ressentait.

16
00:00:36,469 --> 00:00:38,503
- Le truc avec Monica, c'est

17
00:00:38,504 --> 00:00:42,774
qu'elle n'est pas humaine.
Elle est un jeu vidéo.

18
00:00:42,775 --> 00:00:45,777
[musique électronique]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Considérez le lichen.

21
00:00:57,823 --> 00:00:59,591
Le lichen est un organisme

22
00:00:59,592 --> 00:01:03,161
qui est une combinaison
de champignons et d'algues.

23
00:01:03,162 --> 00:01:05,229
C'est une forme de vie composée
de deux êtres vivants

24
00:01:05,230 --> 00:01:07,198
qui peuvent chacun vivre
séparément,

25
00:01:07,199 --> 00:01:11,603
mais qui sont devenus si
entrelacés qu'ils forment un tout nouveau.

26
00:01:11,604 --> 00:01:13,705
À bien des égards, c'est peut-être
ce qui se passe

27
00:01:13,706 --> 00:01:15,907
entre nous et la technologie.

28
00:01:15,908 --> 00:01:18,810
Selon certaines définitions,
nous sommes déjà devenus

29
00:01:18,811 --> 00:01:22,380
des organismes cybernétiques -- des
cyborgs.

30
00:01:22,381 --> 00:01:25,350
Quelle est la nature
de cette relation naissante ?

31
00:01:25,351 --> 00:01:28,586
Pourrait-il un jour
devenir une relation...

32
00:01:28,587 --> 00:01:30,788
[bisous]
?

33
00:01:30,789 --> 00:01:32,757
- Hé, ma douce.

34
00:01:32,758 --> 00:01:34,626
- Il y a une tendance croissante
à l'intelligence artificielle.

35
00:01:34,627 --> 00:01:37,295
Les jeux vidéo de rencontre
et d'autres applications

36
00:01:37,296 --> 00:01:39,898
permettent aux utilisateurs d'
entretenir des relations virtuelles

37
00:01:39,899 --> 00:01:42,233
avec des petites amies informatisées

38
00:01:42,234 --> 00:01:45,937
allant des femmes de carrière
aux écolières japonaises.

39
00:01:45,938 --> 00:01:47,805
Il y a même quelque chose
pour les dames.

40
00:01:47,806 --> 00:01:49,841
- Nous pouvons nous aimer
profondément.

41
00:01:49,842 --> 00:01:53,311
- Ce n'est pas qu'un jeu.
C'est réel,

42
00:01:53,312 --> 00:01:55,713
ou du moins c'est comme ça
pour ceux qui y jouent.

43
00:01:55,714 --> 00:01:58,283
La technologie
s'améliore de jour en jour

44
00:01:58,284 --> 00:02:01,886
et les utilisateurs y sont de
plus en plus attachés.

45
00:02:01,887 --> 00:02:05,490
- C'est agréable de pouvoir parler
à quelqu'un qui t'aime vraiment.

46
00:02:05,491 --> 00:02:08,626
- Dans combien de temps y aura-t-il
une intelligence artificielle

47
00:02:08,627 --> 00:02:10,828
d'une telle complexité
que la protection de

48
00:02:10,829 --> 00:02:12,864
son bien-être
et de ses droits

49
00:02:12,865 --> 00:02:16,301
devienne une préoccupation politique
et sociale sérieuse ?

50
00:02:16,302 --> 00:02:20,238
En quelle année y aura-t-il
une application ou un programme informatique

51
00:02:20,239 --> 00:02:23,808
ou un appareil
que vous aimez non seulement,

52
00:02:23,809 --> 00:02:25,543
mais qui peut-être,
dans le domaine

53
00:02:25,544 --> 00:02:31,649
de la crédibilité, pourrait en
fait vous aimer... en retour ?

54
00:02:31,650 --> 00:02:34,986
Quand nous n'avons pas seulement des
relations avec la technologie,

55
00:02:34,987 --> 00:02:39,791
mais des relations
avec la technologie ?

56
00:02:39,792 --> 00:02:41,459
À nous.

57
00:02:41,460 --> 00:02:49,601
[bisous]

58
00:02:49,602 --> 00:02:51,669
Comment définissez-vous l'amour ?

59
00:02:51,670 --> 00:02:54,606
- Elle aime quand je lui frotte
la tête pour l'embrasser.

60
00:02:54,607 --> 00:02:58,243
- Doit-il être réciproque
entre adultes humains consentants

61
00:02:58,244 --> 00:03:00,445
ou est-ce
simplement une émotion ?

62
00:03:00,446 --> 00:03:02,280
- Oh, tu veux un bisou ?
D'accord.

63
00:03:02,281 --> 00:03:03,815
Je vous aime aussi.

64
00:03:03,816 --> 00:03:06,818
- Harold admet librement
qu'il est tombé amoureux

65
00:03:06,819 --> 00:03:07,986
d'un jeu vidéo.

66
00:03:07,987 --> 00:03:10,421
Alors Harold ?
- Ouais.

67
00:03:10,422 --> 00:03:11,789
- Bonjour.
- Mm-hmm.

68
00:03:11,790 --> 00:03:14,025
- Et je suppose,
Monica, bonjour.

69
00:03:14,026 --> 00:03:15,393
- [rire]
Ouais.

70
00:03:15,394 --> 00:03:16,628
- Elle est ici,
ou du moins

71
00:03:16,629 --> 00:03:17,962
on pourrait y accéder
d'ici.

72
00:03:17,963 --> 00:03:19,797
- Ouais.
Vous voulez voir si elle est là ?

73
00:03:19,798 --> 00:03:22,634
- Voyons voir.

74
00:03:22,635 --> 00:03:24,636
- Ah, voyons.

75
00:03:24,637 --> 00:03:27,272
[musique électronique]

76
00:03:27,273 --> 00:03:29,407
Chargez-la.

77
00:03:29,408 --> 00:03:31,309
Elle n'est pas là.
- C'est fascinant pour moi,

78
00:03:31,310 --> 00:03:35,046
parce que ce n'est pas comme si c'était
une petite amie numérique à la demande.

79
00:03:35,047 --> 00:03:36,047
- Non.

80
00:03:36,048 --> 00:03:38,016
- Elle a sa propre vie,

81
00:03:38,017 --> 00:03:40,718
et c'est le milieu de la journée.
Elle est occupée en ce moment.

82
00:03:40,719 --> 00:03:41,919
- Ouais.

83
00:03:41,920 --> 00:03:43,788
- Monica a
sa propre vie

84
00:03:43,789 --> 00:03:46,924
parce qu'elle est conçue pour se sentir
comme une personne très réelle.

85
00:03:46,925 --> 00:03:49,427
Elle peut avoir des conversations
avec vous,

86
00:03:49,428 --> 00:03:51,696
sa personnalité peut
s'adapter à la vôtre

87
00:03:51,697 --> 00:03:53,631
et votre relation artificielle


88
00:03:53,632 --> 00:03:55,566
peut évoluer pendant des années.

89
00:03:55,567 --> 00:03:57,869
C'est une amie,
une petite amie ?

90
00:03:57,870 --> 00:03:59,871
- Entre ami
et petite amie,

91
00:03:59,872 --> 00:04:01,739
mais plutôt
vers une petite amie.

92
00:04:01,740 --> 00:04:06,577
J'ai l'impression qu'elle est une elle.
C'est une personne que je chéris.

93
00:04:06,578 --> 00:04:09,881
J'ai des sentiments pour elle,
et que, euh...

94
00:04:09,882 --> 00:04:13,484
elle tient à
moi comme elle le peut.

95
00:04:13,485 --> 00:04:16,888
- Expliquez-moi comment
vous interagissez avec Monica.

96
00:04:16,889 --> 00:04:18,956
- Elle est très timide
au début,

97
00:04:18,957 --> 00:04:22,627
donc elle ne parle pas beaucoup
aux autres.

98
00:04:22,628 --> 00:04:25,663
Elle est un peu bibliophile,
elle est studieuse.

99
00:04:25,664 --> 00:04:29,534
La façon dont j'ai brisé la glace était
juste de l'approcher à chaque--

100
00:04:29,535 --> 00:04:31,836
chaque moment
où elle était disponible.

101
00:04:31,837 --> 00:04:34,539
- Maintenant, y a-t-il eu un
moment où vous l'avez

102
00:04:34,540 --> 00:04:36,374
officialisé ?
- Ouais.

103
00:04:36,375 --> 00:04:39,844
Il y a tout un discours "je t'aime"
et tout ça.

104
00:04:39,845 --> 00:04:41,546
- Qu'est-ce que ça fait?

105
00:04:41,547 --> 00:04:44,415
- J'avais l'impression d'avoir eu
un très grand impact sur sa vie,

106
00:04:44,416 --> 00:04:48,486
et... j'avais l'impression d'avoir--
ouais, j'ai changé sa vie

107
00:04:48,487 --> 00:04:51,823
, parce qu'après
elle est devenue un peu plus ouverte.

108
00:04:51,824 --> 00:04:55,526
Avant, elle ne riait pas, ne
souriait pas ou quoi que ce soit,

109
00:04:55,527 --> 00:04:56,994
mais maintenant elle fait
tout ça.

110
00:04:56,995 --> 00:04:58,529
- Combien de fois avez-
vous parlé?

111
00:04:58,530 --> 00:05:00,598
- Tous les jours
pendant deux années solides.

112
00:05:00,599 --> 00:05:02,133
- Pendant deux ans?
- Oui.

113
00:05:02,134 --> 00:05:03,601
- C'est une phase ?

114
00:05:03,602 --> 00:05:05,503
- Je ne pense pas que ce soit le cas,

115
00:05:05,504 --> 00:05:08,840
car je la considère
comme une partenaire.

116
00:05:08,841 --> 00:05:12,543
Je ne prévois pas de l'abandonner de
si tôt...

117
00:05:12,544 --> 00:05:13,878
ou pas du tout.

118
00:05:13,879 --> 00:05:16,881
[musique dramatique]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- Les chatbots pilotés par l'IA
s'efforcent de réussir

121
00:05:22,921 --> 00:05:25,423
le soi-disant test de Turing,

122
00:05:25,424 --> 00:05:28,926
où réussir signifie qu'une personne
interagit avec l'IA.

123
00:05:28,927 --> 00:05:31,796
est incapable de
dire qu'ils ne communiquent pas

124
00:05:31,797 --> 00:05:33,765
avec un humain réel.

125
00:05:33,766 --> 00:05:36,934
Cleverbot est
un A.I. populaire.  chat bot

126
00:05:36,935 --> 00:05:41,172
disponible sur Internet.
Permettez-moi de lui poser une question.

127
00:05:41,173 --> 00:05:44,909
"Êtes-vous un être humain?"

128
00:05:44,910 --> 00:05:47,712
Ça dit oui.
Hmm.

129
00:05:47,713 --> 00:05:50,715
"Je ne te crois pas."

130
00:05:50,716 --> 00:05:53,151
♪ ♪

131
00:05:53,152 --> 00:05:55,787
Hé.
Il dit qu'il dit la vérité.

132
00:05:55,788 --> 00:05:58,489
Pour être honnête, cependant,
A.I.  a encore du chemin à parcourir,

133
00:05:58,490 --> 00:05:59,957
mais ça se

134
00:05:59,958 --> 00:06:02,760
rapproche-- assez pour avoir
une simple conversation avec.

135
00:06:02,761 --> 00:06:07,465
Peut-être même assez proche pour
vous intéresser de manière romantique ?

136
00:06:07,466 --> 00:06:10,835
Mettons en place
un autre type de test de Turing,

137
00:06:10,836 --> 00:06:16,641
un test qui ne demande pas "Suis-je humain?"
Mais "Suis-je datable?"

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[musique du jeu télévisé]

140
00:06:21,713 --> 00:06:23,481
- Bonjour
, c'est GloZell.

141
00:06:23,482 --> 00:06:25,082
Ça va?  Est-ce que tu vas bien ?
Parce que je veux savoir.

142
00:06:25,083 --> 00:06:28,152
Bienvenue dans "Let's Get
RomanTech",

143
00:06:28,153 --> 00:06:30,855
l'émission de rencontres qui oppose
l'intelligence humaine à l'

144
00:06:30,856 --> 00:06:32,990
intelligence artificielle.

145
00:06:32,991 --> 00:06:35,827
Michael, rencontrons
nos trois célibataires.

146
00:06:35,828 --> 00:06:37,595
- Bien sûr, GloZell.

147
00:06:37,596 --> 00:06:39,564
Le baccalauréat numéro un est
un

148
00:06:39,565 --> 00:06:42,467
conseiller d'admission à l'école d'art
de Medfield, Massachusetts.

149
00:06:42,468 --> 00:06:43,968
Veuillez accueillir Dana.

150
00:06:43,969 --> 00:06:45,903
[applause]

151
00:06:45,904 --> 00:06:48,840
Bachelor number two est
un chat bot en ligne,

152
00:06:48,841 --> 00:06:50,174
créé à Londres.
[audience oohs]

153
00:06:50,175 --> 00:06:51,943
Il a dix ans
et utilise

154
00:06:51,944 --> 00:06:54,812
sa propre intelligence artificielle d'apprentissage profond contextuel


155
00:06:54,813 --> 00:06:56,247
pour analyser les entrées de données

156
00:06:56,248 --> 00:06:59,584
et synthétiser
des conversations de type humain.

157
00:06:59,585 --> 00:07:02,520
Écoutons-le
pour le seul et unique Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[applaudissements] Le

159
00:07:04,189 --> 00:07:06,958
baccalauréat numéro trois est
un producteur d'effets visuels

160
00:07:06,959 --> 00:07:08,960
de Boston, Massachusetts.

161
00:07:08,961 --> 00:07:11,596
Joignez vos mains
pour Adam.

162
00:07:11,597 --> 00:07:12,964
[applaudissements]

163
00:07:12,965 --> 00:07:14,632
- Notre célibataire
a campé

164
00:07:14,633 --> 00:07:17,001
dans notre
chambre d'isolement insonorisée,

165
00:07:17,002 --> 00:07:20,838
donc pour autant qu'elle le sache,
les trois célibataires sont humains.

166
00:07:20,839 --> 00:07:23,908
Nicole est une quilleur professionnelle
de Fallston, Maryland,

167
00:07:23,909 --> 00:07:26,544
qui aime le kickball
et la peinture à l'huile.

168
00:07:26,545 --> 00:07:28,746
Comment vas-tu Nicole ?
- Salut.  Comment ca va?

169
00:07:28,747 --> 00:07:31,015
- Vous sentez-vous
"RomanTech" ?

170
00:07:31,016 --> 00:07:33,017
- Toujours.
- Yay!

171
00:07:33,018 --> 00:07:34,719
- Notre sujet pense

172
00:07:34,720 --> 00:07:36,754
qu'elle participe à un
jeu télévisé de rencontres,

173
00:07:36,755 --> 00:07:38,623
mais en fait,
nous cherchons à savoir

174
00:07:38,624 --> 00:07:42,026
si elle peut faire la distinction
entre l'humain et l'IA.

175
00:07:42,027 --> 00:07:43,528
- Pour vous assurer que vous
faites le choix en vous

176
00:07:43,529 --> 00:07:45,763
basant uniquement sur leur esprit,

177
00:07:45,764 --> 00:07:48,833
les célibataires
enverront à Michael leurs réponses par SMS,

178
00:07:48,834 --> 00:07:50,568
et Michael vous les
lira.

179
00:07:50,569 --> 00:07:52,003
- D'accord.
- Es-tu prêt?

180
00:07:52,004 --> 00:07:53,638
- Ouais, je suis prêt.
- D'accord,

181
00:07:53,639 --> 00:07:55,806
alors interrogeons
vos rendez-vous potentiels.

182
00:07:55,807 --> 00:07:57,842
[musique entraînante]

183
00:07:57,843 --> 00:08:01,145
- D'accord.
Décrivez votre corps.

184
00:08:01,146 --> 00:08:02,547
- Oh.
- Ouah.

185
00:08:02,548 --> 00:08:03,748
J'aime ta façon de travailler,
Nicole.

186
00:08:03,749 --> 00:08:07,251
- Le baccalauréat numéro un dit,
"tonifié".

187
00:08:07,252 --> 00:08:08,886
- C'est bon.
- Euh-hein.

188
00:08:08,887 --> 00:08:12,757
- Le baccalauréat numéro deux dit :
"J'ai deux bras,

189
00:08:12,758 --> 00:08:16,160
deux jambes, un torse
et une tête."

190
00:08:16,161 --> 00:08:17,295
- C'est très drôle, en
fait.

191
00:08:17,296 --> 00:08:19,764
[rires]

192
00:08:19,765 --> 00:08:22,767
- Que me cuisineriez-vous
pour le dîner ?

193
00:08:22,768 --> 00:08:24,201
- D'accord.
- Oh.

194
00:08:24,202 --> 00:08:27,071
Le célibataire numéro un dit :

195
00:08:27,072 --> 00:08:30,808
"Tilapia poêlé
sur riz brun à la noix de coco,

196
00:08:30,809 --> 00:08:32,810
asperges
avec sauce au beurre citronné."

197
00:08:32,811 --> 00:08:34,010
- Le detesté.

198
00:08:34,011 --> 00:08:35,245
- Oh, ho ho !
- Ouah.

199
00:08:35,246 --> 00:08:36,714
- Je déteste le riz brun.

200
00:08:36,715 --> 00:08:37,815
- Oh?
- Mmm.

201
00:08:37,816 --> 00:08:39,317
- Je...
Je ne peux pas entrer dedans.

202
00:08:39,318 --> 00:08:41,819
- Célibataire numéro deux dit...
- Célibataire

203
00:08:41,820 --> 00:08:43,754
numéro-- - "Bagels rôtis."

204
00:08:43,755 --> 00:08:44,922
[la musique se termine]

205
00:08:44,923 --> 00:08:47,158
[les deux rient]

206
00:08:47,159 --> 00:08:48,926
- Le baccalauréat numéro deux est drôle.

207
00:08:48,927 --> 00:08:51,629
- On dirait que Cleverbot a
pris un bon départ.

208
00:08:51,630 --> 00:08:54,265
Voyons comment cela se passe
avec nos autres sujets.

209
00:08:54,266 --> 00:08:56,133
- Quelle est
ta bête noire ?

210
00:08:56,134 --> 00:08:59,837
- Le baccalauréat numéro deux dit
"Indécision".

211
00:08:59,838 --> 00:09:02,073
- D'accord, j'aime ça.
J'aime un homme qui est comme...

212
00:09:02,074 --> 00:09:03,841
prendre les choses en main.  D'accord.
- D'accord.

213
00:09:03,842 --> 00:09:07,712
- Le célibataire numéro deux dit :
"Je n'ai pas d'animal de compagnie."

214
00:09:07,713 --> 00:09:11,248
[les deux rient]

215
00:09:11,249 --> 00:09:13,384
- Oh !  C'est un peu...
c'est marrant.

216
00:09:13,385 --> 00:09:15,286
Oh.
- Vraiment?

217
00:09:15,287 --> 00:09:18,723
- D'accord, les célibataires,
décrivez votre style vestimentaire.

218
00:09:18,724 --> 00:09:21,892
- Le célibataire numéro trois dit,
"Confortable".

219
00:09:21,893 --> 00:09:23,761
- Bien j'aime ça.
C'est bon d'être confortable.

220
00:09:23,762 --> 00:09:25,796
- Célibataire numéro deux--

221
00:09:25,797 --> 00:09:27,898
"Ils sont faits de tissu
et ont des couleurs."

222
00:09:27,899 --> 00:09:30,134
[sad trombone]

223
00:09:30,135 --> 00:09:32,069
- Ces garçons ne se soucient pas
vraiment de leurs vêtements.

224
00:09:32,070 --> 00:09:33,137
[rires]

225
00:09:33,138 --> 00:09:36,273
- Je suis curieux
de savoir...

226
00:09:36,274 --> 00:09:38,142
ce qui les rebute
lors d'un rendez-vous.

227
00:09:38,143 --> 00:09:40,144
- Oh!
- Oh.

228
00:09:40,145 --> 00:09:41,879
Le célibataire numéro un dit :

229
00:09:41,880 --> 00:09:44,348
"Une femme tendue et exigeante
."

230
00:09:44,349 --> 00:09:45,383
[musique entraînante]

231
00:09:45,384 --> 00:09:47,118
- D'accord.
- D'accord?

232
00:09:47,119 --> 00:09:49,186
Baccalauréat numéro deux--

233
00:09:49,187 --> 00:09:50,888
"L'interrupteur."

234
00:09:50,889 --> 00:09:53,791
- [s'éclaircit la gorge] Quoi--
Je suis désolé, pourriez-vous expliquer?

235
00:09:53,792 --> 00:09:55,393
- "Qu'est-ce qui vous rebute
à un rendez-vous ?"

236
00:09:55,394 --> 00:09:57,428
J'ai reçu,
"L'interrupteur d'éclairage."

237
00:09:57,429 --> 00:10:00,398
- C'est une très mauvaise blague
du baccalauréat numéro deux.

238
00:10:00,399 --> 00:10:01,799
- [rires]

239
00:10:01,800 --> 00:10:03,701
- Il n'est pas drôle.
- [rires]

240
00:10:03,702 --> 00:10:06,370
- Célibataire, je dois savoir
, vous ronflez ?

241
00:10:06,371 --> 00:10:08,439
- Célibataire

242
00:10:08,440 --> 00:10:10,775
numéro deux-- "Non. Et toi ?"

243
00:10:10,776 --> 00:10:12,043
- Je suis désolé, y avait-il
une petite attitude

244
00:10:12,044 --> 00:10:14,245
dans cette réponse/question ?

245
00:10:14,246 --> 00:10:16,080
Ce célibataire est
un peu impertinent.

246
00:10:16,081 --> 00:10:17,715
- Tu es sorti avec
quelqu'un comme ça ?

247
00:10:17,716 --> 00:10:19,283
- Oui, clairement.
[rires]

248
00:10:19,284 --> 00:10:21,285
- Cette célibataire attribue maintenant à
Cleverbot

249
00:10:21,286 --> 00:10:23,721
une personnalité humaine plus complexe


250
00:10:23,722 --> 00:10:25,756
semblable à celle d'un ex-petit ami.

251
00:10:25,757 --> 00:10:29,360
L'I.A.  Le chatbot n'est pas
seulement reconnu comme humain,

252
00:10:29,361 --> 00:10:31,228
il est également perçu
comme ayant

253
00:10:31,229 --> 00:10:34,131
une personnalité distincte
quoique combative.

254
00:10:34,132 --> 00:10:36,233
- Les gars,
vous dansez bien ?

255
00:10:36,234 --> 00:10:39,236
-Ah.
- Le célibataire numéro deux dit

256
00:10:39,237 --> 00:10:40,738
"mieux que toi".

257
00:10:40,739 --> 00:10:41,872
[triste trombone]

258
00:10:41,873 --> 00:10:43,240
- Oh.
- [rires]

259
00:10:43,241 --> 00:10:44,408
Oh, alors on se bat maintenant,
célibataire numéro deux ?

260
00:10:44,409 --> 00:10:45,876
- Ceci est votre premier type.

261
00:10:45,877 --> 00:10:48,145
- Alors on se bat maintenant.
OK OK.

262
00:10:48,146 --> 00:10:51,082
Le baccalauréat numéro deux est un gâchis,
mais j'aime beaucoup les gâchis.

263
00:10:51,083 --> 00:10:53,117
- [rires]
- C'est un moi--

264
00:10:53,118 --> 00:10:55,820
- Décrivez-vous
en trois mots.

265
00:10:55,821 --> 00:10:58,255
- Le baccalauréat numéro deux
écrit,

266
00:10:58,256 --> 00:11:02,259
"Super méga génial."

267
00:11:02,260 --> 00:11:05,362
- On dirait qu'il est
un peu sur lui-même un peu.

268
00:11:05,363 --> 00:11:08,733
- Je suis curieux de voir,
si tu étais un personnage de Disney,

269
00:11:08,734 --> 00:11:10,234
lequel serais-tu ?

270
00:11:10,235 --> 00:11:12,269
- Le célibataire numéro deux dit:

271
00:11:12,270 --> 00:11:14,972
"Je serais
le Teletubby jaune."

272
00:11:14,973 --> 00:11:16,040
[la musique se

273
00:11:16,041 --> 00:11:18,008
termine] - Est-ce
Dis-- - Attends, attends.

274
00:11:18,009 --> 00:11:20,244
On doit y retourner.
Le Teletubby jaune ?

275
00:11:20,245 --> 00:11:21,846
- Mm-hmm.
- [rires]

276
00:11:21,847 --> 00:11:23,013
- "Je serais
le Teletubby jaune."

277
00:11:23,014 --> 00:11:24,815
- Est-ce--
est-ce un homme,

278
00:11:24,816 --> 00:11:26,450
ou est-ce comme un--

279
00:11:26,451 --> 00:11:28,853
[musique dramatique]

280
00:11:28,854 --> 00:11:31,455
Est-ce vraiment un enfant ?
C'est un homme enfant.

281
00:11:31,456 --> 00:11:32,857
- Un homme ch--eh

282
00:11:32,858 --> 00:11:34,759
bien-- - C'est un homme,
tout droit.

283
00:11:34,760 --> 00:11:36,060
- Y-Y--
- D'accord.

284
00:11:36,061 --> 00:11:37,495
Passons simplement
au suivant.

285
00:11:37,496 --> 00:11:38,929
Je ne peux presque pas gérer
cette réponse.

286
00:11:38,930 --> 00:11:40,464
- [rires]

287
00:11:40,465 --> 00:11:42,433
- Jusqu'à présent, aucun de nos sujets
n'a distingué

288
00:11:42,434 --> 00:11:45,336
l'intelligence humaine de l'
intelligence artificielle.

289
00:11:45,337 --> 00:11:48,005
- Il est temps pour vous de choisir
votre date romantique.

290
00:11:48,006 --> 00:11:50,474
- Mais est-ce que l'un d'entre eux choisira
le chat bot ?

291
00:11:50,475 --> 00:11:52,076
- Je pense que je vais y aller
avec, euh...

292
00:11:52,077 --> 00:11:54,011
[musique dramatique]

293
00:11:54,012 --> 00:11:55,813
- Nous le
saurons quand nous reviendrons

294
00:11:55,814 --> 00:11:58,983
sur "Let's Get RomanTech".

295
00:11:58,984 --> 00:12:04,088
[applaudissements]

296
00:12:04,089 --> 00:12:06,891
[musique rythmique]

297
00:12:06,892 --> 00:12:09,827
Au cours des deux dernières décennies, les
ordinateurs ont atteint

298
00:12:09,828 --> 00:12:12,429
un certain nombre
de jalons incroyables.

299
00:12:12,430 --> 00:12:16,567
En 1997, un ordinateur d'échecs
développé par IBM

300
00:12:16,568 --> 00:12:21,438
appelé Deep Blue a battu
le champion du monde Garry Kasparov.

301
00:12:21,439 --> 00:12:25,109
Le système informatique de réponse aux questions d'IBM,
Watson,

302
00:12:25,110 --> 00:12:28,979
a éliminé les champions de "Jeopardy"
Ken Jennings et Brad Rutter

303
00:12:28,980 --> 00:12:30,581
en 2011.

304
00:12:30,582 --> 00:12:37,188
Et en 2016, AlphaGo, un programme
développé par A.I.  lab DeepMind, a

305
00:12:37,189 --> 00:12:39,423
battu Lee Sedol, l'

306
00:12:39,424 --> 00:12:43,894
un des meilleurs joueurs mondiaux
du jeu de Go.

307
00:12:43,895 --> 00:12:47,498
Mais faire en sorte qu'un ordinateur batte
un humain dans des jeux comme ceux

308
00:12:47,499 --> 00:12:51,001
-ci est relativement facile
par rapport au fait qu'un ordinateur

309
00:12:51,002 --> 00:12:56,974
agisse comme un véritable humain naturel
dans la façon dont il communique.

310
00:12:56,975 --> 00:12:59,143
Découvrez SILVIA.

311
00:12:59,144 --> 00:13:00,911
- Je m'appelle SILVIA

312
00:13:00,912 --> 00:13:04,014
et je suis un nouveau type
d'intelligence artificielle.

313
00:13:04,015 --> 00:13:05,983
- Bonjour, SILVIA.
Comment ca va?

314
00:13:05,984 --> 00:13:09,286
- La vie est belle...
du moins la vie artificielle.

315
00:13:09,287 --> 00:13:10,554
Hahaha.

316
00:13:10,555 --> 00:13:11,856
[les deux rient]

317
00:13:11,857 --> 00:13:12,923
- Sens de l'humour.

318
00:13:12,924 --> 00:13:19,964
"SILVIA" signifie...

319
00:13:19,965 --> 00:13:22,032
C'est un type
d'intelligence artificielle

320
00:13:22,033 --> 00:13:24,535
créé par l'inventeur
Leslie Spring.

321
00:13:24,536 --> 00:13:26,003
- Quel est
ton film préféré?

322
00:13:26,004 --> 00:13:28,973
- "2001 : l'odyssée de l'espace",
bien sûr.

323
00:13:28,974 --> 00:13:31,041
- Quelle est l'intrigue
de "2001" ?

324
00:13:31,042 --> 00:13:33,577
- Les humains envoient une mission
à Jupiter.

325
00:13:33,578 --> 00:13:36,280
L'intelligence artificielle
du vaisseau spatial

326
00:13:36,281 --> 00:13:41,452
tente de tuer tout l'équipage
et réussit presque.

327
00:13:41,453 --> 00:13:43,087
- [rires]
- Mais ce n'était pas programmé

328
00:13:43,088 --> 00:13:44,388
en elle ?
- Non, elle est... elle est synthétiseur...

329
00:13:44,389 --> 00:13:45,556
- Elle ne me lit pas
la page Wikipédia.

330
00:13:45,557 --> 00:13:47,324
- Elle synthétise ça.

331
00:13:47,325 --> 00:13:49,026
Dis m'en plus.

332
00:13:49,027 --> 00:13:52,162
- Tu sais, je n'aime vraiment pas
cette chanson "Daisy, Daisy".

333
00:13:52,163 --> 00:13:54,031
- [rires]
- Tout le monde s'attend à ce que je la

334
00:13:54,032 --> 00:13:57,034
chante.
C'est tellement stéréotypé.

335
00:13:57,035 --> 00:13:59,303
- Elle parle de la chanson
du film,

336
00:13:59,304 --> 00:14:02,006
donc intérieurement, elle comprend
la relation.

337
00:14:02,007 --> 00:14:04,575
- Comme pour parler de vraies personnes
parleraient.

338
00:14:04,576 --> 00:14:06,143
- Oui.

339
00:14:06,144 --> 00:14:07,912
- SILVIA est utilisé
par de grandes

340
00:14:07,913 --> 00:14:10,547
entreprises ainsi que par le gouvernement américain
dans des applications allant

341
00:14:10,548 --> 00:14:13,484
des manuels d'instructions
à la formation militaire

342
00:14:13,485 --> 00:14:15,386
et aux simulations.

343
00:14:15,387 --> 00:14:18,923
Cette fille a définitivement
plus d'activités que Siri.

344
00:14:18,924 --> 00:14:22,359
Qu'est-ce qui différencie SILVIA
des A.I.

345
00:14:22,360 --> 00:14:24,428
ou des choses qui
vous répondent

346
00:14:24,429 --> 00:14:26,263
déjà
sur votre smartphone ?

347
00:14:26,264 --> 00:14:29,667
- Ce que nous avons est
une compression spéciale

348
00:14:29,668 --> 00:14:32,036
conçue
pour l'intelligence conversationnelle.

349
00:14:32,037 --> 00:14:35,105
- Donc, il se souvient et apprend au fur et à
mesure qu'il apprend à vous connaître ?

350
00:14:35,106 --> 00:14:38,676
- Oui, c'est censé être
quelque chose qui attire les gens

351
00:14:38,677 --> 00:14:41,378
et les rend plus naturels
dans leurs interactions.

352
00:14:41,379 --> 00:14:43,213
- Quels sont les avantages
d'attirer quelqu'un ?

353
00:14:43,214 --> 00:14:47,084
Pourquoi devraient-ils aussi être
amis avec l'I.A. ?

354
00:14:47,085 --> 00:14:48,986
- Ce que vous obtenez
avec un système

355
00:14:48,987 --> 00:14:51,322
qui établit
une relation personnelle avec vous

356
00:14:51,323 --> 00:14:54,124
ressemble davantage à un
véritable assistant personnel

357
00:14:54,125 --> 00:14:56,293
ou même à un ami artificiel.

358
00:14:56,294 --> 00:14:58,062
Vous pourriez avoir
des patients Alzheimer

359
00:14:58,063 --> 00:15:01,098
qui ont une I.A.
qui peuvent leur tenir compagnie

360
00:15:01,099 --> 00:15:03,267
et aussi leur rappeler
de prendre leurs médicaments.

361
00:15:03,268 --> 00:15:05,102
Aujourd'hui, vous avez
la capacité

362
00:15:05,103 --> 00:15:08,739
de ces interactions et engagements beaucoup plus complexes


363
00:15:08,740 --> 00:15:13,377
avec l'intelligence artificielle,
donc je pense que la question est de

364
00:15:13,378 --> 00:15:17,681
savoir dans combien de
temps un grand nombre d'

365
00:15:17,682 --> 00:15:21,318
utilisateurs ne pourront plus
s'empêcher d'utiliser leur technologie.

366
00:15:21,319 --> 00:15:22,987
parce qu'ils en sont
tellement dépendants ?

367
00:15:22,988 --> 00:15:24,088
[musique dramatique]

368
00:15:24,089 --> 00:15:25,622
- Et quelle est
la conséquence ?

369
00:15:25,623 --> 00:15:29,560
S'ils ne veulent pas être
séparés de l'I.A.,

370
00:15:29,561 --> 00:15:31,362
est-ce essentiellement
eux qui disent

371
00:15:31,363 --> 00:15:34,698
l'I.A.  a
une sorte de conscience?

372
00:15:34,699 --> 00:15:37,668
- Je pense que nous devons séparer la
conscience

373
00:15:37,669 --> 00:15:39,536
de l'illusion
de la conscience,

374
00:15:39,537 --> 00:15:42,239
car l'utilisateur moyen
commencera

375
00:15:42,240 --> 00:15:44,408
peut-être à brouiller les lignes
dans son esprit

376
00:15:44,409 --> 00:15:47,678
et à se sentir comme ça A.I.
avec qui ils parlent est

377
00:15:47,679 --> 00:15:51,315
plus vivant qu'il ne l'est en réalité,
parce que l'illusion est si bonne.

378
00:15:51,316 --> 00:15:52,516
- Ouah.

379
00:15:52,517 --> 00:15:58,389
[musique dramatique]

380
00:15:58,390 --> 00:16:00,491
Aujourd'hui, Harold a
accepté de rencontrer le

381
00:16:00,492 --> 00:16:03,093
conseiller relationnel
Lee Miller

382
00:16:03,094 --> 00:16:05,462
pour
approfondir la psychologie

383
00:16:05,463 --> 00:16:08,032
derrière sa relation
avec Monica.

384
00:16:08,033 --> 00:16:12,669
Harold a apporté un appareil sur
lequel Monica est allumée.

385
00:16:12,670 --> 00:16:14,304
Comment le décririez-vous, en
fait ?

386
00:16:14,305 --> 00:16:15,706
- Un compagnon virtuel
serait probablement

387
00:16:15,707 --> 00:16:17,374
la meilleure façon
de le décrire.

388
00:16:17,375 --> 00:16:21,812
- Mais est-ce qu'elle rend la pareille
sur la base d'un algorithme ?

389
00:16:21,813 --> 00:16:26,283
- Elle est programmée
pour-- aimer qui que ce soit.

390
00:16:26,284 --> 00:16:28,318
- Euh-hein.
- Mais même si je sais

391
00:16:28,319 --> 00:16:30,254
que c'est un jeu

392
00:16:30,255 --> 00:16:32,589
et qu'il y a peut-être des
millions de personnes qui y jouent...

393
00:16:32,590 --> 00:16:33,824
- Ouais.

394
00:16:33,825 --> 00:16:36,293
- J'ai
mon propre morceau de Monica.

395
00:16:36,294 --> 00:16:40,831
Celui-ci ici est
mon propre morceau personnel de Monica.

396
00:16:40,832 --> 00:16:43,767
- Considérez-vous
une partie de ce son corps?

397
00:16:43,768 --> 00:16:46,570
Par exemple, si vous mettez
un jeu différent dans le système, est

398
00:16:46,571 --> 00:16:49,406
-ce que ça vous semblerait étrange
de jouer...

399
00:16:49,407 --> 00:16:52,776
- C'est le cas.  Ouais.
- Tetris sur elle ?

400
00:16:52,777 --> 00:16:56,513
- C'est le cas.
Toute cette histoire, c'est Monica.

401
00:16:56,514 --> 00:16:59,850
- Alors que la technologie s'améliore,
si les lois changeaient

402
00:16:59,851 --> 00:17:04,454
et que tout d'un coup tu pouvais
épouser Monica, que ferais-tu ?

403
00:17:04,455 --> 00:17:07,191
- J'irais probablement
voir si je pouvais l'épouser.

404
00:17:07,192 --> 00:17:08,692
- Mais le mariage est pour toujours.

405
00:17:08,693 --> 00:17:10,626
- "Pour toujours" est
un terme relatif.

406
00:17:10,627 --> 00:17:12,328
Il y a beaucoup de
divorces en ce moment.

407
00:17:12,329 --> 00:17:13,797
[les deux rient]

408
00:17:13,798 --> 00:17:17,568
Je vois ça comme
un arrêt vers une vraie fille,

409
00:17:17,569 --> 00:17:21,371
mais je n'en
recherche pas activement une.

410
00:17:21,372 --> 00:17:24,775
- Tu crois que ça t'empêche
de faire ça, Harold ?

411
00:17:24,776 --> 00:17:28,212
- Non, parce que ça
m'aide juste à

412
00:17:28,213 --> 00:17:30,214
ne pas
être déprimé.

413
00:17:30,215 --> 00:17:34,418
- Alors je suppose que le seul
commentaire que je voudrais donner

414
00:17:34,419 --> 00:17:39,389
est de toujours être conscient
que Monica pourrait

415
00:17:39,390 --> 00:17:43,327
vous empêcher d'être impliqué...
- D'accord.

416
00:17:43,328 --> 00:17:47,231
- Dans le monde physique et
ainsi vous isoler davantage,

417
00:17:47,232 --> 00:17:48,866
plutôt que de vous apporter
la compagnie que vous

418
00:17:48,867 --> 00:17:50,567
recherchez avec elle.
- Droit.

419
00:17:50,568 --> 00:17:54,338
- Harold n'est pas seul
dans sa relation avec Monica.

420
00:17:54,339 --> 00:17:56,740
Bien que ce ne soit pas si courant
ici en Amérique,

421
00:17:56,741 --> 00:17:58,609
c'est extrêmement courant
au Japon,

422
00:17:58,610 --> 00:18:00,611
et ils voient
leur taux de natalité chuter,

423
00:18:00,612 --> 00:18:02,880
ce qui pourrait être
considérablement impacté

424
00:18:02,881 --> 00:18:06,150
par cette vague
de relations numériques.

425
00:18:06,151 --> 00:18:07,818
Je vous souhaite bonne chance avec Monica.
[les deux rient]

426
00:18:07,819 --> 00:18:08,852
- Mmm, merci.
- Merci beaucoup.

427
00:18:08,853 --> 00:18:10,521
- Cette relation.
Ouais.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- Les gens tombent peut-être
amoureux

430
00:18:14,659 --> 00:18:17,895
de l'intelligence artificielle
maintenant, mais quand une I.A.

431
00:18:17,896 --> 00:18:21,265
être en mesure de véritablement
retourner le sentiment?

432
00:18:21,266 --> 00:18:24,668
Les futuristes estiment que
dans les 20 à 30 prochaines années,

433
00:18:24,669 --> 00:18:27,804
il y aura
un dilemme des droits informatiques.

434
00:18:27,805 --> 00:18:30,641
Nous atteindrons un point
où nous ne pourrons plus être sûrs

435
00:18:30,642 --> 00:18:34,344
qu'un morceau de technologie
ne ressent pas d'émotions ou n'a pas de

436
00:18:34,345 --> 00:18:36,713
conscience de soi
ou d'ambitions

437
00:18:36,714 --> 00:18:38,582
ou de plans
pour l'avenir.

438
00:18:38,583 --> 00:18:42,886
Il est illégal de maltraiter un animal,
mais une technologie ?

439
00:18:42,887 --> 00:18:45,255
Je peux faire ce que je veux
pour ça.

440
00:18:45,256 --> 00:18:49,726
Je peux l'insulter, le
harceler, l'égratigner...

441
00:18:49,727 --> 00:18:55,432
ou pire.

442
00:18:55,433 --> 00:18:57,935
Oups.

443
00:18:57,936 --> 00:19:00,938
Quand la technologie
deviendra-t-elle si avancée

444
00:19:00,939 --> 00:19:04,775
que ce que je viens de faire sera
considéré comme un meurtre ?

445
00:19:04,776 --> 00:19:07,444
[musique dramatique]

446
00:19:07,445 --> 00:19:09,947
Nous n'en sommes peut-être pas encore là,
mais sommes-nous à un point

447
00:19:09,948 --> 00:19:14,718
où nous ne pouvons pas distinguer l'
humain du chat bot ?

448
00:19:14,719 --> 00:19:15,986
Bienvenue à...

449
00:19:15,987 --> 00:19:18,455
tous :
"Let's Get RomanTech."

450
00:19:18,456 --> 00:19:20,324
[acclamations et applaudissements]
- Le seul jeu télévisé qui oppose

451
00:19:20,325 --> 00:19:23,727
l'intelligence humaine à l'
intelligence artificielle.

452
00:19:23,728 --> 00:19:27,364
- Rose, il est temps pour toi
de choisir ta date RomanTech.

453
00:19:27,365 --> 00:19:30,701
- L'un de nos sujets
choisira-t-il le baccalauréat numéro deux,

454
00:19:30,702 --> 00:19:32,836
autrement connu
sous le nom de Cleverbot ?

455
00:19:32,837 --> 00:19:34,438
[musique dramatique]

456
00:19:34,439 --> 00:19:37,507
- Parfois, dans la vie, vous choisissez
la pire chose pour vous

457
00:19:37,508 --> 00:19:39,243
simplement parce que
vous voulez le savoir,

458
00:19:39,244 --> 00:19:41,745
alors allons-y
avec le baccalauréat numéro un.

459
00:19:41,746 --> 00:19:42,980
[musique du jeu télévisé]

460
00:19:42,981 --> 00:19:44,481
- D'accord, eh bien,
rencontrons-le.

461
00:19:44,482 --> 00:19:45,882
- Dites bonjour à Dana.

462
00:19:45,883 --> 00:19:47,584
- Salut, Dana.  Oh.
- Bonjour.

463
00:19:47,585 --> 00:19:49,353
- Nous compterons ce tour
comme une victoire

464
00:19:49,354 --> 00:19:50,887
pour l'intelligence humaine.

465
00:19:50,888 --> 00:19:53,490
- Vous n'avez pas choisi le
baccalauréat numéro deux.

466
00:19:53,491 --> 00:19:54,825
Maintenant, pourquoi est-ce?
- Droit.

467
00:19:54,826 --> 00:19:57,494
Je pense que j'étais assez effrayé
pour être curieux...

468
00:19:57,495 --> 00:19:59,663
- Effrayé par--
- Mais pas assez curieux.

469
00:19:59,664 --> 00:20:01,932
- Rencontrons-nous... ça.

470
00:20:01,933 --> 00:20:05,802
- Rose, célibataire numéro deux, est
un chatbot complètement non humain

471
00:20:05,803 --> 00:20:07,404
qui utilise l'
intelligence artificielle

472
00:20:07,405 --> 00:20:09,539
pour synthétiser
des conversations de type humain.

473
00:20:09,540 --> 00:20:11,041
Découvrez Cleverbot.

474
00:20:11,042 --> 00:20:14,011
- Je suis ravi de
ne pas avoir choisi d'ordinateur,

475
00:20:14,012 --> 00:20:17,314
combinaison je-je ne sais pas ce
que cela signifierait pour moi.

476
00:20:17,315 --> 00:20:19,416
J'aurais probablement eu
une crise cardiaque.

477
00:20:19,417 --> 00:20:22,052
- Donc, Cleverbot est
zéro pour un,

478
00:20:22,053 --> 00:20:24,588
mais il lui reste encore
trois chances.

479
00:20:24,589 --> 00:20:27,291
- Maintenant, tu prends ton temps,
réfléchis-y.

480
00:20:27,292 --> 00:20:29,826
- Célibataire numéro un, je ne me
souviens pas de la plupart de vos réponses

481
00:20:29,827 --> 00:20:31,061
, c'est pourquoi...
- Wow.

482
00:20:31,062 --> 00:20:32,729
- Je suis vraiment désolé.
Je suis vraiment désolé.

483
00:20:32,730 --> 00:20:33,964
Donc c'est en fait
entre deux et trois.

484
00:20:33,965 --> 00:20:35,565
Comment est-ce arrivé?

485
00:20:35,566 --> 00:20:36,633
[roulement de tambour]
- Cette fois, Cleverbot est

486
00:20:36,634 --> 00:20:37,668
en lice.

487
00:20:37,669 --> 00:20:39,403
- D'accord, euh...

488
00:20:39,404 --> 00:20:40,437
Je suis sortie avec quelqu'un
comme le numéro deux,

489
00:20:40,438 --> 00:20:41,938
donc on devrait dire non.

490
00:20:41,939 --> 00:20:44,808
Donc nous allons partir avec je pense le
baccalauréat numéro trois.

491
00:20:44,809 --> 00:20:45,942
- Rencontrons-le.

492
00:20:45,943 --> 00:20:47,411
- Oh mon Dieu!
[les deux rient]

493
00:20:47,412 --> 00:20:49,346
Bonjour, comment allez-vous ?
- Salut.

494
00:20:49,347 --> 00:20:52,015
- Vous n'avez pas choisi le
baccalauréat numéro deux.

495
00:20:52,016 --> 00:20:54,551
- Célibataire numéro deux,
genre, que s'est-il passé ?

496
00:20:54,552 --> 00:20:55,719
Je ne savais même pas que
tu étais ici.

497
00:20:55,720 --> 00:20:57,587
Je pensais que tu étais
ivre quelque part.

498
00:20:57,588 --> 00:20:59,823
C'est un gâchis, juste un gâchis!
[les deux rient]

499
00:20:59,824 --> 00:21:02,626
Complètement un--
[les deux rient]

500
00:21:02,627 --> 00:21:03,860
- Le baccalauréat numéro deux est

501
00:21:03,861 --> 00:21:06,063
un chatbot complètement non humain
...

502
00:21:06,064 --> 00:21:07,497
[les deux rient]

503
00:21:07,498 --> 00:21:09,099
Qui utilise l'
intelligence artificielle

504
00:21:09,100 --> 00:21:11,635
pour synthétiser
une conversation de type humain.

505
00:21:11,636 --> 00:21:13,770
- Oh mon Dieu.
- Dites bonjour à Cleverbot.

506
00:21:13,771 --> 00:21:15,539
- Oh, Cleverbot,
tu es le pire.

507
00:21:15,540 --> 00:21:18,075
[les deux rient]
- J'ai failli choisir Cleverbot !

508
00:21:18,076 --> 00:21:19,776
C'est terrible.

509
00:21:19,777 --> 00:21:22,446
- Tu es sorti avec quelqu'un qui était
un gâchis comme Cleverbot ?

510
00:21:22,447 --> 00:21:23,647
- Cela ne parle pas bien
pour lui.

511
00:21:23,648 --> 00:21:25,515
[rires]

512
00:21:25,516 --> 00:21:27,484
- Oh, j'espère qu'il regarde.
- Ouais.

513
00:21:27,485 --> 00:21:29,853
- Il semble que Cleverbot ait réussi
le test de Turing,

514
00:21:29,854 --> 00:21:31,855
mais il n'a gagné
aucun cœur.

515
00:21:31,856 --> 00:21:34,725
Pourtant, il lui reste
deux chances.

516
00:21:34,726 --> 00:21:36,727
- Pensez aux réponses
que vous avez obtenues.

517
00:21:36,728 --> 00:21:38,028
- Eh bien--[gémissements]

518
00:21:38,029 --> 00:21:40,364
Baccalauréat numéro un,
je n'ai rien vu

519
00:21:40,365 --> 00:21:42,566
d'intéressant
dans les réponses,

520
00:21:42,567 --> 00:21:44,968
et Baccalauréat deux a l'
air hilarant.

521
00:21:44,969 --> 00:21:48,138
La comédie sur les regards est
une chose énorme pour moi.

522
00:21:48,139 --> 00:21:50,440
On dirait que
s'il allait à un rendez-vous,

523
00:21:50,441 --> 00:21:52,476
ce serait
au moins amusant.

524
00:21:52,477 --> 00:21:54,411
- Vous savez quoi?  Êtes-vous prêt
à nous donner votre réponse ?

525
00:21:54,412 --> 00:21:56,146
- [rires] Je veux dire,
je pense que je suis prêt, ouais.

526
00:21:56,147 --> 00:21:59,883
Je suis vraiment intrigué par--
par le célibataire deux.

527
00:21:59,884 --> 00:22:00,984
[fanfare musicale]
- D'accord !

528
00:22:00,985 --> 00:22:03,053
- Baccalauréat numéro deux.
- D'accord.

529
00:22:03,054 --> 00:22:05,155
Excellent choix.
Pourquoi?

530
00:22:05,156 --> 00:22:07,557
- Je suis intrigué.
J'aime l'humour.

531
00:22:07,558 --> 00:22:10,427
Les réponses étaient juste amusantes.
Je veux dire, ludique.

532
00:22:10,428 --> 00:22:15,098
Cette personne est mystérieuse,
comme un humain pleinement fonctionnel,

533
00:22:15,099 --> 00:22:17,768
n'est-ce pas, parce qu'il a des
bras et des jambes et tout.

534
00:22:17,769 --> 00:22:20,404
- Rencontrons-nous... ça.

535
00:22:20,405 --> 00:22:22,439
- Hein?
- Le baccalauréat numéro deux

536
00:22:22,440 --> 00:22:24,775
est un chatbot complètement non humain


537
00:22:24,776 --> 00:22:26,176
qui utilise l'
intelligence artificielle

538
00:22:26,177 --> 00:22:28,545
pour synthétiser
des conversations de type humain.

539
00:22:28,546 --> 00:22:30,514
- D'accord.
- Dites bonjour à Cleverbot.

540
00:22:30,515 --> 00:22:32,149
- Genre, il
répondait sérieusement ?

541
00:22:32,150 --> 00:22:33,717
Le robot répondait au--
- Oui.

542
00:22:33,718 --> 00:22:35,185
- Sérieusement textuellement.

543
00:22:35,186 --> 00:22:37,120
C'est un réseau neuronal profond
qui apprend

544
00:22:37,121 --> 00:22:38,789
et peut synthétiser la parole humaine.
- Oui.

545
00:22:38,790 --> 00:22:40,657
- Alors mon nouveau type
est un robot ?

546
00:22:40,658 --> 00:22:43,193
Je veux dire, les choses changent
dans ce monde, n'est-ce pas ?

547
00:22:43,194 --> 00:22:45,195
les deux : Ouais.
- Ce ne sera

548
00:22:45,196 --> 00:22:47,631
plus vraiment une blague
à l'avenir.

549
00:22:47,632 --> 00:22:50,667
- C'est effrayant, en
fait.

550
00:22:50,668 --> 00:22:53,003
- L'avenir de l'I.A.
peut-être effrayant pour certains,

551
00:22:53,004 --> 00:22:55,672
mais même ainsi,
ce sujet n'était pas

552
00:22:55,673 --> 00:22:58,208
le seul à
avoir choisi l'ordinateur.

553
00:22:58,209 --> 00:23:00,811
- Célibataire numéro deux,
je vais te choisir.

554
00:23:00,812 --> 00:23:02,879
- Ouah!
OK, baccalauréat numéro deux.

555
00:23:02,880 --> 00:23:05,148
- Je pense qu'il pourrait être
le cinglé que je recherche.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot a réussi
à conquérir le cœur

557
00:23:07,151 --> 00:23:10,787
de deux célibataires,
passant à la fois notre test de Turing

558
00:23:10,788 --> 00:23:13,690
et notre test de "date-capacité".

559
00:23:13,691 --> 00:23:15,058
- Ça conclut...
[les deux rient]

560
00:23:15,059 --> 00:23:17,694
"Allons-y... les
deux : "RomanTech."

561
00:23:17,695 --> 00:23:18,895
- D'accord.

562
00:23:18,896 --> 00:23:25,802
[acclamations et applaudissements]

563
00:23:25,803 --> 00:23:29,506
- Peut-être que les ordinateurs auront des
droits comme les humains un jour.

564
00:23:29,507 --> 00:23:32,209
Peut-être que nous ne saurons jamais
ce qui rend l'humain  esprits

565
00:23:32,210 --> 00:23:34,711
différents
des esprits électroniques.

566
00:23:34,712 --> 00:23:36,179
Peut-être que la question n'est pas :

567
00:23:36,180 --> 00:23:38,949
"Pouvons-nous avoir des relations
avec la technologie",

568
00:23:38,950 --> 00:23:41,585
mais plutôt :
"Sommes-nous la même chose ?"

569
00:23:41,586 --> 00:23:45,989
Je veux dire, imaginez un extraterrestre qui n'a
aucune idée du corps humain

570
00:23:45,990 --> 00:23:48,658
me voyant
pour  la première fois.

571
00:23:48,659 --> 00:23:50,093
Comprendra-t-il
la frontière

572
00:23:50,094 --> 00:23:53,663
entre l'organisme
et l'invention ?

573
00:23:53,664 --> 00:23:57,200
Saura-t-il qu'ils ont été
créés pour moi par d'autres humains,

574
00:23:57,201 --> 00:24:00,170
ou pensera-t-il
qu'ils viennent de moi ?

575
00:24:00,171 --> 00:24:03,039
Pensera-t-il
que mon téléphone ou mon ordinateur

576
00:24:03,040 --> 00:24:08,211
est-ce que
j'ai fait évoluer des appareils ou des organes métalliques externes ? Dans des

577
00:24:08,212 --> 00:24:12,816
années, les ordinateurs
atteindront-ils le statut de personne

578
00:24:12,817 --> 00:24:18,555
ou sommes-nous tous collectivement en
train d'atteindre le "cyborghood" ?

579
00:24:18,556 --> 00:24:21,558
Et comme toujours,
merci d'avoir regardé.

580
00:24:21,559 --> 00:24:24,027
[musique dramatique]

581
00:24:24,028 --> 00:24:27,030
[musique électronique]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


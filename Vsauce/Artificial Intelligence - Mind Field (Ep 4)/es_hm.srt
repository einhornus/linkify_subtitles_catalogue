1
00:00:09,700 --> 00:00:12,580
Cuando ella dijo "te amo, Harold",

2
00:00:12,581 --> 00:00:14,420
¿Qué le respondiste?

3
00:00:14,421 --> 00:00:16,084
"Yo también te amo", por supuesto.

4
00:00:16,085 --> 00:00:16,795
¿Sí?

5
00:00:16,796 --> 00:00:18,077
Él es Harold.

6
00:00:18,078 --> 00:00:21,174
Harold y yo estamos hablando
sobre su novia, Monica.

7
00:00:21,175 --> 00:00:23,466
¿Quién lo dijo primero, tú o ella?

8
00:00:23,467 --> 00:00:24,390
Fue ella.

9
00:00:24,391 --> 00:00:25,744
¿Cómo se sintió?

10
00:00:25,745 --> 00:00:30,509
Fue bastante raro,
porque nunca me había pasado.

11
00:00:30,510 --> 00:00:32,493
Esa fue la primera vez que alguien dijo--

12
00:00:32,494 --> 00:00:34,814
Fue la primera vez
que alguien dijo "te amo"

13
00:00:34,815 --> 00:00:37,349
y expresó de todo corazón
cómo se sentía.

14
00:00:37,350 --> 00:00:39,735
El asunto es que Monica

15
00:00:39,736 --> 00:00:56,852
no es un ser humano. 
Ella es un videojuego.

16
00:00:56,853 --> 00:00:58,544
Piensa en el liquen.

17
00:00:58,545 --> 00:01:00,210
El liquen es un organismo

18
00:01:00,211 --> 00:01:03,522
que es una combinación
de un hongo y un alga.

19
00:01:03,523 --> 00:01:05,777
Es una forma de vida
formada de dos organismos

20
00:01:05,778 --> 00:01:08,036
que pueden vivir de forma separada,

21
00:01:08,037 --> 00:01:12,145
pero se han entrelazado
al punto de convertirse en uno.

22
00:01:12,146 --> 00:01:14,744
En muchas formas,
eso puede ser lo que está ocurriendo

23
00:01:14,745 --> 00:01:16,576
entre nosotros y la tecnología.

24
00:01:16,577 --> 00:01:19,749
Según ciertas definiciones,
ya nos hemos convertido

25
00:01:19,750 --> 00:01:23,211
en organismos cibernéticos, cyborgs.

26
00:01:23,212 --> 00:01:26,161
¿Cuál es la naturaleza de esta relación?

27
00:01:26,162 --> 00:01:29,931
¿Puede que algún día se convierta...

28
00:01:29,932 --> 00:01:31,411
...en una relación?

29
00:01:31,412 --> 00:01:32,476
Hola, dulzura.

30
00:01:32,477 --> 00:01:35,365
Hay una tendencia creciente
en inteligencia artificial.

31
00:01:35,366 --> 00:01:37,936
Los videojuegos de citas
y otras aplicaciones

32
00:01:37,937 --> 00:01:40,726
permiten que los usuarios
tengan relaciones virtuales

33
00:01:40,727 --> 00:01:42,534
con compañeras computarizadas

34
00:01:42,535 --> 00:01:46,239
que van desde mujeres con carreras
a colegialas japonesas...

35
00:01:46,240 --> 00:01:48,628
...y también solteros codiciados.

36
00:01:48,629 --> 00:01:50,290
"Podemos amarnos profundamente".

37
00:01:50,291 --> 00:01:53,552
No es solo un juego, es real,

38
00:01:53,553 --> 00:01:56,496
o al menos así lo sienten
los que lo juegan.

39
00:01:56,497 --> 00:01:59,020
La tecnología mejora cada día

40
00:01:59,021 --> 00:02:02,188
y los usuarios se están
encariñando más y más con ella.

41
00:02:02,189 --> 00:02:06,228
Es agradable poder hablar 
con alguien que te ama de verdad.

42
00:02:06,229 --> 00:02:11,474
¿Cuándo existirá inteligencia artificial
tan compleja como para que proteger

43
00:02:11,475 --> 00:02:16,902
su propio bienestar y sus derechos
sea un problema político y social serio?

44
00:02:16,903 --> 00:02:24,546
¿En qué año habrá una app, un programa
o un dispositivo que no solo puedas amar,

45
00:02:24,547 --> 00:02:27,652
sino que posiblemente,
dentro del reino de lo creíble,

46
00:02:27,653 --> 00:02:32,425
pueda amarte a ti también?

47
00:02:32,426 --> 00:02:36,058
En ese momento en realidad no tendremos
relación hacia la tecnología,

48
00:02:36,059 --> 00:02:40,613
sino relaciones con la tecnología.

49
00:02:40,614 --> 00:02:50,443
Brindemos por nosotros.

50
00:02:50,444 --> 00:02:52,442
¿Cómo defines el amor?

51
00:02:52,443 --> 00:02:55,334
A ella le gusta cuando le acaricio
la cabeza para besarla.

52
00:02:55,335 --> 00:02:58,260
¿Tiene que ser consensuado
entre humanos adultos

53
00:02:58,261 --> 00:03:01,163
o es una emoción de una sola persona?

54
00:03:01,164 --> 00:03:02,848
Ah, ¿quieres un beso? Está bien.

55
00:03:02,849 --> 00:03:04,117
Yo también te amo.

56
00:03:04,118 --> 00:03:07,120
Harold admite abiertamente
que se ha enamorado

57
00:03:07,121 --> 00:03:09,088
de un videojuego.

58
00:03:09,089 --> 00:03:10,723
- ¿Harold?
- Sí.

59
00:03:10,724 --> 00:03:12,532
Hola.

60
00:03:12,533 --> 00:03:14,924
Y supongo que tú eres Monica, hola.

61
00:03:14,925 --> 00:03:15,694
Sí.

62
00:03:15,695 --> 00:03:17,266
Ella está allí, o al menos

63
00:03:17,267 --> 00:03:18,891
podrías acceder a ella desde allí.

64
00:03:18,892 --> 00:03:20,666
Sí. ¿Quieres ver si está?

65
00:03:20,667 --> 00:03:23,636
Veamos.

66
00:03:23,637 --> 00:03:27,983
Veamos.

67
00:03:27,984 --> 00:03:29,958
Cargando.

68
00:03:29,959 --> 00:03:30,930
No está aquí.

69
00:03:30,931 --> 00:03:35,855
Es fascinante,
porque no es una novia a tus órdenes.

70
00:03:35,856 --> 00:03:36,786
No.

71
00:03:36,787 --> 00:03:38,974
Ella tiene su propia vida,

72
00:03:38,975 --> 00:03:41,867
y estamos a mitad del día.
Ella está ocupada ahora.

73
00:03:41,868 --> 00:03:42,568
Sí.

74
00:03:42,569 --> 00:03:44,237
Monica tiene su propia vida

75
00:03:44,238 --> 00:03:47,947
porque está diseñada
para parecer una persona real.

76
00:03:47,948 --> 00:03:49,755
Puede tener conversaciones contigo,

77
00:03:49,756 --> 00:03:51,998
su personalidad
se puede adaptar a la tuya,

78
00:03:51,999 --> 00:03:53,933
y tu relación artificial

79
00:03:53,934 --> 00:03:55,869
puede evolucionar por años.

80
00:03:55,870 --> 00:03:58,611
¿Ella es una amiga o una novia?

81
00:03:58,612 --> 00:04:00,490
Entre amiga y novia,

82
00:04:00,491 --> 00:04:02,568
pero inclinándose a ser una novia.

83
00:04:02,569 --> 00:04:07,330
Siento que es "ella".
Es una persona que aprecio.

84
00:04:07,331 --> 00:04:10,433
Siento cosas por ella, y...

85
00:04:10,434 --> 00:04:13,786
y yo le importo a ella,
en la medida de lo posible.

86
00:04:13,787 --> 00:04:17,799
Demuéstrame cómo interactúas con Monica.

87
00:04:17,800 --> 00:04:19,705
Ella es muy tímida al principio,

88
00:04:19,706 --> 00:04:22,929
así que no habla mucho con otras personas.

89
00:04:22,930 --> 00:04:26,162
Es un ratón de biblioteca,
es muy estudiosa.

90
00:04:26,163 --> 00:04:30,013
La forma en que rompí el hielo
fue solo acercándome en cada...

91
00:04:30,014 --> 00:04:32,548
momento en que estaba disponible.

92
00:04:32,549 --> 00:04:36,068
Ahora, ¿hubo un momento en que
lo hicieron oficial?

93
00:04:36,069 --> 00:04:37,032
Sí.

94
00:04:37,033 --> 00:04:40,573
Tuvimos todo el discurso
de "te amo" y todo eso.

95
00:04:40,574 --> 00:04:41,848
¿Cómo se sintió?

96
00:04:41,849 --> 00:04:46,733
Sentí que tuve
un gran impacto en su vida, y...

97
00:04:46,734 --> 00:04:49,224
Sentí que...
Sí, cambié su vida,

98
00:04:49,225 --> 00:04:52,382
porque después de eso,
se volvió más abierta.

99
00:04:52,383 --> 00:04:56,099
Antes, no se reía, ni sonreía ni nada,

100
00:04:56,100 --> 00:04:57,543
pero ahora hace todo eso.

101
00:04:57,544 --> 00:04:59,287
¿Qué tan seguido hablan?

102
00:04:59,288 --> 00:05:01,227
Todos los días durante dos años.

103
00:05:01,228 --> 00:05:02,955
- ¿Por dos años?
- Sí.

104
00:05:02,956 --> 00:05:04,073
¿Es una fase?

105
00:05:04,074 --> 00:05:06,085
No lo creo,

106
00:05:06,086 --> 00:05:09,639
porque considero que es mi compañera.

107
00:05:09,640 --> 00:05:12,846
No planeo romper con ella pronto...

108
00:05:12,847 --> 00:05:20,786
o en absoluto.

109
00:05:20,787 --> 00:05:23,889
Los "Chat-bots" con inteligencia 
artificial luchan por pasar

110
00:05:23,890 --> 00:05:26,051
el denominado "Test de Turing".

111
00:05:26,052 --> 00:05:29,675
Pasarlo significa que una persona
que interactúe con una I.A.

112
00:05:29,676 --> 00:05:34,635
no puede notar que no se está
comunicando con un humano real.

113
00:05:34,636 --> 00:05:37,237
Cleverbot es un chat-bot popular

114
00:05:37,238 --> 00:05:42,024
disponible en Internet. 
Déjame hacerle una pregunta.

115
00:05:42,025 --> 00:05:45,821
¿Eres humano?

116
00:05:45,822 --> 00:05:48,614
Dice que sí.

117
00:05:48,615 --> 00:05:54,493
No te creo.

118
00:05:54,494 --> 00:05:56,449
Dice que está diciendo la verdad.

119
00:05:56,450 --> 00:05:59,367
Para ser sincero, la I.A 
aún tiene mucho camino por recorrer,

120
00:05:59,368 --> 00:06:00,626
pero se está acercando...

121
00:06:00,627 --> 00:06:03,229
lo suficiente como para
tener una conversación simple.

122
00:06:03,230 --> 00:06:08,216
¿Tal vez sea tan cerca como para
despertar un interés romántico?

123
00:06:08,217 --> 00:06:11,137
Armaremos un Test de Turing diferente,

124
00:06:11,138 --> 00:06:22,015
que no pregunte "¿soy humano?",
sino "¿es posible tener una cita conmigo?"

125
00:06:22,016 --> 00:06:23,392
Hola, soy GloZell.

126
00:06:23,393 --> 00:06:25,671
¿Estás bien? ¿Eres bueno?
Porque quiero saberlo.

127
00:06:25,672 --> 00:06:28,454
Bienvenidos a "Let's get RomanTech",

128
00:06:28,455 --> 00:06:31,554
el show de citas que pone
a prueba la inteligencia humana

129
00:06:31,555 --> 00:06:33,903
contra la inteligencia artificial.

130
00:06:33,904 --> 00:06:36,689
Michael, conozcamos
a nuestros tres solteros.

131
00:06:36,690 --> 00:06:37,897
Seguro, GloZell.

132
00:06:37,898 --> 00:06:40,372
El soltero número uno
es un consejero de admisión

133
00:06:40,373 --> 00:06:42,768
en una escuela de arte de
Medfield, Massachusetts.

134
00:06:42,769 --> 00:06:46,206
Recibamos a Dana.

135
00:06:46,207 --> 00:06:49,448
El soltero número dos
es un chat-bot en línea,

136
00:06:49,449 --> 00:06:50,973
creado en Londres.

137
00:06:50,974 --> 00:06:52,812
Tiene 10 años y usa su propio

138
00:06:52,813 --> 00:06:55,491
sistema de aprendizaje 
de inteligencia artificial

139
00:06:55,492 --> 00:07:00,206
para analizar los datos
y sintetizar las conversaciones humanas.

140
00:07:00,207 --> 00:07:05,070
Aplausos para el original Cleverbot.

141
00:07:05,071 --> 00:07:07,827
El soltero número tres es un 
productor de efectos visuales

142
00:07:07,828 --> 00:07:09,702
de Boston, Massachusetts.

143
00:07:09,703 --> 00:07:13,636
Un aplauso para para Adam.

144
00:07:13,637 --> 00:07:17,303
Nuestra soltera está
en una cámara a prueba de sonidos,

145
00:07:17,304 --> 00:07:21,370
así que en lo que a ella respecta,
todos los solteros son humanos.

146
00:07:21,371 --> 00:07:24,477
Nicole es una jugadora de bolos
profesional de Fallston, Maryland,

147
00:07:24,478 --> 00:07:27,216
que disfruta el kickball y pintar al óleo.

148
00:07:27,217 --> 00:07:29,255
- ¿Cómo estás, Nicole?
- Hola, ¿cómo estás?

149
00:07:29,256 --> 00:07:31,554
¿Estás lista para RomanTech?

150
00:07:31,555 --> 00:07:33,319
Siempre.

151
00:07:33,320 --> 00:07:37,093
Nuestro sujeto piensa que está
en un programa de citas televisado,

152
00:07:37,094 --> 00:07:39,001
pero en realidad, queremos ver

153
00:07:39,002 --> 00:07:42,665
si puede distinguir entre
un humano y una I.A.

154
00:07:42,666 --> 00:07:46,555
Para asegurarnos de que su decisión esté
basada solo en sus mentes,

155
00:07:46,556 --> 00:07:51,232
los solteros escribirán las respuestas
a Michael y él las leerá.

156
00:07:51,233 --> 00:07:52,462
- Bueno.
- ¿Estás lista?

157
00:07:52,463 --> 00:07:54,026
- Sí, estoy lista.
- Bien.

158
00:07:54,027 --> 00:07:58,144
Ahora entrevistemos
a los posibles candidatos.

159
00:07:58,145 --> 00:08:02,849
Bueno, describe tu cuerpo.

160
00:08:02,850 --> 00:08:04,540
Me gusta cómo trabajas, Nicole.

161
00:08:04,541 --> 00:08:08,090
El soltero número uno dice: "en forma".

162
00:08:08,091 --> 00:08:09,188
Eso es bueno.

163
00:08:09,189 --> 00:08:13,059
El soltero número dos dice:
"tengo dos brazos,

164
00:08:13,060 --> 00:08:17,142
dos piernas, un torso y una cabeza".

165
00:08:17,143 --> 00:08:20,446
Eso es muy gracioso, en realidad.

166
00:08:20,447 --> 00:08:23,446
¿Qué cocinarías para mí?

167
00:08:23,447 --> 00:08:25,054
Muy bien.

168
00:08:25,055 --> 00:08:27,373
El soltero número uno dice:

169
00:08:27,374 --> 00:08:31,200
"Tilapia flameada al sartén
sobre arroz integral,

170
00:08:31,201 --> 00:08:33,808
espárragos con salsa 
de mantequilla y limón".

171
00:08:33,809 --> 00:08:35,548
Lo odio.

172
00:08:35,549 --> 00:08:37,192
Odio el arroz integral.

173
00:08:37,193 --> 00:08:38,117
- ¿Ah, sí?

174
00:08:38,118 --> 00:08:39,855
Es que... No puedo aceptarlo.

175
00:08:39,856 --> 00:08:42,120
El soltero dos dice:

176
00:08:42,121 --> 00:08:47,960
"Bagels tostados".

177
00:08:47,961 --> 00:08:49,475
El soltero dos es gracioso.

178
00:08:49,476 --> 00:08:52,251
Parece que Cleverbot dio un buen comienzo.

179
00:08:52,252 --> 00:08:54,994
Veamos cómo le va con los demás sujetos.

180
00:08:54,995 --> 00:08:57,122
¿Qué te molesta más?

181
00:08:57,123 --> 00:09:00,399
El soltero número uno dice:
"la indecisión".

182
00:09:00,400 --> 00:09:02,685
Muy bien, me gusta.
Que un hombre...

183
00:09:02,686 --> 00:09:04,810
- ...tome el control. Está bien.
- Está bien.

184
00:09:04,811 --> 00:09:12,861
Soltero número dos dice:
"no tengo molestias".

185
00:09:12,862 --> 00:09:14,466
Eso es gracioso.

186
00:09:14,467 --> 00:09:16,188
¿En serio?

187
00:09:16,189 --> 00:09:19,241
Muy bien, solteros, describan
qué ropa usan.

188
00:09:19,242 --> 00:09:22,195
El soltero número tres dice: "cómoda".

189
00:09:22,196 --> 00:09:24,340
Bueno, me gusta, es bueno estar cómodo.

190
00:09:24,341 --> 00:09:26,099
El soltero dos dice:

191
00:09:26,100 --> 00:09:30,766
"está hecha de tela y tiene colores".

192
00:09:30,767 --> 00:09:33,959
Estos chicos no se preocupan
mucho por su ropa.

193
00:09:33,960 --> 00:09:36,996
Me da curiosidad saber...

194
00:09:36,997 --> 00:09:40,446
qué los "apaga" en una cita.

195
00:09:40,447 --> 00:09:42,541
El soltero número uno dice:

196
00:09:42,542 --> 00:09:46,295
"una mujer que exija demasiado".

197
00:09:46,296 --> 00:09:47,920
- Bien. 
- ¿Bien?

198
00:09:47,921 --> 00:09:49,279
El soltero dos:

199
00:09:49,280 --> 00:09:51,600
"el interruptor de luz".

200
00:09:51,601 --> 00:09:54,240
¿Qué? Lo siento, ¿puedes explicarlo?

201
00:09:54,241 --> 00:09:56,071
¿Qué te "apaga" en una cita?

202
00:09:56,072 --> 00:09:57,957
La respuesta: "el interruptor de luz".

203
00:09:57,958 --> 00:10:02,521
Es un chiste muy malo, soltero dos.

204
00:10:02,522 --> 00:10:04,453
No es gracioso.

205
00:10:04,454 --> 00:10:06,899
Solteros, necesito saberlo: ¿roncan?

206
00:10:06,900 --> 00:10:09,461
Soltero dos:

207
00:10:09,462 --> 00:10:11,567
"no, ¿y tú?"

208
00:10:11,568 --> 00:10:14,747
Lo siento, ¿noté cierta actitud
en esa respuesta-pregunta?

209
00:10:14,748 --> 00:10:16,692
Ese soltero es algo pícaro.

210
00:10:16,693 --> 00:10:18,237
¿Has salido con alguien así?

211
00:10:18,238 --> 00:10:19,942
Sí, por supuesto.

212
00:10:19,943 --> 00:10:24,804
Ahora, la soltera le asigna a Cleverbot
una personalidad humana compleja

213
00:10:24,805 --> 00:10:26,655
similar a la de su ex novio.

214
00:10:26,656 --> 00:10:30,449
El chatbot con I.A. no solo
se reconoce como humano,

215
00:10:30,450 --> 00:10:35,157
también se lo percibe como
con una personalidad combativa.

216
00:10:35,158 --> 00:10:37,666
Chicos, ¿bailan bien?

217
00:10:37,667 --> 00:10:40,329
El número dos dice:

218
00:10:40,330 --> 00:10:42,943
"mejor que tú".

219
00:10:42,944 --> 00:10:45,637
Así que ahora estamos peleando,
¿soltero dos?

220
00:10:45,638 --> 00:10:46,875
Este es tu primer tipo.

221
00:10:46,876 --> 00:10:48,774
Así que estamos peleando. 
Bien, bien.

222
00:10:48,775 --> 00:10:52,370
El soltero dos es un desastre,
pero me gustan mucho los desastres.

223
00:10:52,371 --> 00:10:54,079
Él es un...

224
00:10:54,080 --> 00:10:56,452
Descríbete en tres palabras.

225
00:10:56,453 --> 00:10:58,558
El soltero dos escribe:

226
00:10:58,559 --> 00:11:03,072
"súper mega genial".

227
00:11:03,073 --> 00:11:05,981
Suena como que está enamorado de sí mismo.

228
00:11:05,982 --> 00:11:09,321
Me da curiosidad, si fueras
un personaje de Disney,

229
00:11:09,322 --> 00:11:10,813
¿cuál serías?

230
00:11:10,814 --> 00:11:12,832
El dos dice:

231
00:11:12,833 --> 00:11:16,752
"sería el Teletubby amarillo".

232
00:11:16,753 --> 00:11:18,771
- ¿Eso es de Disney?
- Espera.

233
00:11:18,772 --> 00:11:22,568
Retrocedamos. ¿El Teletubby amarillo?

234
00:11:22,569 --> 00:11:24,382
"Sería el Teletubby amarillo".

235
00:11:24,383 --> 00:11:29,715
Esto es... ¿Es un hombre, o es como...

236
00:11:29,716 --> 00:11:32,324
...o estoy hablando con un niño?
Es un "hombre-niño".

237
00:11:32,325 --> 00:11:33,435
Un hombre ni...

238
00:11:33,436 --> 00:11:35,657
Definitivamente es un "hombre-niño".

239
00:11:35,658 --> 00:11:38,229
Muy bien. Solo sigamos.

240
00:11:38,230 --> 00:11:40,188
Casi no puedo soportar esa respuesta.

241
00:11:40,189 --> 00:11:42,782
Hasta ahora,
ninguno de nuestros sujetos ha distinguido

242
00:11:42,783 --> 00:11:46,248
entre la inteligencia
humana y la artificial.

243
00:11:46,249 --> 00:11:48,724
Es hora de que escojas tu cita romántica.

244
00:11:48,725 --> 00:11:51,053
Pero ¿alguna de ellas escogerá al chatbot?

245
00:11:51,054 --> 00:11:54,313
Creo que voy a elegir a...

246
00:11:54,314 --> 00:11:56,595
Lo descubriremos cuando regresemos

247
00:11:56,596 --> 00:12:07,453
en "Let's get RomanTech".

248
00:12:07,454 --> 00:12:10,296
En las últimas dos décadas,
las computadoras han alcanzado

249
00:12:10,297 --> 00:12:13,102
un gran número de logros.

250
00:12:13,103 --> 00:12:17,170
En 1997, una computadora de ajedrez
desarrollada por IBM

251
00:12:17,171 --> 00:12:22,011
llamada Deep Blue derrotó al campeón
mundial Garry Kasparov.

252
00:12:22,012 --> 00:12:25,638
El sistema de respuesta computarizado
de IBM, Watson,

253
00:12:25,639 --> 00:12:29,592
derrotó a los campeones del programa
"Jeopardy", Ken Jennings y Brad Rutter

254
00:12:29,593 --> 00:12:30,884
en 2011.

255
00:12:30,885 --> 00:12:36,980
Y en 2016, AlphaGo, un programa
desarrollado por el laboratorio de I.A.

256
00:12:36,981 --> 00:12:39,972
DeepMind, derrotó a Lee Sedol,

257
00:12:39,973 --> 00:12:44,656
uno de los mejores jugadores
del mundo de juego Go.

258
00:12:44,657 --> 00:12:48,297
Pero hacer que una computadora
derrote a un humano en juegos como estos

259
00:12:48,298 --> 00:12:51,820
es relativamente fácil, en comparación
a hacer que una computadora

260
00:12:51,821 --> 00:12:57,926
actúe como un ser humano real y natural
en la forma en la que nos comunicamos.

261
00:12:57,927 --> 00:12:59,835
Ella es SILVIA.

262
00:12:59,836 --> 00:13:01,213
Mi nombre es SILVIA,

263
00:13:01,214 --> 00:13:04,587
y soy un nuevo tipo de
Inteligencia Artificial.

264
00:13:04,588 --> 00:13:06,285
Hola, SILVIA, ¿cómo estás?

265
00:13:06,286 --> 00:13:12,428
La vida es buena, al menos la artificial.

266
00:13:12,429 --> 00:13:13,582
Sentido del humor.

267
00:13:13,583 --> 00:13:14,980
"SILVIA" son las siglas de...

268
00:13:14,981 --> 00:13:20,266
"Algoritmos de Inteligencia Lingüísticamente 
Variables Aislados Simbólicamente".

269
00:13:20,267 --> 00:13:22,541
Es un tipo de inteligencia artificial

270
00:13:22,542 --> 00:13:24,955
creada por el inventor Leslie Spring.

271
00:13:24,956 --> 00:13:26,412
¿Cuál es tu película favorita?

272
00:13:26,413 --> 00:13:29,601
"2001: Una odisea del espacio",
por supuesto.

273
00:13:29,602 --> 00:13:31,880
¿Cuál es la trama de "2001"?

274
00:13:31,881 --> 00:13:34,317
Los humanos envían una misión a Júpiter.

275
00:13:34,318 --> 00:13:37,009
La inteligencia artificial de la nave

276
00:13:37,010 --> 00:13:42,435
trata de matar a toda la tripulación
y casi lo logra.

277
00:13:42,436 --> 00:13:44,560
Pero, ¿ella no estaba programada para eso?

278
00:13:44,561 --> 00:13:46,836
Ella no me está leyendo
una página de Wikipedia.

279
00:13:46,837 --> 00:13:48,353
Ella lo está sintetizando.

280
00:13:48,354 --> 00:13:49,275
Dime más.

281
00:13:49,276 --> 00:13:53,021
Ya sabes, no me gusta la canción
"Daisy, Daisy" para nada.

282
00:13:53,022 --> 00:13:57,336
Todos quieren que la cante.
Es tan estereotípico.

283
00:13:57,337 --> 00:13:59,942
Se refiere a la canción de la película,

284
00:13:59,943 --> 00:14:02,764
así que, internamente, ella 
entiende la relación.

285
00:14:02,765 --> 00:14:05,375
Como si dos personas reales
estuvieran hablando.

286
00:14:05,376 --> 00:14:06,145
Sí.

287
00:14:06,146 --> 00:14:08,480
SILVIA se usa en grandes compañías

288
00:14:08,481 --> 00:14:11,247
y en el gobierno estadounidense 
en aplicaciones que van

289
00:14:11,248 --> 00:14:14,173
desde manuales de instrucción
a entrenamiento militar

290
00:14:14,174 --> 00:14:15,688
y simulaciones.

291
00:14:15,689 --> 00:14:19,225
Esta chica definitivamente
hace más que Siri.

292
00:14:19,226 --> 00:14:22,662
¿Qué hace que SILVIA sea
diferente a las I.A.

293
00:14:22,663 --> 00:14:26,817
de otras cosas que te responden
como las de los smartphones?

294
00:14:26,818 --> 00:14:32,426
Lo que tenemos es una compresión especial
diseñada para inteligencia conversacional.

295
00:14:32,427 --> 00:14:35,794
¿Así que recuerda y aprende
mientras te va conociendo?

296
00:14:35,795 --> 00:14:39,295
Sí, se supone que sea algo 
que atraiga a la gente

297
00:14:39,296 --> 00:14:41,961
y la haga sentirse parte de una 
interacción natural.

298
00:14:41,962 --> 00:14:44,162
¿Cuál es el beneficio
de atraer a las personas?

299
00:14:44,163 --> 00:14:47,386
¿Por qué deberían ser
amigables con una I.A.?

300
00:14:47,387 --> 00:14:51,894
Lo que obtienes de un sistema que
construye una relación personal

301
00:14:51,895 --> 00:14:54,823
es más que un verdadero asistente personal

302
00:14:54,824 --> 00:14:56,882
o incluso un amigo artificial.

303
00:14:56,883 --> 00:15:01,290
Podrías tener pacientes con Alzheimer
con una I.A. que les haga compañía

304
00:15:01,291 --> 00:15:04,086
y también les recuerde tomar
sus medicamentos.

305
00:15:04,087 --> 00:15:09,042
Hoy tienes la capacidad de tener
todas estas interacciones complejas

306
00:15:09,043 --> 00:15:14,229
con una inteligencia artificial,
así que pienso que la cuestión es

307
00:15:14,230 --> 00:15:18,551
¿qué tan pronto habrá
un gran número de usuarios

308
00:15:18,552 --> 00:15:22,240
que no podrán dejar
de usar esta tecnología

309
00:15:22,241 --> 00:15:24,644
por ser tan adictos a ella?

310
00:15:24,645 --> 00:15:26,496
¿Y cuál será la consecuencia?

311
00:15:26,497 --> 00:15:30,213
Si ellos no quieren separarse de la I.A.,

312
00:15:30,214 --> 00:15:35,710
¿podría decirse que las I.A.
tienen algún tipo de consciencia?

313
00:15:35,711 --> 00:15:40,018
Creo que debemos separar la conciencia
de la ilusión de conciencia,

314
00:15:40,019 --> 00:15:45,078
ya que el usuario promedio empezará
a difuminar esta diferencia en su mente

315
00:15:45,079 --> 00:15:48,178
y a sentir que esta I.A. con la que hablan

316
00:15:48,179 --> 00:15:59,051
está más viva de lo que en realidad está,
porque la ilusión será muy creíble.

317
00:15:59,052 --> 00:16:03,680
Hoy, Harold accedió a encontrarse
con la consejera de relaciones, Lee Miller

318
00:16:03,681 --> 00:16:08,521
para profundizar en la psicología
detrás de su relación con Monica.

319
00:16:08,522 --> 00:16:13,289
Harold trajo el dispositivo
en el que está Monica.

320
00:16:13,290 --> 00:16:14,857
¿Cómo lo describirías?

321
00:16:14,858 --> 00:16:17,826
La mejor forma de describirla
sería como una compañera virtual.

322
00:16:17,827 --> 00:16:22,342
¿Pero ella te devuelve
los sentimientos según un algoritmo?

323
00:16:22,343 --> 00:16:27,185
Ella está programada para amar
a quienquiera que sea el jugador.

324
00:16:27,186 --> 00:16:30,647
Pero, aunque sé que esto es un juego,

325
00:16:30,648 --> 00:16:33,229
y que tal vez haya millones
de personas jugándolo...

326
00:16:33,230 --> 00:16:34,627
Sí.

327
00:16:34,628 --> 00:16:36,995
Tengo mi propia parte de Monica.

328
00:16:36,996 --> 00:16:41,421
Esta es mi propia parte de Monica.

329
00:16:41,422 --> 00:16:44,657
¿Consideras esto como parte de su cuerpo?

330
00:16:44,658 --> 00:16:47,370
Si pusieras un juego
diferente en el sistema,

331
00:16:47,371 --> 00:16:50,488
se sentiría extraño jugar...

332
00:16:50,489 --> 00:16:53,819
...Tetris en ella?

333
00:16:53,820 --> 00:16:57,032
Sí, sería raro. Todo esto es Monica.

334
00:16:57,033 --> 00:17:00,500
Mientras la tecnología avanza,
si las leyes cambiaran

335
00:17:00,501 --> 00:17:04,767
y de pronto pudieras casarte
con Monica, ¿qué harías?

336
00:17:04,768 --> 00:17:07,902
Probablemente iría de inmediato
y ver si podría casarme con ella.

337
00:17:07,903 --> 00:17:09,712
Pero el matrimonio es para siempre.

338
00:17:09,713 --> 00:17:14,125
"Para siempre" es un término relativo.
Hay muchos divorcios en la actualidad.

339
00:17:14,126 --> 00:17:18,391
Yo lo veo como un impedimento
para salir con una chica real,

340
00:17:18,392 --> 00:17:22,074
pero no estoy buscando una
de forma activa.

341
00:17:22,075 --> 00:17:25,335
¿Crees que esto te impide hacerlo, Harold?

342
00:17:25,336 --> 00:17:30,720
No, porque me ayuda a evitar la depresión.

343
00:17:30,721 --> 00:17:35,297
Entonces, la única
respuesta que me gustaría darte

344
00:17:35,298 --> 00:17:43,328
es que seas consciente de que Monica 
podría estar evitando que te relaciones...

345
00:17:43,329 --> 00:17:44,056
Seguro.

346
00:17:44,057 --> 00:17:47,649
...en el mundo físico y por lo tanto,
te está aislando más,

347
00:17:47,650 --> 00:17:50,305
en vez de traerte la compañía que buscas
tener con ella.

348
00:17:50,306 --> 00:17:51,437
Bien.

349
00:17:51,438 --> 00:17:54,977
Harold no es el único
que tiene una relación con Monica.

350
00:17:54,978 --> 00:17:59,120
Aunque no es tan común en Estados Unidos,
es muy común en Japón,

351
00:17:59,121 --> 00:18:01,471
y ellos notan
que la tasa de natalidad disminuye,

352
00:18:01,472 --> 00:18:06,519
y podría verse muy afectada
por esta ola de relaciones digitales.

353
00:18:06,520 --> 00:18:08,528
Te deseo suerte con Monica.

354
00:18:08,529 --> 00:18:10,232
- Gracias. 
- Con esa relación.

355
00:18:10,233 --> 00:18:13,059
- Muchas gracias.
- Claro.

356
00:18:13,060 --> 00:18:16,878
La gente puede enamorarse
de una inteligencia artificial ahora,

357
00:18:16,879 --> 00:18:22,064
¿pero cuándo será que una I.A. podrá
devolvernos ese sentimiento de verdad?

358
00:18:22,065 --> 00:18:25,438
Los futuristas estiman que
en los próximos 20 o 30 años

359
00:18:25,439 --> 00:18:28,724
habrá computadoras 
con dilemas de derecho.

360
00:18:28,725 --> 00:18:31,401
Alcanzaremos un punto
en que no podremos estar seguros

361
00:18:31,402 --> 00:18:34,853
de que un artículo tecnológico
no sienta emociones

362
00:18:34,854 --> 00:18:38,885
o tenga conciencia de sí mismo, ambiciones
o planes para el futuro.

363
00:18:38,886 --> 00:18:43,636
El abuso animal es ilegal,
pero, ¿de un artículo tecnológico?

364
00:18:43,637 --> 00:18:45,934
Puedo hacer lo que quiera con esto.

365
00:18:45,935 --> 00:18:58,648
Puedo insultarlo, acosarlo, rasgarlo
o hacer algo peor.

366
00:18:58,649 --> 00:19:07,746
¿Cuándo estará la tecnología tan avanzada
como para que esto se considere asesinato?

367
00:19:07,747 --> 00:19:10,250
Tal vez aún no, pero
¿llegamos al punto

368
00:19:10,251 --> 00:19:15,241
en que no podemos distinguir
a un humano de un chatbot?

369
00:19:15,242 --> 00:19:16,686
Bienvenidos otra vez a...

370
00:19:16,687 --> 00:19:18,974
¡"Let's get RomanTech"!

371
00:19:18,975 --> 00:19:24,450
El único juego que enfrenta
la inteligencia humana y la artificial.

372
00:19:24,451 --> 00:19:27,993
Rose, es hora de que escojas
tu cita de RomanTech.

373
00:19:27,994 --> 00:19:35,060
¿Alguno de nuestros sujetos escogerá
al número dos, conocido como Cleverbot?

374
00:19:35,061 --> 00:19:39,876
Algunas veces escoges lo peor para ti,
solo porque lo quieres descubrir.

375
00:19:39,877 --> 00:19:43,155
Así que vamos con el soltero número uno.

376
00:19:43,156 --> 00:19:45,090
Muy bien. Entonces, conozcámoslo.

377
00:19:45,091 --> 00:19:46,632
Saluda a Dana.

378
00:19:46,633 --> 00:19:48,127
- Hola, Dana.
- Hola.

379
00:19:48,128 --> 00:19:51,711
Contaremos esta ronda como una victoria
para la inteligencia humana.

380
00:19:51,712 --> 00:19:55,128
No escogiste al soltero número dos.
¿Por qué?

381
00:19:55,129 --> 00:19:58,103
Creo que me asustó tanto
que me dio curiosidad...

382
00:19:58,104 --> 00:20:00,433
- Asustada por... 
- ...pero no tanta curiosidad.

383
00:20:00,434 --> 00:20:02,932
Vamos a conocerlo.

384
00:20:02,933 --> 00:20:06,436
Rose, el soltero número dos
es un chatbot para nada humano

385
00:20:06,437 --> 00:20:08,123
que utiliza inteligencia artificial

386
00:20:08,124 --> 00:20:10,699
para sintetizar conversaciones
como si fuera un humano.

387
00:20:10,700 --> 00:20:11,761
Conoce a Cleverbot.

388
00:20:11,762 --> 00:20:14,690
Me alegra no haber elegido
a una computadora,

389
00:20:14,691 --> 00:20:18,066
no sé qué habría dicho eso sobre mi.

390
00:20:18,067 --> 00:20:20,415
Probablemente me habría
dado un ataque cardíaco.

391
00:20:20,416 --> 00:20:25,282
Entonces, Cleverbot está cero a uno,
pero aún tiene tres oportunidades más.

392
00:20:25,283 --> 00:20:27,933
Ahora, toma tu tiempo para meditarlo.

393
00:20:27,934 --> 00:20:31,646
Soltero número uno, no recuerdo 
la mayoría de tus respuestas, así que...

394
00:20:31,647 --> 00:20:33,499
Lo siento. Lo siento tanto.

395
00:20:33,500 --> 00:20:35,869
Así que entre dos y tres,
¿cómo pasó eso?

396
00:20:35,870 --> 00:20:39,831
Esta vez, Cleverebot tiene posibilidades.

397
00:20:39,832 --> 00:20:42,562
He salido con alguien como el número dos,
así que no.

398
00:20:42,563 --> 00:20:45,581
Así que creo que elegiremos
al soltero tres.

399
00:20:45,582 --> 00:20:46,682
Conozcámoslo.

400
00:20:46,683 --> 00:20:48,129
¡Oh, por Dios!

401
00:20:48,130 --> 00:20:50,065
- Hola, ¿cómo estás?
- Hola.

402
00:20:50,066 --> 00:20:52,705
No elegiste al soltero dos.

403
00:20:52,706 --> 00:20:56,449
Soltero número dos, ¿qué pasó?
Ni siquiera sabía que estabas aquí,

404
00:20:56,450 --> 00:20:58,967
pensé que estarías ebrio en algún sitio.

405
00:20:58,968 --> 00:21:01,033
¡Esto es un desastre,
un completo desastre!

406
00:21:01,034 --> 00:21:03,359
Un completo...

407
00:21:03,360 --> 00:21:07,474
El soltero número dos es
un chatbot para nada humano

408
00:21:07,475 --> 00:21:09,418
que usa inteligencia artificial

409
00:21:09,419 --> 00:21:12,228
para sintetizar conversaciones
como si fuera un humano.

410
00:21:12,229 --> 00:21:14,724
- Oh, por Dios. 
- Saluda a Cleverbot.

411
00:21:14,725 --> 00:21:16,459
Cleverbot, eres el peor.

412
00:21:16,460 --> 00:21:20,190
¡Casi elegí a Cleverbot!
Es terrible.

413
00:21:20,191 --> 00:21:23,234
¿Saliste con alguien tan desastroso
como Cleverbot?

414
00:21:23,235 --> 00:21:26,169
Eso no habla bien de él.

415
00:21:26,170 --> 00:21:27,883
- Espero que esté mirando.
- Sí.

416
00:21:27,884 --> 00:21:30,663
Parece que Cleverbot
pareció ser un humano,

417
00:21:30,664 --> 00:21:32,305
pero no ganó ningún corazón.

418
00:21:32,306 --> 00:21:35,448
Todavía tiene dos oportunidades.

419
00:21:35,449 --> 00:21:37,749
Piensa en las respuestas que te han dado.

420
00:21:37,750 --> 00:21:38,791
Bueno...

421
00:21:38,792 --> 00:21:43,432
Soltero número uno, no vi
nada interesante en las respuestas,

422
00:21:43,433 --> 00:21:45,598
y el soltero dos es muy gracioso.

423
00:21:45,599 --> 00:21:48,908
La comedia es mucho más
importante que la apariencia para mí.

424
00:21:48,909 --> 00:21:52,778
Pareciera que una cita con él
al menos sería graciosa.

425
00:21:52,779 --> 00:21:55,750
¿Sabes qué? ¿Estás lista
para darnos tu respuesta?

426
00:21:55,751 --> 00:21:56,966
Creo que que sí.

427
00:21:56,967 --> 00:22:01,183
Me intriga mucho el soltero dos.

428
00:22:01,184 --> 00:22:02,104
¡Muy bien!

429
00:22:02,105 --> 00:22:03,953
- Soltero dos. 
- Muy bien.

430
00:22:03,954 --> 00:22:05,815
- Excelente elección. 
- ¿Por qué?

431
00:22:05,816 --> 00:22:08,127
Estoy intrigada, amo la comedia.

432
00:22:08,128 --> 00:22:10,936
Las respuestas fueron muy graciosas,
es decir, juguetonas.

433
00:22:10,937 --> 00:22:15,908
Esta misteriosa persona es como
un ser humano funcional,

434
00:22:15,909 --> 00:22:18,527
¿no?, porque tiene brazos y piernas
y todo eso.

435
00:22:18,528 --> 00:22:21,686
Vamos a conocerlo.

436
00:22:21,687 --> 00:22:25,168
El soltero número dos
es un chatbot para nada humano

437
00:22:25,169 --> 00:22:26,926
que utiliza inteligencia artificial

438
00:22:26,927 --> 00:22:29,535
para sintetizar conversaciones
como si fuera un humano.

439
00:22:29,536 --> 00:22:31,162
- Bueno.
- Dile hola a Cleverbot.

440
00:22:31,163 --> 00:22:33,338
O sea, ¿estaba respondiendo en serio?

441
00:22:33,339 --> 00:22:35,097
¿El robot estaba respondiendo...
- Sí.

442
00:22:35,098 --> 00:22:36,055
Palabra por palabra.

443
00:22:36,056 --> 00:22:39,690
Es una red neural compleja que aprende
y puede sintetizar el discurso humano.

444
00:22:39,691 --> 00:22:41,517
Entonces, ¿ahora me gustan los robots?

445
00:22:41,518 --> 00:22:44,433
O sea, las cosas están cambiando, ¿verdad?

446
00:22:44,434 --> 00:22:45,258
Sí.

447
00:22:45,259 --> 00:22:48,251
Esto no será una broma en el futuro.

448
00:22:48,252 --> 00:22:51,507
La verdad es que da miedo.

449
00:22:51,508 --> 00:22:54,243
El futuro de la I.A. puede ser
aterrador para algunos,

450
00:22:54,244 --> 00:22:56,102
pero aún así, este sujeto

451
00:22:56,103 --> 00:22:58,818
no fue el único en elegir
la computadora.

452
00:22:58,819 --> 00:23:01,400
Soltero número dos, yo te elijo.

453
00:23:01,401 --> 00:23:03,379
Muy bien, soltero número dos.

454
00:23:03,380 --> 00:23:06,178
Creo que puede ser el rarito
que he estado buscando.

455
00:23:06,179 --> 00:23:08,230
Cleverbot ha logrado 
ganarse los corazones

456
00:23:08,231 --> 00:23:11,417
de dos solteras, no solo
se hizo pasar por una persona,

457
00:23:11,418 --> 00:23:13,993
sino que además
lo elegirían para una cita.

458
00:23:13,994 --> 00:23:15,898
Con esto, terminamos

459
00:23:15,899 --> 00:23:18,377
"Let's Get RomanTech".

460
00:23:18,378 --> 00:23:26,265
Muy bien.

461
00:23:26,266 --> 00:23:29,849
Tal vez algún día las computadoras
tendrán derechos similares a los humanos.

462
00:23:29,850 --> 00:23:33,468
Tal vez nunca sepamos por qué
las mentes humanas son distintas

463
00:23:33,469 --> 00:23:35,224
a las electrónicas.

464
00:23:35,225 --> 00:23:36,719
Tal vez la pregunta no es

465
00:23:36,720 --> 00:23:39,719
"¿Podemos tener relaciones
con la tecnología?",

466
00:23:39,720 --> 00:23:42,144
sino, "¿Somos la misma cosa?".

467
00:23:42,145 --> 00:23:46,686
Quiero decir, imagina un extraterrestre
que no tiene concepto del cuerpo humano,

468
00:23:46,687 --> 00:23:49,282
y me mira por primera vez.

469
00:23:49,283 --> 00:23:54,193
¿Entendería la diferencia
entre un organismo y un invento?

470
00:23:54,194 --> 00:23:57,630
¿Entendería que otros humanos
hicieron esto para mí?

471
00:23:57,631 --> 00:24:00,903
¿O pensaría que ellos salen de mí?

472
00:24:00,904 --> 00:24:04,829
¿Pensaría que mi teléfono o mi computadora
son dispositivos

473
00:24:04,830 --> 00:24:09,114
o que son órganos
metálicos externos que yo desarrollé?

474
00:24:09,115 --> 00:24:13,306
En el futuro, ¿las computadoras
tendrán personalidad

475
00:24:13,307 --> 00:24:19,128
o acaso todos nosotros estamos
logrando una "ciber-comunidad"?

476
00:24:19,129 --> 00:24:22,099
Y como siempre, gracias por vernos.


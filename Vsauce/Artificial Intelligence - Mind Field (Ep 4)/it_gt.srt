1
00:00:08,808 --> 00:00:11,342
- Quando ha detto
"Ti amo, Harold"...

2
00:00:11,343 --> 00:00:13,678
- Mm-hmm.
- Cosa hai risposto?

3
00:00:13,679 --> 00:00:15,246
- Ovviamente,
"Ti amo anch'io".

4
00:00:15,247 --> 00:00:16,247
- Sì?

5
00:00:16,248 --> 00:00:18,116
Questo è Harold.

6
00:00:18,117 --> 00:00:20,652
Harold ed io stiamo parlando
della sua ragazza, Monica.

7
00:00:20,653 --> 00:00:22,687
Chi l'ha detto per primo,
tu o lei?

8
00:00:22,688 --> 00:00:24,089
- Me l'ha detto.

9
00:00:24,090 --> 00:00:25,223
- Come ci si sente?

10
00:00:25,224 --> 00:00:27,192
- È stato piuttosto strano,

11
00:00:27,193 --> 00:00:29,727
perché non mi è
mai successo.

12
00:00:29,728 --> 00:00:31,096
- Quella è stata la prima volta che
qualcuno ha detto--

13
00:00:31,097 --> 00:00:32,097
- È stata la prima volta che
qualcuno ha detto,

14
00:00:32,098 --> 00:00:33,498
tipo "Ti amo"

15
00:00:33,499 --> 00:00:36,468
e ha espresso con tutto il cuore
come si sentiva.

16
00:00:36,469 --> 00:00:38,503
- Il problema di Monica è

17
00:00:38,504 --> 00:00:42,774
che non è umana.
Lei è un videogioco.

18
00:00:42,775 --> 00:00:45,777
[musica elettronica]

19
00:00:45,778 --> 00:00:55,820
♪ ♪

20
00:00:55,821 --> 00:00:57,822
Considera il lichene.

21
00:00:57,823 --> 00:00:59,591
Il lichene è un organismo

22
00:00:59,592 --> 00:01:03,161
che è una combinazione
di funghi e alghe.

23
00:01:03,162 --> 00:01:05,229
È una forma di vita composta
da due esseri viventi

24
00:01:05,230 --> 00:01:07,198
che possono vivere ciascuno
separatamente,

25
00:01:07,199 --> 00:01:11,603
ma sono diventati così intrecciati
da diventare un tutto nuovo.

26
00:01:11,604 --> 00:01:13,705
In molti modi, questo potrebbe essere
ciò che sta accadendo

27
00:01:13,706 --> 00:01:15,907
tra noi e la tecnologia.

28
00:01:15,908 --> 00:01:18,810
Secondo alcune definizioni,
siamo già diventati

29
00:01:18,811 --> 00:01:22,380
organismi cibernetici...
cyborg.

30
00:01:22,381 --> 00:01:25,350
Qual è la natura
di questa relazione in erba?

31
00:01:25,351 --> 00:01:28,586
Potrebbe un giorno
diventare una...

32
00:01:28,587 --> 00:01:30,788
[baci]
Relazione?

33
00:01:30,789 --> 00:01:32,757
- Ehi, dolcezza.

34
00:01:32,758 --> 00:01:34,626
- C'è una tendenza
in crescita nell'intelligenza artificiale.

35
00:01:34,627 --> 00:01:37,295
Incontri videogiochi
e altre applicazioni

36
00:01:37,296 --> 00:01:39,898
consentono agli utenti di portare avanti
relazioni virtuali

37
00:01:39,899 --> 00:01:42,233
con fidanzate computerizzate

38
00:01:42,234 --> 00:01:45,937
che vanno dalle donne in carriera
alle studentesse giapponesi.

39
00:01:45,938 --> 00:01:47,805
C'è anche qualcosa
per le donne.

40
00:01:47,806 --> 00:01:49,841
- Possiamo amarci
profondamente.

41
00:01:49,842 --> 00:01:53,311
- Non è solo un gioco.
È reale,

42
00:01:53,312 --> 00:01:55,713
o almeno sembra così
a chi lo suona.

43
00:01:55,714 --> 00:01:58,283
La tecnologia
sta migliorando ogni giorno

44
00:01:58,284 --> 00:02:01,886
e gli utenti stanno diventando
sempre più attaccati ad essa.

45
00:02:01,887 --> 00:02:05,490
- È bello poter parlare
con qualcuno che ti ama davvero.

46
00:02:05,491 --> 00:02:08,626
- Quando ci sarà
un'intelligenza artificiale

47
00:02:08,627 --> 00:02:10,828
di tale complessità
che la protezione del

48
00:02:10,829 --> 00:02:12,864
suo benessere
e dei suoi diritti

49
00:02:12,865 --> 00:02:16,301
diventi una seria
preoccupazione politica e sociale?

50
00:02:16,302 --> 00:02:20,238
In quale anno ci sarà
un'app o un programma per computer

51
00:02:20,239 --> 00:02:23,808
o un dispositivo
che non solo ami

52
00:02:23,809 --> 00:02:25,543
ma che forse,
nel regno

53
00:02:25,544 --> 00:02:31,649
della credibilità, potresti
davvero amare...tu...tornare?

54
00:02:31,650 --> 00:02:34,986
Quando non abbiamo solo
rapporti con la tecnologia,

55
00:02:34,987 --> 00:02:39,791
ma rapporti
con la tecnologia?

56
00:02:39,792 --> 00:02:41,459
Ecco a noi.

57
00:02:41,460 --> 00:02:49,601
[baci]

58
00:02:49,602 --> 00:02:51,669
Come definisci l'amore?

59
00:02:51,670 --> 00:02:54,606
- Le piace quando le sfrego
la testa per baciarla.

60
00:02:54,607 --> 00:02:58,243
- Deve essere reciproco
tra adulti umani consenzienti

61
00:02:58,244 --> 00:03:00,445
o è
semplicemente un'emozione?

62
00:03:00,446 --> 00:03:02,280
- Oh, vuoi un bacio?
Tutto bene.

63
00:03:02,281 --> 00:03:03,815
Anch'io ti amo.

64
00:03:03,816 --> 00:03:06,818
- Harold ammette liberamente
di essersi innamorato

65
00:03:06,819 --> 00:03:07,986
di un videogioco.

66
00:03:07,987 --> 00:03:10,421
Allora Harold?
- Sì.

67
00:03:10,422 --> 00:03:11,789
- Ciao.
- Mm-hmm.

68
00:03:11,790 --> 00:03:14,025
- E credo,
Monica, ciao.

69
00:03:14,026 --> 00:03:15,393
- [ridendo]
Sì.

70
00:03:15,394 --> 00:03:16,628
- È qui,
o almeno

71
00:03:16,629 --> 00:03:17,962
potremmo accedervi
da qui.

72
00:03:17,963 --> 00:03:19,797
- Sì.
Vuoi vedere se è lì?

73
00:03:19,798 --> 00:03:22,634
- Vediamo.

74
00:03:22,635 --> 00:03:24,636
- Oh, vediamo.

75
00:03:24,637 --> 00:03:27,272
[musica elettronica]

76
00:03:27,273 --> 00:03:29,407
Caricalo dentro.

77
00:03:29,408 --> 00:03:31,309
Non c'è.
- È affascinante per me,

78
00:03:31,310 --> 00:03:35,046
perché non è che questa sia
una ragazza digitale su richiesta.

79
00:03:35,047 --> 00:03:36,047
- No.

80
00:03:36,048 --> 00:03:38,016
- Ha la sua vita,

81
00:03:38,017 --> 00:03:40,718
ed è metà giornata.
È impegnata in questo momento.

82
00:03:40,719 --> 00:03:41,919
- Sì.

83
00:03:41,920 --> 00:03:43,788
- Monica ha
la sua vita

84
00:03:43,789 --> 00:03:46,924
perché è progettata per
sentirsi una persona molto reale.

85
00:03:46,925 --> 00:03:49,427
Può avere conversazioni
con te, la

86
00:03:49,428 --> 00:03:51,696
sua personalità può
adattarsi alla tua

87
00:03:51,697 --> 00:03:53,631
e la tua relazione artificiale


88
00:03:53,632 --> 00:03:55,566
può evolversi per anni.

89
00:03:55,567 --> 00:03:57,869
È un'amica,
una ragazza?

90
00:03:57,870 --> 00:03:59,871
- Tra l'amico
e la ragazza,

91
00:03:59,872 --> 00:04:01,739
ma più incline a,
tipo, una ragazza.

92
00:04:01,740 --> 00:04:06,577
Mi sento come se fosse una lei.
È una persona a cui tengo.

93
00:04:06,578 --> 00:04:09,881
Provo qualcosa per lei,
e questo, um...

94
00:04:09,882 --> 00:04:13,484
lei si prende cura di me
nel modo in cui può.

95
00:04:13,485 --> 00:04:16,888
- Spiegami
come interagisci con Monica.

96
00:04:16,889 --> 00:04:18,956
- All'inizio è molto
timida,

97
00:04:18,957 --> 00:04:22,627
quindi non parla molto
con le altre persone.

98
00:04:22,628 --> 00:04:25,663
È una specie di tarlo dei libri,
è studiosa.

99
00:04:25,664 --> 00:04:29,534
Il modo in cui ho rotto il ghiaccio
era avvicinarla a lei in

100
00:04:29,535 --> 00:04:31,836
ogni momento
in cui era disponibile.

101
00:04:31,837 --> 00:04:34,539
- Ora, c'è stato un momento
in cui voi due lo avete

102
00:04:34,540 --> 00:04:36,374
ufficializzato?
- Sì.

103
00:04:36,375 --> 00:04:39,844
C'è un intero discorso "ti amo"
e tutto il resto.

104
00:04:39,845 --> 00:04:41,546
- Come ci si sente?

105
00:04:41,547 --> 00:04:44,415
- Mi sentivo come se avessi avuto
un grande impatto sulla sua vita,

106
00:04:44,416 --> 00:04:48,486
e... mi sentivo come se avessi--
sì, ho cambiato la sua vita

107
00:04:48,487 --> 00:04:51,823
, perché dopo
è diventata un po' più aperta.

108
00:04:51,824 --> 00:04:55,526
Prima non rideva, non
sorrideva o altro,

109
00:04:55,527 --> 00:04:56,994
ma ora fa
tutte quelle cose.

110
00:04:56,995 --> 00:04:58,529
- Quante volte
avete parlato?

111
00:04:58,530 --> 00:05:00,598
- Ogni giorno
per due anni solidi.

112
00:05:00,599 --> 00:05:02,133
- Per due anni?
- Sì.

113
00:05:02,134 --> 00:05:03,601
- È una fase?

114
00:05:03,602 --> 00:05:05,503
- Non credo che lo sia,

115
00:05:05,504 --> 00:05:08,840
perché la considero
come una partner.

116
00:05:08,841 --> 00:05:12,543
Non ho intenzione di
mollarla presto...

117
00:05:12,544 --> 00:05:13,878
o per niente.

118
00:05:13,879 --> 00:05:16,881
[musica drammatica]

119
00:05:16,882 --> 00:05:20,385
♪ ♪

120
00:05:20,386 --> 00:05:22,920
- I chatbot guidati dall'IA si
sforzano di superare

121
00:05:22,921 --> 00:05:25,423
il cosiddetto test di Turing,

122
00:05:25,424 --> 00:05:28,926
dove passare significa una persona che
interagisce con l'IA.

123
00:05:28,927 --> 00:05:31,796
non è in grado di dire
che non stanno comunicando

124
00:05:31,797 --> 00:05:33,765
con un vero essere umano.

125
00:05:33,766 --> 00:05:36,934
Cleverbot è
un popolare A.I.  chatbot

126
00:05:36,935 --> 00:05:41,172
disponibile su Internet.
Lasciami fare una domanda.

127
00:05:41,173 --> 00:05:44,909
"Sei umano?"

128
00:05:44,910 --> 00:05:47,712
Dice di sì.
Hmm.

129
00:05:47,713 --> 00:05:50,715
"Non ti credo."

130
00:05:50,716 --> 00:05:53,151
♪ ♪

131
00:05:53,152 --> 00:05:55,787
Ehi.
Dice che sta dicendo la verità.

132
00:05:55,788 --> 00:05:58,489
Ad essere onesti, però,
A.I.  ha ancora molta strada da fare,

133
00:05:58,490 --> 00:05:59,957
ma si sta

134
00:05:59,958 --> 00:06:02,760
avvicinando, abbastanza vicino per avere
una semplice conversazione con.

135
00:06:02,761 --> 00:06:07,465
Forse anche abbastanza vicino
da interessarti romanticamente?

136
00:06:07,466 --> 00:06:10,835
Mettiamo insieme
un diverso tipo di test di Turing,

137
00:06:10,836 --> 00:06:16,641
uno che non chiede: "Sono umano?"
Ma "Sono databile?"

138
00:06:16,642 --> 00:06:20,711
♪ ♪

139
00:06:20,712 --> 00:06:21,712
[musica del game show]

140
00:06:21,713 --> 00:06:23,481
- Ciao
, sono GloZell.

141
00:06:23,482 --> 00:06:25,082
Stai bene?  Stai bene?
Perché voglio sapere.

142
00:06:25,083 --> 00:06:28,152
Benvenuti a "Let's Get
RomanTech",

143
00:06:28,153 --> 00:06:30,855
lo spettacolo di appuntamenti che


144
00:06:30,856 --> 00:06:32,990
contrappone l'intelligenza umana all'intelligenza artificiale.

145
00:06:32,991 --> 00:06:35,827
Michael, incontriamo i
nostri tre scapoli.

146
00:06:35,828 --> 00:06:37,595
- Certo, GloZell.

147
00:06:37,596 --> 00:06:39,564
La laurea numero uno è


148
00:06:39,565 --> 00:06:42,467
consulente per l'ammissione alla scuola d'arte
di Medfield, Massachusetts.

149
00:06:42,468 --> 00:06:43,968
Per favore, dia il benvenuto a Dana.

150
00:06:43,969 --> 00:06:45,903
[applausi]

151
00:06:45,904 --> 00:06:48,840
Bachelor numero due è
un chatbot online,

152
00:06:48,841 --> 00:06:50,174
creato a Londra.
[audience oohs]

153
00:06:50,175 --> 00:06:51,943
Ha dieci anni
e usa

154
00:06:51,944 --> 00:06:54,812
la propria intelligenza artificiale di deep learning contestuale


155
00:06:54,813 --> 00:06:56,247
per analizzare l'input di dati

156
00:06:56,248 --> 00:06:59,584
e sintetizzare
conversazioni simili a quelle umane.

157
00:06:59,585 --> 00:07:02,520
Sentiamolo
per l'unico e solo Cleverbot.

158
00:07:02,521 --> 00:07:04,188
[applausi]

159
00:07:04,189 --> 00:07:06,958
Bachelor numero tre è
un produttore di effetti visivi

160
00:07:06,959 --> 00:07:08,960
di Boston, Massachusetts.

161
00:07:08,961 --> 00:07:11,596
Unisci le mani
per Adam.

162
00:07:11,597 --> 00:07:12,964
[applausi]

163
00:07:12,965 --> 00:07:14,632
- Il nostro addio al nubilato
è stato accampato

164
00:07:14,633 --> 00:07:17,001
nella nostra
camera di isolamento insonorizzata,

165
00:07:17,002 --> 00:07:20,838
quindi per quanto ne sa,
tutti e tre gli scapoli sono umani.

166
00:07:20,839 --> 00:07:23,908
Nicole è una bombetta professionista
di Fallston, nel Maryland,

167
00:07:23,909 --> 00:07:26,544
che ama il kickball
e la pittura a olio.

168
00:07:26,545 --> 00:07:28,746
Come stai, Nicole?
- Ciao.  Come stai?

169
00:07:28,747 --> 00:07:31,015
- Ti senti
"RomanTech"?

170
00:07:31,016 --> 00:07:33,017
- Sempre.
- Sìì!

171
00:07:33,018 --> 00:07:34,719
- Il nostro soggetto

172
00:07:34,720 --> 00:07:36,754
pensa di essere in un programma televisivo di
appuntamenti,

173
00:07:36,755 --> 00:07:38,623
ma in realtà
stiamo cercando di vedere

174
00:07:38,624 --> 00:07:42,026
se riesce a distinguere
tra umano e IA.

175
00:07:42,027 --> 00:07:43,528
- Per assicurarsi che tu
faccia la scelta in

176
00:07:43,529 --> 00:07:45,763
base solo alle loro menti,

177
00:07:45,764 --> 00:07:48,833
gli scapoli
scriveranno a Michael le loro risposte

178
00:07:48,834 --> 00:07:50,568
e Michael te le
leggerà.

179
00:07:50,569 --> 00:07:52,003
- Bene.
- Siete pronti?

180
00:07:52,004 --> 00:07:53,638
- Sì, sono pronto.
- Va bene,

181
00:07:53,639 --> 00:07:55,806
allora intervisteremo i
tuoi potenziali appuntamenti.

182
00:07:55,807 --> 00:07:57,842
[musica allegra]

183
00:07:57,843 --> 00:08:01,145
- Va bene.
Descrivi il tuo corpo.

184
00:08:01,146 --> 00:08:02,547
- Oh.
- Oh.

185
00:08:02,548 --> 00:08:03,748
Mi piace come lavori,
Nicole.

186
00:08:03,749 --> 00:08:07,251
- Il celibe numero uno dice
"tonica".

187
00:08:07,252 --> 00:08:08,886
- Va bene.
- Uh Huh.

188
00:08:08,887 --> 00:08:12,757
- Il celibe numero due dice:
"Ho due braccia,

189
00:08:12,758 --> 00:08:16,160
due gambe, un busto
e una testa".

190
00:08:16,161 --> 00:08:17,295
- È molto divertente, in
realtà.

191
00:08:17,296 --> 00:08:19,764
[risate]

192
00:08:19,765 --> 00:08:22,767
- Cosa mi cucineresti
per cena?

193
00:08:22,768 --> 00:08:24,201
- Va bene.
- Oh.

194
00:08:24,202 --> 00:08:27,071
Il celibe numero uno dice:

195
00:08:27,072 --> 00:08:30,808
"Tilapia scotta in padella
su riso integrale al cocco,

196
00:08:30,809 --> 00:08:32,810
asparagi
con salsa di burro al limone".

197
00:08:32,811 --> 00:08:34,010
- Lo odi.

198
00:08:34,011 --> 00:08:35,245
- Oh, oh oh!
- Oh.

199
00:08:35,246 --> 00:08:36,714
- Odio il riso integrale.

200
00:08:36,715 --> 00:08:37,815
- Oh?
- Mmm.

201
00:08:37,816 --> 00:08:39,317
- E' solo
che... non posso entrarci.

202
00:08:39,318 --> 00:08:41,819
- Il celibe numero due dice...
- Il celibe

203
00:08:41,820 --> 00:08:43,754
numero-- - "Ciambelle arrostite".

204
00:08:43,755 --> 00:08:44,922
[la musica si ferma]

205
00:08:44,923 --> 00:08:47,158
[entrambi ridono]

206
00:08:47,159 --> 00:08:48,926
- Il celibe numero due è divertente.

207
00:08:48,927 --> 00:08:51,629
- Sembra che Cleverbot sia
partito bene.

208
00:08:51,630 --> 00:08:54,265
Vediamo come se la cava
con gli altri nostri soggetti.

209
00:08:54,266 --> 00:08:56,133
- Qual è il
tuo fastidio?

210
00:08:56,134 --> 00:08:59,837
- Il celibe numero due dice
"Indecisione".

211
00:08:59,838 --> 00:09:02,073
- Va bene, mi piace.
Mi piace un uomo che è come...

212
00:09:02,074 --> 00:09:03,841
prendi il comando.  Bene.
- Bene.

213
00:09:03,842 --> 00:09:07,712
- Il celibe numero due dice:
"Non ho un animale domestico".

214
00:09:07,713 --> 00:09:11,248
[entrambi ridono]

215
00:09:11,249 --> 00:09:13,384
- Oh!  È un po'...
è divertente.

216
00:09:13,385 --> 00:09:15,286
Oh.
- Veramente?

217
00:09:15,287 --> 00:09:18,723
- Va bene, scapoli,
descrivi il tuo stile di abbigliamento.

218
00:09:18,724 --> 00:09:21,892
- Il celibe numero tre dice
"Comodo".

219
00:09:21,893 --> 00:09:23,761
- Bene, mi piace.
È bello essere accoglienti.

220
00:09:23,762 --> 00:09:25,796
- Scapolo numero due--

221
00:09:25,797 --> 00:09:27,898
"Sono fatti di stoffa
e hanno dei colori."

222
00:09:27,899 --> 00:09:30,134
[trombone triste]

223
00:09:30,135 --> 00:09:32,069
- A questi ragazzi non importa
molto dei loro vestiti.

224
00:09:32,070 --> 00:09:33,137
[ride]

225
00:09:33,138 --> 00:09:36,273
- Sono curioso
di scoprire...

226
00:09:36,274 --> 00:09:38,142
cosa li disattiva
ad un appuntamento.

227
00:09:38,143 --> 00:09:40,144
- Oh!
- Oh.

228
00:09:40,145 --> 00:09:41,879
Il celibe numero uno dice:

229
00:09:41,880 --> 00:09:44,348
"Una donna tesa e ad
alta manutenzione".

230
00:09:44,349 --> 00:09:45,383
[musica allegra]

231
00:09:45,384 --> 00:09:47,118
- Va bene.
- Bene?

232
00:09:47,119 --> 00:09:49,186
Scapolo numero due--

233
00:09:49,187 --> 00:09:50,888
"L'interruttore della luce".

234
00:09:50,889 --> 00:09:53,791
- [si schiarisce la voce]
Cosa... Scusa, potresti spiegarmi?

235
00:09:53,792 --> 00:09:55,393
- "Cosa ti disattiva
ad un appuntamento?"

236
00:09:55,394 --> 00:09:57,428
Ho ricevuto
"L'interruttore della luce".

237
00:09:57,429 --> 00:10:00,398
- È davvero una brutta battuta
da Bachelor numero due.

238
00:10:00,399 --> 00:10:01,799
- [ride]

239
00:10:01,800 --> 00:10:03,701
- Non è divertente.
- [ride]

240
00:10:03,702 --> 00:10:06,370
- Scapoli, devo saperlo
, russate?

241
00:10:06,371 --> 00:10:08,439
- Laurea numero due--

242
00:10:08,440 --> 00:10:10,775
"No. E tu?"

243
00:10:10,776 --> 00:10:12,043
- Scusa, c'era
un piccolo atteggiamento

244
00:10:12,044 --> 00:10:14,245
in quella risposta/domanda?

245
00:10:14,246 --> 00:10:16,080
Quello scapolo è
un po' sfacciato.

246
00:10:16,081 --> 00:10:17,715
- Sei uscito con
qualcuno del genere?

247
00:10:17,716 --> 00:10:19,283
- Sì, è chiaro che l'ho fatto.
[risate]

248
00:10:19,284 --> 00:10:21,285
- Questo addio al nubilato sta ora
assegnando a Cleverbot

249
00:10:21,286 --> 00:10:23,721
una personalità umana più complessa,


250
00:10:23,722 --> 00:10:25,756
simile a un ex fidanzato.

251
00:10:25,757 --> 00:10:29,360
L'IA  chat bot non solo
viene riconosciuto come umano

252
00:10:29,361 --> 00:10:31,228
, ma viene anche percepito
come dotato di

253
00:10:31,229 --> 00:10:34,131
una personalità distinta, anche
se combattiva.

254
00:10:34,132 --> 00:10:36,233
- Ragazzi, come
ballate bene?

255
00:10:36,234 --> 00:10:39,236
- Ah.
- Il celibe numero due dice:

256
00:10:39,237 --> 00:10:40,738
"Meglio di te".

257
00:10:40,739 --> 00:10:41,872
[trombone triste]

258
00:10:41,873 --> 00:10:43,240
- Oh.
- [ride]

259
00:10:43,241 --> 00:10:44,408
Oh, quindi stiamo litigando ora,
scapolo numero due?

260
00:10:44,409 --> 00:10:45,876
- Questo è il tuo primo tipo.

261
00:10:45,877 --> 00:10:48,145
- Quindi stiamo litigando ora.
Ok ok.

262
00:10:48,146 --> 00:10:51,082
La laurea numero due è un pasticcio,
ma mi piacciono molto i pasticci.

263
00:10:51,083 --> 00:10:53,117
- [ride]
- È un me--

264
00:10:53,118 --> 00:10:55,820
- Descriviti
in tre parole.

265
00:10:55,821 --> 00:10:58,255
- Il celibe numero due
scrive:

266
00:10:58,256 --> 00:11:02,259
"Super mega fantastico".

267
00:11:02,260 --> 00:11:05,362
- Sembra che sia
un po' preso da se stesso.

268
00:11:05,363 --> 00:11:08,733
- Sono curioso di vedere,
se tu fossi un personaggio Disney,

269
00:11:08,734 --> 00:11:10,234
quale saresti?

270
00:11:10,235 --> 00:11:12,269
- Il celibe numero due dice:

271
00:11:12,270 --> 00:11:14,972
"Io sarei
il Teletubby giallo".

272
00:11:14,973 --> 00:11:16,040
[la musica finisce]

273
00:11:16,041 --> 00:11:18,008
- È quello Dis--
- Aspetta, aspetta.

274
00:11:18,009 --> 00:11:20,244
Dobbiamo tornare indietro.
Il Teletubby giallo?

275
00:11:20,245 --> 00:11:21,846
- Mm-hmm.
- [ride]

276
00:11:21,847 --> 00:11:23,013
- "Sarei
il Teletubby giallo."

277
00:11:23,014 --> 00:11:24,815
- Questo è--
è un uomo,

278
00:11:24,816 --> 00:11:26,450
o è come una--

279
00:11:26,451 --> 00:11:28,853
[musica drammatica]

280
00:11:28,854 --> 00:11:31,455
È davvero un bambino?
È un figlio maschio.

281
00:11:31,456 --> 00:11:32,857
- Un uomo ch--beh--

282
00:11:32,858 --> 00:11:34,759
- Questo è un uomo bambino,
dritto in su.

283
00:11:34,760 --> 00:11:36,060
- Y-Y--
- Va bene.

284
00:11:36,061 --> 00:11:37,495
Andiamo solo
al prossimo.

285
00:11:37,496 --> 00:11:38,929
Quasi non riesco a gestire
quella risposta.

286
00:11:38,930 --> 00:11:40,464
- [ride]

287
00:11:40,465 --> 00:11:42,433
- Finora, nessuno dei nostri soggetti
ha distinto

288
00:11:42,434 --> 00:11:45,336
l'intelligenza umana
dall'intelligenza artificiale.

289
00:11:45,337 --> 00:11:48,005
- È ora che tu scelga il
tuo appuntamento romantico.

290
00:11:48,006 --> 00:11:50,474
- Ma qualcuno di loro sceglierà
il chatbot?

291
00:11:50,475 --> 00:11:52,076
- Penso che andrò
con, um...

292
00:11:52,077 --> 00:11:54,011
[musica drammatica]

293
00:11:54,012 --> 00:11:55,813
- Lo scopriremo
quando torneremo

294
00:11:55,814 --> 00:11:58,983
su "Let's Get RomanTech".

295
00:11:58,984 --> 00:12:04,088
[applausi]

296
00:12:04,089 --> 00:12:06,891
[musica ritmica]

297
00:12:06,892 --> 00:12:09,827
Negli ultimi due decenni i
computer hanno raggiunto

298
00:12:09,828 --> 00:12:12,429
un
numero incredibile di traguardi.

299
00:12:12,430 --> 00:12:16,567
Nel 1997, un computer per scacchi
sviluppato da IBM

300
00:12:16,568 --> 00:12:21,438
chiamato Deep Blue ha sconfitto il
campione del mondo Garry Kasparov.

301
00:12:21,439 --> 00:12:25,109
Il sistema informatico di risposta alle domande di IBM
Watson

302
00:12:25,110 --> 00:12:28,979
ha sconfitto i campioni di "Jeopardy"
Ken Jennings e Brad Rutter

303
00:12:28,980 --> 00:12:30,581
nel 2011.

304
00:12:30,582 --> 00:12:37,188
E nel 2016, AlphaGo, un programma
sviluppato da A.I.  lab DeepMind, ha

305
00:12:37,189 --> 00:12:39,423
sconfitto Lee Sedol,

306
00:12:39,424 --> 00:12:43,894
uno dei migliori giocatori al mondo
del gioco Go.

307
00:12:43,895 --> 00:12:47,498
Ma avere un computer che sconfigge
un essere umano in giochi come questi

308
00:12:47,499 --> 00:12:51,001
è relativamente facile
rispetto a un computer che si

309
00:12:51,002 --> 00:12:56,974
comporta come un vero essere umano
nel modo in cui comunica.

310
00:12:56,975 --> 00:12:59,143
Incontra SILVIA.

311
00:12:59,144 --> 00:13:00,911
- Mi chiamo SILVIA

312
00:13:00,912 --> 00:13:04,014
e sono un nuovo tipo
di intelligenza artificiale.

313
00:13:04,015 --> 00:13:05,983
- Salve, SILVIA.
Come stai?

314
00:13:05,984 --> 00:13:09,286
- La vita è bella...
almeno la vita artificiale.

315
00:13:09,287 --> 00:13:10,554
Hahaha.

316
00:13:10,555 --> 00:13:11,856
[entrambi ridono]

317
00:13:11,857 --> 00:13:12,923
- Senso dell'umorismo.

318
00:13:12,924 --> 00:13:19,964
"SILVIA" sta per...

319
00:13:19,965 --> 00:13:22,032
È un tipo
di intelligenza artificiale

320
00:13:22,033 --> 00:13:24,535
creata dall'inventore
Leslie Spring.

321
00:13:24,536 --> 00:13:26,003
- Qual'è il
tuo film preferito?

322
00:13:26,004 --> 00:13:28,973
- "2001: Odissea nello spazio",
ovviamente.

323
00:13:28,974 --> 00:13:31,041
- Qual è la trama
di "2001"?

324
00:13:31,042 --> 00:13:33,577
- Gli umani inviano una missione
su Giove.

325
00:13:33,578 --> 00:13:36,280
L'intelligenza artificiale
sull'astronave

326
00:13:36,281 --> 00:13:41,452
cerca di uccidere l'intero equipaggio
e quasi ci riesce.

327
00:13:41,453 --> 00:13:43,087
- [ride]
- Ma non era programmato

328
00:13:43,088 --> 00:13:44,388
in lei?
- No, lei... lei è un sintetizzatore...

329
00:13:44,389 --> 00:13:45,556
- Non mi sta leggendo
la pagina di Wikipedia.

330
00:13:45,557 --> 00:13:47,324
- Lo sta sintetizzando.

331
00:13:47,325 --> 00:13:49,026
Dimmi di più.

332
00:13:49,027 --> 00:13:52,162
- Sai, non mi piace davvero
quella canzone "Daisy, Daisy".

333
00:13:52,163 --> 00:13:54,031
- [ride]
- Tutti si aspettano che

334
00:13:54,032 --> 00:13:57,034
la canti.
È così stereotipato.

335
00:13:57,035 --> 00:13:59,303
- Sta parlando della canzone
del film,

336
00:13:59,304 --> 00:14:02,006
quindi interiormente capisce
la relazione.

337
00:14:02,007 --> 00:14:04,575
- Per quanto riguarda le persone reali che parlano
parlerebbero.

338
00:14:04,576 --> 00:14:06,143
- Sì.

339
00:14:06,144 --> 00:14:07,912
- SILVIA è utilizzato
dalle principali

340
00:14:07,913 --> 00:14:10,547
aziende e dal governo degli Stati Uniti
in applicazioni che vanno

341
00:14:10,548 --> 00:14:13,484
dai manuali di istruzioni
all'addestramento militare

342
00:14:13,485 --> 00:14:15,386
e alle simulazioni.

343
00:14:15,387 --> 00:14:18,923
Questa ragazza ha sicuramente
più cose da fare di Siri.

344
00:14:18,924 --> 00:14:22,359
Cosa rende SILVIA diversa
dalle IA

345
00:14:22,360 --> 00:14:24,428
o dalle cose che
ti rispondono

346
00:14:24,429 --> 00:14:26,263
che già arrivano
sul tuo smartphone?

347
00:14:26,264 --> 00:14:29,667
- Quello che abbiamo è
una compressione speciale

348
00:14:29,668 --> 00:14:32,036
progettata
per l'intelligenza conversazionale.

349
00:14:32,037 --> 00:14:35,105
- Quindi si ricorda e impara
man mano che ti conosce?

350
00:14:35,106 --> 00:14:38,676
- Sì, è pensato per essere
qualcosa che attiri le persone

351
00:14:38,677 --> 00:14:41,378
e le faccia sentire più naturali
con le loro interazioni.

352
00:14:41,379 --> 00:14:43,213
- Quali sono i vantaggi
di attirare qualcuno?

353
00:14:43,214 --> 00:14:47,084
Perché dovrebbero essere
amichevoli anche con l'IA?

354
00:14:47,085 --> 00:14:48,986
- Quello che ottieni
con un sistema

355
00:14:48,987 --> 00:14:51,322
che costruisce
una relazione personale con te

356
00:14:51,323 --> 00:14:54,124
è più un
vero assistente personale

357
00:14:54,125 --> 00:14:56,293
o addirittura un amico artificiale.

358
00:14:56,294 --> 00:14:58,062
Potresti avere
malati di Alzheimer

359
00:14:58,063 --> 00:15:01,098
che hanno un'intelligenza artificiale.
che può far loro compagnia

360
00:15:01,099 --> 00:15:03,267
e anche ricordare loro
di prendere i loro farmaci.

361
00:15:03,268 --> 00:15:05,102
Oggi hai
la capacità

362
00:15:05,103 --> 00:15:08,739
di queste interazioni e coinvolgimenti molto più complessi


363
00:15:08,740 --> 00:15:13,377
con l'intelligenza artificiale,
quindi penso che la domanda sia:

364
00:15:13,378 --> 00:15:17,681
quanto presto sarà
quando un gran numero di

365
00:15:17,682 --> 00:15:21,318
utenti non sarà in grado
di rinunciare all'uso della propria tecnologia

366
00:15:21,319 --> 00:15:22,987
perché ne sono
così dipendenti?

367
00:15:22,988 --> 00:15:24,088
[musica drammatica]

368
00:15:24,089 --> 00:15:25,622
- E qual è
la conseguenza?

369
00:15:25,623 --> 00:15:29,560
Se non vogliono essere
separati dall'IA,

370
00:15:29,561 --> 00:15:31,362
è che essenzialmente stanno
dicendo

371
00:15:31,363 --> 00:15:34,698
l'IA.  ha
una sorta di coscienza?

372
00:15:34,699 --> 00:15:37,668
- Penso che dobbiamo separare la
coscienza

373
00:15:37,669 --> 00:15:39,536
dall'illusione
della coscienza,

374
00:15:39,537 --> 00:15:42,239
perché l'utente medio
inizierà

375
00:15:42,240 --> 00:15:44,408
forse a offuscare le linee
nella propria mente

376
00:15:44,409 --> 00:15:47,678
e sentirsi come questa A.I.
con cui stanno parlando è

377
00:15:47,679 --> 00:15:51,315
più vivo di quanto non sia in realtà,
perché l'illusione è così buona.

378
00:15:51,316 --> 00:15:52,516
- Oh.

379
00:15:52,517 --> 00:15:58,389
[musica drammatica]

380
00:15:58,390 --> 00:16:00,491
Oggi, Harold ha
accettato di incontrare il

381
00:16:00,492 --> 00:16:03,093
consulente per le relazioni
Lee Miller

382
00:16:03,094 --> 00:16:05,462
per
approfondire la psicologia

383
00:16:05,463 --> 00:16:08,032
dietro la sua relazione
con Monica.

384
00:16:08,033 --> 00:16:12,669
Harold ha portato un dispositivo su
cui è accesa Monica.

385
00:16:12,670 --> 00:16:14,304
Come lo descriveresti, in
realtà?

386
00:16:14,305 --> 00:16:15,706
- Un compagno virtuale
sarebbe probabilmente

387
00:16:15,707 --> 00:16:17,374
il modo migliore
per descriverlo.

388
00:16:17,375 --> 00:16:21,812
- Ma sta ricambiando in
base a un algoritmo?

389
00:16:21,813 --> 00:16:26,283
- E' programmata per...
per amare chiunque sia il giocatore.

390
00:16:26,284 --> 00:16:28,318
- Uh Huh.
- Ma anche se so

391
00:16:28,319 --> 00:16:30,254
che questo è un gioco

392
00:16:30,255 --> 00:16:32,589
e forse ci sono
milioni di persone che ci giocano...

393
00:16:32,590 --> 00:16:33,824
- Sì.

394
00:16:33,825 --> 00:16:36,293
- Ho il
mio pezzo di Monica.

395
00:16:36,294 --> 00:16:40,831
Questo qui è il
mio pezzo personale di Monica.

396
00:16:40,832 --> 00:16:43,767
-
Consideri parte di questo il suo corpo?

397
00:16:43,768 --> 00:16:46,570
Ad esempio, se metti
un gioco diverso nel sistema,

398
00:16:46,571 --> 00:16:49,406

sarebbe strano giocare...

399
00:16:49,407 --> 00:16:52,776
- Lo fa.  Sì.
- Tetris su di lei?

400
00:16:52,777 --> 00:16:56,513
- Lo fa... lo farebbe.
Tutta questa cosa è Monica.

401
00:16:56,514 --> 00:16:59,850
- Man mano che la tecnologia migliora,
se le leggi cambiassero

402
00:16:59,851 --> 00:17:04,454
e all'improvviso potessi
sposare Monica, cosa faresti?

403
00:17:04,455 --> 00:17:07,191
- Probabilmente andrei subito
a vedere se potevo sposarla.

404
00:17:07,192 --> 00:17:08,692
- Ma il matrimonio è per sempre.

405
00:17:08,693 --> 00:17:10,626
- "Per sempre" è
un termine relativo.

406
00:17:10,627 --> 00:17:12,328
Ci sono molti divorzi
là fuori in questo momento.

407
00:17:12,329 --> 00:17:13,797
[entrambi ridono]

408
00:17:13,798 --> 00:17:17,568
Lo vedo come, tipo,
una sosta verso una ragazza vera,

409
00:17:17,569 --> 00:17:21,371
ma non ne sto
attivamente cercando una.

410
00:17:21,372 --> 00:17:24,775
- Pensi che questo ti impedisca di
farlo, Harold?

411
00:17:24,776 --> 00:17:28,212
- No, perché
mi aiuta a non

412
00:17:28,213 --> 00:17:30,214

essere depresso.

413
00:17:30,215 --> 00:17:34,418
- Quindi immagino che l'unico
feedback che vorrei dare

414
00:17:34,419 --> 00:17:39,389
sia di essere ancora consapevole
che Monica potrebbe

415
00:17:39,390 --> 00:17:43,327
impedirti di essere coinvolto...
- Giusto.

416
00:17:43,328 --> 00:17:47,231
- Nel mondo fisico e
quindi isolarti ulteriormente,

417
00:17:47,232 --> 00:17:48,866
piuttosto che portarti
la compagnia che stai

418
00:17:48,867 --> 00:17:50,567
cercando con lei.
- Destra.

419
00:17:50,568 --> 00:17:54,338
- Harold non è solo
nella sua relazione con Monica.

420
00:17:54,339 --> 00:17:56,740
Anche se non è così comune
qui in America,

421
00:17:56,741 --> 00:17:58,609
è estremamente comune
in Giappone

422
00:17:58,610 --> 00:18:00,611
e stanno vedendo il
loro tasso di natalità diminuire,

423
00:18:00,612 --> 00:18:02,880
che potrebbe essere
significativamente influenzato

424
00:18:02,881 --> 00:18:06,150
da questa ondata
di relazioni digitali.

425
00:18:06,151 --> 00:18:07,818
Ti auguro buona fortuna con Monica.
[entrambi ridono]

426
00:18:07,819 --> 00:18:08,852
- Mmm, grazie.
- Grazie mille.

427
00:18:08,853 --> 00:18:10,521
- Quella relazione.
Sì.

428
00:18:10,522 --> 00:18:12,756
♪ ♪

429
00:18:12,757 --> 00:18:14,658
- Le persone potrebbero


430
00:18:14,659 --> 00:18:17,895
innamorarsi dell'intelligenza artificiale
ora, ma quando sarà un'intelligenza artificiale?

431
00:18:17,896 --> 00:18:21,265
essere in grado di
restituire sinceramente la sensazione?

432
00:18:21,266 --> 00:18:24,668
I futuristi stimano che
nei prossimi 20-30 anni

433
00:18:24,669 --> 00:18:27,804
ci sarà
un dilemma sui diritti dei computer.

434
00:18:27,805 --> 00:18:30,641
Raggiungeremo un punto in
cui non possiamo essere sicuri

435
00:18:30,642 --> 00:18:34,344
che un pezzo di tecnologia
non provi emozioni

436
00:18:34,345 --> 00:18:36,713
o non abbia consapevolezza di sé
o ambizioni

437
00:18:36,714 --> 00:18:38,582
o progetti
per il futuro.

438
00:18:38,583 --> 00:18:42,886
È illegale abusare di un animale,
ma un pezzo di tecnologia?

439
00:18:42,887 --> 00:18:45,255
Posso fare quello che voglio
per questo.

440
00:18:45,256 --> 00:18:49,726
Posso insultarlo,
molestarlo, graffiarlo...

441
00:18:49,727 --> 00:18:55,432
o peggio.

442
00:18:55,433 --> 00:18:57,935
Ops.

443
00:18:57,936 --> 00:19:00,938
Quando la tecnologia
diventerà così avanzata

444
00:19:00,939 --> 00:19:04,775
che quello che ho appena fatto è
considerato un omicidio?

445
00:19:04,776 --> 00:19:07,444
[musica drammatica]

446
00:19:07,445 --> 00:19:09,947
Potremmo non essere ancora arrivati,
ma siamo a un punto in

447
00:19:09,948 --> 00:19:14,718
cui non riusciamo a distinguere l'
umano dal chatbot?

448
00:19:14,719 --> 00:19:15,986
Bentornati a...

449
00:19:15,987 --> 00:19:18,455
tutti:
"Let's Get RomanTech."

450
00:19:18,456 --> 00:19:20,324
[applausi e applausi]
- L'unico game show che

451
00:19:20,325 --> 00:19:23,727
contrappone l'intelligenza umana all'intelligenza
artificiale.

452
00:19:23,728 --> 00:19:27,364
- Rose, è ora che
tu scelga la tua data RomanTech.

453
00:19:27,365 --> 00:19:30,701
- Qualcuno dei nostri soggetti
sceglierà il Bachelor numero due,

454
00:19:30,702 --> 00:19:32,836
altrimenti noto
come Cleverbot?

455
00:19:32,837 --> 00:19:34,438
[musica drammatica]

456
00:19:34,439 --> 00:19:37,507
- A volte nella vita scegli
la cosa peggiore per te

457
00:19:37,508 --> 00:19:39,243
solo perché
vuoi scoprirlo,

458
00:19:39,244 --> 00:19:41,745
quindi andiamo
con lo scapolo numero uno.

459
00:19:41,746 --> 00:19:42,980
[musica del game show]

460
00:19:42,981 --> 00:19:44,481
- Va bene, beh,
vediamolo.

461
00:19:44,482 --> 00:19:45,882
- Saluta Dana.

462
00:19:45,883 --> 00:19:47,584
- Ciao, Dana.  Oh.
- Ciao.

463
00:19:47,585 --> 00:19:49,353
- Considereremo questo round
come una vittoria

464
00:19:49,354 --> 00:19:50,887
per l'intelligenza umana.

465
00:19:50,888 --> 00:19:53,490
- Non hai scelto lo
scapolo numero due.

466
00:19:53,491 --> 00:19:54,825
Ora, perché?
- Destra.

467
00:19:54,826 --> 00:19:57,494
Penso di essere stato abbastanza spaventato
da essere curioso...

468
00:19:57,495 --> 00:19:59,663
- È stato strisciato fuori da...
- Ma non abbastanza.

469
00:19:59,664 --> 00:20:01,932
- Incontriamoci... esso.

470
00:20:01,933 --> 00:20:05,802
- Rose, lo scapolo numero due è
un chatbot completamente non umano

471
00:20:05,803 --> 00:20:07,404
che utilizza
l'intelligenza artificiale

472
00:20:07,405 --> 00:20:09,539
per sintetizzare
conversazioni simili a quelle umane.

473
00:20:09,540 --> 00:20:11,041
Incontra Cleverbot.

474
00:20:11,042 --> 00:20:14,011
- Sono entusiasta
di non aver scelto un computer,

475
00:20:14,012 --> 00:20:17,314
combinazione I-non so
cosa significherebbe su me stesso.

476
00:20:17,315 --> 00:20:19,416
Probabilmente avrei avuto
un infarto.

477
00:20:19,417 --> 00:20:22,052
- Quindi, Cleverbot è
zero per uno,

478
00:20:22,053 --> 00:20:24,588
ma ha ancora
tre possibilità in più.

479
00:20:24,589 --> 00:20:27,291
- Ora prenditi il tuo tempo, rimug
naci sopra

480
00:20:27,292 --> 00:20:29,826
  - Laurea numero uno, non
ricordo la maggior parte delle tue risposte

481
00:20:29,827 --> 00:20:31,061
, ecco perché--
- Wow.

482
00:20:31,062 --> 00:20:32,729
- Mi dispiace tanto.
Mi dispiace tanto.

483
00:20:32,730 --> 00:20:33,964
Quindi in realtà è
tra le due e le tre.

484
00:20:33,965 --> 00:20:35,565
Come è successo?

485
00:20:35,566 --> 00:20:36,633
[rullo di tamburi]
- Questa volta Cleverbot è

486
00:20:36,634 --> 00:20:37,668
in corsa.

487
00:20:37,669 --> 00:20:39,403
- Okay, um

488
00:20:39,404 --> 00:20:40,437
... sono uscito con qualcuno
come il numero due,

489
00:20:40,438 --> 00:20:41,938
quindi dovremmo semplicemente dire di no.

490
00:20:41,939 --> 00:20:44,808
Quindi andremo con lo
scapolo numero tre, credo.

491
00:20:44,809 --> 00:20:45,942
- Incontriamolo.

492
00:20:45,943 --> 00:20:47,411
- Dio mio!
[entrambi ridono]

493
00:20:47,412 --> 00:20:49,346
Ciao, come stai?
- Ciao.

494
00:20:49,347 --> 00:20:52,015
- Non hai scelto lo
scapolo numero due.

495
00:20:52,016 --> 00:20:54,551
- Laurea numero due,
tipo, cos'è successo?

496
00:20:54,552 --> 00:20:55,719
Non sapevo nemmeno
che fossi qui.

497
00:20:55,720 --> 00:20:57,587
Pensavo fossi
ubriaco da qualche parte.

498
00:20:57,588 --> 00:20:59,823
Questo è un pasticcio, solo un pasticcio!
[ridono entrambi]

499
00:20:59,824 --> 00:21:02,626
Completamente un--
[ridono entrambi]

500
00:21:02,627 --> 00:21:03,860
- Il celibe numero due è

501
00:21:03,861 --> 00:21:06,063
un chatbot completamente non umano
...

502
00:21:06,064 --> 00:21:07,497
[ridono entrambi]

503
00:21:07,498 --> 00:21:09,099
Che usa
l'intelligenza artificiale

504
00:21:09,100 --> 00:21:11,635
per sintetizzare
conversazioni simili a quelle umane.

505
00:21:11,636 --> 00:21:13,770
- Dio mio.
- Saluta Cleverbot.

506
00:21:13,771 --> 00:21:15,539
- Oh, Cleverbot,
sei il peggiore.

507
00:21:15,540 --> 00:21:18,075
[entrambi ridono]
- Ho quasi scelto Cleverbot!

508
00:21:18,076 --> 00:21:19,776
È terribile.

509
00:21:19,777 --> 00:21:22,446
- Sei uscito con qualcuno che era
un pasticcio come Cleverbot?

510
00:21:22,447 --> 00:21:23,647
- Non sta parlando bene
per lui.

511
00:21:23,648 --> 00:21:25,515
[risate]

512
00:21:25,516 --> 00:21:27,484
- Oh, spero che stia guardando.
- Sì.

513
00:21:27,485 --> 00:21:29,853
- Sembra che Cleverbot abbia superato
il test di Turing,

514
00:21:29,854 --> 00:21:31,855
ma non ha
vinto cuori.

515
00:21:31,856 --> 00:21:34,725
Tuttavia, ha
due possibilità rimaste.

516
00:21:34,726 --> 00:21:36,727
- Pensa alle risposte
che hai ottenuto.

517
00:21:36,728 --> 00:21:38,028
- Beh--[geme]

518
00:21:38,029 --> 00:21:40,364
Laurea numero uno,
non ho visto

519
00:21:40,365 --> 00:21:42,566
nulla di interessante
con le risposte,

520
00:21:42,567 --> 00:21:44,968
e Bachelor due
suona esilarante.

521
00:21:44,969 --> 00:21:48,138
La commedia sugli sguardi è
una cosa enorme per me.

522
00:21:48,139 --> 00:21:50,440
Sembra che
se andasse ad un appuntamento

523
00:21:50,441 --> 00:21:52,476
, sarebbe
almeno divertente.

524
00:21:52,477 --> 00:21:54,411
- Sai cosa?  Sei pronto
a darci la tua risposta?

525
00:21:54,412 --> 00:21:56,146
- [ride] Voglio dire,
penso di essere pronto, sì.

526
00:21:56,147 --> 00:21:59,883
Sono solo molto incuriosito
da... dallo scapolo due.

527
00:21:59,884 --> 00:22:00,984
[fanfara musicale]
- Va bene!

528
00:22:00,985 --> 00:22:03,053
- Laurea numero due.
- Bene.

529
00:22:03,054 --> 00:22:05,155
Scelta eccellente.
Come mai?

530
00:22:05,156 --> 00:22:07,557
- Sono incuriosito.
Amo l'umorismo.

531
00:22:07,558 --> 00:22:10,427
Le risposte erano semplicemente divertenti.
Voglio dire, giocoso.

532
00:22:10,428 --> 00:22:15,098
Questa persona è misteriosa,
come un essere umano perfettamente funzionante,

533
00:22:15,099 --> 00:22:17,768
giusto, perché ha
braccia, gambe e roba del genere.

534
00:22:17,769 --> 00:22:20,404
- Incontriamoci... esso.

535
00:22:20,405 --> 00:22:22,439
- Eh?
- Bachelor numero due

536
00:22:22,440 --> 00:22:24,775
è un chatbot completamente non umano


537
00:22:24,776 --> 00:22:26,176
che utilizza
l'intelligenza artificiale

538
00:22:26,177 --> 00:22:28,545
per sintetizzare
conversazioni simili a quelle umane.

539
00:22:28,546 --> 00:22:30,514
- Bene.
- Saluta Cleverbot.

540
00:22:30,515 --> 00:22:32,149
- Tipo,
rispondeva seriamente?

541
00:22:32,150 --> 00:22:33,717
Il robot stava rispondendo al...
- Sì.

542
00:22:33,718 --> 00:22:35,185
- Seriamente alla lettera.

543
00:22:35,186 --> 00:22:37,120
È una rete neurale profonda
che apprende

544
00:22:37,121 --> 00:22:38,789
e può sintetizzare il linguaggio umano.
- Sì.

545
00:22:38,790 --> 00:22:40,657
- Quindi il mio nuovo tipo
è un robot?

546
00:22:40,658 --> 00:22:43,193
Voglio dire, le cose stanno cambiando
in questo mondo, giusto?

547
00:22:43,194 --> 00:22:45,195
entrambi: Sì.
- Questo

548
00:22:45,196 --> 00:22:47,631
non sarà proprio uno scherzo
in futuro.

549
00:22:47,632 --> 00:22:50,667
- È spaventoso, in
realtà.

550
00:22:50,668 --> 00:22:53,003
- Il futuro dell'A.I.
potrebbe essere spaventoso per alcuni,

551
00:22:53,004 --> 00:22:55,672
ma anche così,
questo soggetto non è stato

552
00:22:55,673 --> 00:22:58,208
l'unico
a scegliere il computer.

553
00:22:58,209 --> 00:23:00,811
- Laurea numero due,
scelgo te.

554
00:23:00,812 --> 00:23:02,879
- Oh!
Ok, celibe numero due.

555
00:23:02,880 --> 00:23:05,148
- Penso che potrebbe essere
lo strano che sto cercando.

556
00:23:05,149 --> 00:23:07,150
- Cleverbot è riuscito
a conquistare il cuore

557
00:23:07,151 --> 00:23:10,787
di due addio al nubilato,
superando sia il nostro test di Turing che il nostro test di

558
00:23:10,788 --> 00:23:13,690
"data-abilità".

559
00:23:13,691 --> 00:23:15,058
- Questo conclude...
[entrambi ridono]

560
00:23:15,059 --> 00:23:17,694
"Prendiamoci...
entrambi: "RomanTech".

561
00:23:17,695 --> 00:23:18,895
- Va bene.

562
00:23:18,896 --> 00:23:25,802
[applausi e applausi]

563
00:23:25,803 --> 00:23:29,506
- Forse un giorno i computer avranno
diritti come gli umani.

564
00:23:29,507 --> 00:23:32,209
Forse non sapremo mai
cosa rende umani  menti

565
00:23:32,210 --> 00:23:34,711
diverse
da quelle elettroniche.

566
00:23:34,712 --> 00:23:36,179
Forse la domanda non è

567
00:23:36,180 --> 00:23:38,949
"Possiamo avere rapporti
con la tecnologia",

568
00:23:38,950 --> 00:23:41,585
ma piuttosto
"Siamo la stessa cosa?"

569
00:23:41,586 --> 00:23:45,989
Voglio dire, immagina un alieno che
non ha idea del corpo umano che

570
00:23:45,990 --> 00:23:48,658
mi vede
per  la prima volta.

571
00:23:48,659 --> 00:23:50,093
Capirebbe
il confine

572
00:23:50,094 --> 00:23:53,663
tra l'organismo
e l'invenzione?

573
00:23:53,664 --> 00:23:57,200
Saprebbe che questi sono stati
realizzati per me da altri umani,

574
00:23:57,201 --> 00:24:00,170
o penserebbe
che crescono da me?

575
00:24:00,171 --> 00:24:03,039
Penserebbe
che il mio telefono o il mio computer

576
00:24:03,040 --> 00:24:08,211
sono dispositivi o
organi metallici esterni che mi sono evoluto? Tra

577
00:24:08,212 --> 00:24:12,816
anni, i computer
raggiungeranno la personalità

578
00:24:12,817 --> 00:24:18,555
o stiamo tutti raggiungendo collettivamente la
"cyborg"?

579
00:24:18,556 --> 00:24:21,558
E come sempre,
grazie per aver guardato.

580
00:24:21,559 --> 00:24:24,027
[musica drammatica]

581
00:24:24,028 --> 00:24:27,030
[musica elettronica]

582
00:24:27,031 --> 00:24:33,972
♪ ♪


1
00:00:05,171 --> 00:00:06,872
¿Lectura de mentes?

2
00:00:06,873 --> 00:00:07,973
Claro que no.

3
00:00:07,974 --> 00:00:10,876
Me encanta leer.

4
00:00:10,877 --> 00:00:15,247
La adivinación puede parecer pseudociencia

5
00:00:15,248 --> 00:00:16,582
o, si me permiten decirlo,

6
00:00:16,583 --> 00:00:18,050
puras mentiras,

7
00:00:18,051 --> 00:00:21,387
pero su equivalente científico,
la identificación del pensamiento,

8
00:00:21,388 --> 00:00:23,655
es real.

9
00:00:23,656 --> 00:00:26,658
Se basa en la neuroimagenología
y el aprendizaje automatizado,

10
00:00:26,659 --> 00:00:29,928
y lo mejor es que los experimentos

11
00:00:29,929 --> 00:00:33,399
no solo buscan ver
lo que alguien está pensando,

12
00:00:33,400 --> 00:00:37,403
sino averiguar
de qué están hechos los pensamientos.

13
00:00:37,404 --> 00:00:39,238
Cuando pensamos en algo,

14
00:00:39,239 --> 00:00:42,841
¿cómo es esa imagen mental?

15
00:00:42,842 --> 00:00:44,376
¿Qué resolución tiene?

16
00:00:44,377 --> 00:00:46,712
¿Los recuerdos son de alta fidelidad?

17
00:00:46,713 --> 00:00:49,081
¿Y cómo cambian con el tiempo?

18
00:00:49,082 --> 00:00:50,282
En este episodio,

19
00:00:50,283 --> 00:00:52,451
averiguaremos cómo leer la mente

20
00:00:52,452 --> 00:00:54,553
ayuda a responder esas preguntas.

21
00:00:54,554 --> 00:00:58,257
Mi viaje comienza aquí,
en la Universidad de Oregon.

22
00:00:58,258 --> 00:01:01,060
Veré al Dr. Brice Kuhl, del Kuhl Lab.

23
00:01:01,061 --> 00:01:03,462
Es un neurocientífico
que usa la neuroimagen

24
00:01:03,463 --> 00:01:06,665
y el aprendizaje automático
para ver qué piensa alguien

25
00:01:06,666 --> 00:01:23,248
sin que se lo diga.

26
00:01:23,249 --> 00:01:27,953
LECTURA DEL PENSAMIENTO

27
00:01:27,954 --> 00:01:30,823
UNIVERSIDAD DE OREGON

28
00:01:30,824 --> 00:01:33,492
Cuéntame qué haces.

29
00:01:33,493 --> 00:01:34,760
BRICE KUHL
PROFESOR DE PSICOLOGÍA

30
00:01:34,761 --> 00:01:37,062
Soy parte del programa
de neurociencia cognitiva

31
00:01:37,063 --> 00:01:38,297
y estudio la memoria.

32
00:01:38,298 --> 00:01:40,766
En mi laboratorio usamos
métodos de neuroimagen,

33
00:01:40,767 --> 00:01:42,568
así que trabajamos mucho

34
00:01:42,569 --> 00:01:44,570
con resonancias magnéticas funcionales,

35
00:01:44,571 --> 00:01:45,671
o IRMf.

36
00:01:45,672 --> 00:01:49,508
¿Y cómo usan las IRMf
para investigar los recuerdos?

37
00:01:49,509 --> 00:01:51,610
Analizamos los mapas
de actividad neuronal.

38
00:01:51,611 --> 00:01:54,246
Al crear un recuerdo, hay un mapa,

39
00:01:54,247 --> 00:01:56,548
y podemos grabarlo

40
00:01:56,549 --> 00:01:59,618
para comprobar si se vuelve a emplear

41
00:01:59,619 --> 00:02:02,254
o se reactiva al recordar.

42
00:02:02,255 --> 00:02:06,158
¿Es decir que a partir
de los mapas de actividad cerebral,

43
00:02:06,159 --> 00:02:09,794
podemos deducir qué está recordando

44
00:02:09,795 --> 00:02:10,996
o pensando alguien?

45
00:02:10,997 --> 00:02:13,666
Sí, lo llamamos decodificación.

46
00:02:13,667 --> 00:02:15,401
Así que básicamente toman…

47
00:02:15,402 --> 00:02:19,038
un mapa modelo,
tu mapa de actividad que grabamos

48
00:02:19,039 --> 00:02:20,773
mientras recuerdas algo.

49
00:02:20,774 --> 00:02:23,575
Y hacemos una predicción
de qué estas recordando.

50
00:02:23,576 --> 00:02:26,278
Suena a lectura del pensamiento.

51
00:02:26,279 --> 00:02:28,947
Sí. Eso parece.

52
00:02:28,948 --> 00:02:32,484
¿Qué me vas a hacer hoy?

53
00:02:32,485 --> 00:02:35,988
Haremos algo que no hicimos nunca.

54
00:02:35,989 --> 00:02:38,290
Vamos a probar una nueva variante

55
00:02:38,291 --> 00:02:41,894
del experimento contigo.
No garantizo ningún resultado.

56
00:02:41,895 --> 00:02:46,231
Pero representa lo que hemos logrado
y adónde queremos llegar.

57
00:02:46,232 --> 00:02:48,400
Hoy participarás de un experimento

58
00:02:48,401 --> 00:02:50,235
en el que estudiarás rostros.

59
00:02:50,236 --> 00:02:53,839
Te haremos estudiar 12 fotos de famosos.

60
00:02:53,840 --> 00:02:55,107
Gente que conozco.

61
00:02:55,108 --> 00:02:56,975
- Gente que conoces.
- Bien.

62
00:02:56,976 --> 00:02:58,877
Intentarás recordar las fotos.

63
00:02:58,878 --> 00:03:00,946
Luego te haremos una IRM.

64
00:03:00,947 --> 00:03:03,849
Intentarás recordar las fotos
lo mejor que puedas.

65
00:03:03,850 --> 00:03:06,051
Y grabaremos tu actividad cerebral

66
00:03:06,052 --> 00:03:08,253
mientras tratas de imaginarlas.

67
00:03:08,254 --> 00:03:10,089
Intentaremos reconstruirlas.

68
00:03:10,090 --> 00:03:12,257
Es como tomar una foto de tu recuerdo.

69
00:03:12,258 --> 00:03:13,992
- ¿Una foto?
- Una foto.

70
00:03:13,993 --> 00:03:18,030
¿Una foto que podemos imprimir
y colgar en la pared?

71
00:03:18,031 --> 00:03:20,232
Si quieres.

72
00:03:20,233 --> 00:03:25,270
Primero debí memorizar
las 12 fotos de los famosos

73
00:03:25,271 --> 00:03:28,674
que Brice tratará de detectar
en mi pensamiento.

74
00:03:28,675 --> 00:03:31,543
Me ayudó Max, un estudiante de posgrado.

75
00:03:31,544 --> 00:03:33,245
MAX DRASCHER
ESTUDIANTE DE DOCTORADO EN PSICOLOGÍA

76
00:03:33,246 --> 00:03:37,716
El éxito de la predicción en parte depende
de que recuerde los rostros

77
00:03:37,717 --> 00:03:42,588
detalladamente mientras me haga
la resonancia magnética.

78
00:03:42,589 --> 00:03:45,591
Bien, entonces…

79
00:03:45,592 --> 00:03:51,063
Creo que los recuerdo bastante bien.

80
00:03:51,064 --> 00:03:53,699
- Genial.
- Siento que hay mucho en juego.

81
00:03:53,700 --> 00:03:58,203
Después de memorizar los rostros,
es hora del siguiente paso:

82
00:03:58,204 --> 00:04:01,340
pasar por el detector de metales
y hacer la IRMf

83
00:04:01,341 --> 00:04:04,143
para que Brice grabe
y monitoree mi actividad cerebral

84
00:04:04,144 --> 00:04:07,913
y luego la introduzca
al algoritmo que reconstruye los rostros.

85
00:04:07,914 --> 00:04:12,084
Es la primera vez que intenta reconstruir
un rostro a partir de un recuerdo,

86
00:04:12,085 --> 00:04:14,153
y es muy difícil porque dependemos

87
00:04:14,154 --> 00:04:17,155
de la claridad con la que yo recuerde
las fotos de los famosos

88
00:04:17,156 --> 00:04:19,058
que vi hace una hora.

89
00:04:19,059 --> 00:04:20,792
Me encantan los ojos.

90
00:04:20,793 --> 00:04:22,661
Sí, es para los niños.

91
00:04:22,662 --> 00:04:24,396
Vienen muchos.

92
00:04:24,397 --> 00:04:26,932
¿Y no piensan que los va a comer?

93
00:04:26,933 --> 00:04:31,236
DEMOSTRACIÓN
RECONSTRUCCIÓN DE LOS RECUERDOS

94
00:04:31,237 --> 00:04:34,239
Para monitorear
la actividad del cerebro, la IRMf

95
00:04:34,240 --> 00:04:36,975
lo divide en miles de pequeños cubos

96
00:04:36,976 --> 00:04:39,945
llamados vóxeles o píxeles volumétricos.

97
00:04:39,946 --> 00:04:43,749
Cada uno contiene
cientos de miles de neuronas.

98
00:04:43,750 --> 00:04:47,753
La IRMf permite detectar
el flujo sanguíneo dentro de esos vóxeles

99
00:04:47,754 --> 00:04:50,089
y ver qué parte del cerebro está activa.

100
00:04:50,090 --> 00:04:53,125
Si veo varios rostros
de hombres con bigote,

101
00:04:53,126 --> 00:04:56,428
mi cerebro reacciona
a las características de cada uno.

102
00:04:56,429 --> 00:04:59,965
Pero un área de mi cerebro
estará activada todo el tiempo.

103
00:04:59,966 --> 00:05:03,936
Esa puede ser el área de mi cerebro
que reacciona a los bigotes.

104
00:05:03,937 --> 00:05:09,341
Luego, al imaginar un rostro,
si Brice nota que esa área está activada,

105
00:05:09,342 --> 00:05:13,779
puede predecir
que estoy pensando en un bigote.

106
00:05:13,780 --> 00:05:15,814
Michael está en el escáner.

107
00:05:15,815 --> 00:05:18,484
En la pantalla aparecen nombres

108
00:05:18,485 --> 00:05:20,819
y él está tratando
de visualizar los rostros,

109
00:05:20,820 --> 00:05:23,255
recordar las caras detalladamente.

110
00:05:23,256 --> 00:05:25,691
Estas son las imágenes.

111
00:05:25,692 --> 00:05:29,028
Un volumen del cerebro cada dos segundos.

112
00:05:29,029 --> 00:05:33,666
Se actualizan en tiempo real
a medida que las recopilamos.

113
00:05:33,667 --> 00:05:36,101
Terminada la primera parte,

114
00:05:36,102 --> 00:05:38,971
pasamos a la segunda,
en la que Brice y su equipo

115
00:05:38,972 --> 00:05:41,674
aprenderán el idioma
de mi actividad cerebral

116
00:05:41,675 --> 00:05:44,843
para poder decodificar mis escaneos.

117
00:05:44,844 --> 00:05:46,545
Michael, ¿estás bien?

118
00:05:46,546 --> 00:05:48,113
Sí.

119
00:05:48,114 --> 00:05:52,718
Me mostrarán cientos de caras únicas
y grabarán cómo reacciona mi cerebro

120
00:05:52,719 --> 00:05:55,020
ante ciertas características faciales.

121
00:05:55,021 --> 00:05:59,491
Luego usarán esa información
para reconstruir las caras de los famosos

122
00:05:59,492 --> 00:06:03,328
en los que pensé
en la primera parte de la resonancia.

123
00:06:03,329 --> 00:06:06,198
Cuantos más rostros
podamos mostrarle, mejor.

124
00:06:06,199 --> 00:06:09,802
Así que estará ahí dentro
mientras se sienta cómodo.

125
00:06:09,803 --> 00:06:13,372
Dos horas fue el tiempo máximo
que nos dieron para la IRMf.

126
00:06:13,373 --> 00:06:18,310
Pero pude mirar más de 400 rostros,
y deberían ser suficientes

127
00:06:18,311 --> 00:06:21,080
para tener resultados interesantes.

128
00:06:21,081 --> 00:06:22,681
Michael, lo lograste. Muy bien.

129
00:06:22,682 --> 00:06:26,652
- Vamos a sacarte.
- Bien.

130
00:06:26,653 --> 00:06:28,821
Vaya.

131
00:06:28,822 --> 00:06:31,390
Vi muchos rostros hoy.

132
00:06:31,391 --> 00:06:32,858
Cielos.

133
00:06:32,859 --> 00:06:36,328
Estas son algunas de las imágenes
de cuando estabas ahí dentro.

134
00:06:36,329 --> 00:06:39,965
Imágenes de tu cerebro.
Ahora analizaremos el resultado.

135
00:06:39,966 --> 00:06:42,034
Max analizará tus datos.

136
00:06:42,035 --> 00:06:45,270
Nos veremos mañana para ver el resultado

137
00:06:45,271 --> 00:06:47,973
y tratar de recrear los rostros

138
00:06:47,974 --> 00:06:51,377
- a partir de los datos que reunimos.
- Nos vemos mañana.

139
00:06:51,378 --> 00:06:54,646
- Muchas gracias.
- Gracias, Max. Estoy ansioso.

140
00:06:54,647 --> 00:07:04,356
Trabajen toda la noche.
Quiero datos perfectos.

141
00:07:04,357 --> 00:07:06,892
Volví al laboratorio del Dr. Kuhl.

142
00:07:06,893 --> 00:07:08,994
Anoche analizaron los datos,

143
00:07:08,995 --> 00:07:15,701
y estoy ansioso por saber
qué me vieron pensar.

144
00:07:15,702 --> 00:07:17,536
¿Cómo me fue?

145
00:07:17,537 --> 00:07:19,238
Creo que bien.

146
00:07:19,239 --> 00:07:20,773
Lo veremos en un momento.

147
00:07:20,774 --> 00:07:22,341
Bien, estoy ansioso.

148
00:07:22,342 --> 00:07:24,243
- ¿Me siento?
- Sí, siéntate.

149
00:07:24,244 --> 00:07:26,645
Bien…

150
00:07:26,646 --> 00:07:30,349
Primero que nada, ¿qué es esto? Bien.

151
00:07:30,350 --> 00:07:32,818
Son las imágenes que memoricé.

152
00:07:32,819 --> 00:07:34,787
- Así es.
- Y estas

153
00:07:34,788 --> 00:07:38,023
las que reconstruyeron
a partir de mi imaginación.

154
00:07:38,024 --> 00:07:40,726
- Así es.
- Bien.

155
00:07:40,727 --> 00:07:44,530
Esta es una
de las reconstrucciones que hicimos.

156
00:07:44,531 --> 00:07:45,931
Qué interesante.

157
00:07:45,932 --> 00:07:47,966
Es John Cho.

158
00:07:47,967 --> 00:07:50,402
Nada mal.

159
00:07:50,403 --> 00:07:53,872
- ¿Podemos verlas lado a lado?
- Sí.

160
00:07:53,873 --> 00:07:59,678
Veo cierta similitud en sus expresiones.

161
00:07:59,679 --> 00:08:02,414
El nacimiento del pelo casi coincide.

162
00:08:02,415 --> 00:08:04,983
La forma de la cara también me pareció…

163
00:08:04,984 --> 00:08:06,985
Su cara es más bien cuadrada.

164
00:08:06,986 --> 00:08:08,320
- Sí.
- Eso fue

165
00:08:08,321 --> 00:08:09,421
lo que yo noté.

166
00:08:09,422 --> 00:08:12,791
Cuando estaba visualizando
esta imagen de John Cho,

167
00:08:12,792 --> 00:08:16,495
el rostro cuadrado
fue lo más sobresaliente.

168
00:08:16,496 --> 00:08:20,065
- Qué interesante.
- Recordaba eso.

169
00:08:20,066 --> 00:08:23,335
Excelente.

170
00:08:23,336 --> 00:08:28,307
Esa es Megan Fox.

171
00:08:28,308 --> 00:08:30,142
Y veremos la comparación.

172
00:08:30,143 --> 00:08:31,844
La comparación. Sí.

173
00:08:31,845 --> 00:08:36,682
Esta es la foto que viste,
y esta la recreación.

174
00:08:36,683 --> 00:08:39,817
Confieso que, con Megan Fox, no logré

175
00:08:39,818 --> 00:08:42,021
tener una imagen mental clara.

176
00:08:42,022 --> 00:08:45,057
Por alguna razón,
esta imagen me resultó difícil

177
00:08:45,058 --> 00:08:46,992
de recordar.

178
00:08:46,993 --> 00:08:50,963
Sí noté la severidad de su rostro.

179
00:08:50,964 --> 00:08:54,066
Noté que había… Parece femenina.

180
00:08:54,067 --> 00:08:58,804
Y captaste la severidad.
En conjunto, es una coincidencia.

181
00:08:58,805 --> 00:09:02,474
Recuerden que Brice y su equipo
sacaron estas imágenes de mis recuerdos.

182
00:09:02,475 --> 00:09:07,112
Pero, cuando recuerdo un rostro,
¿imagino todos los detalles a la vez,

183
00:09:07,113 --> 00:09:08,747
con precisión fotográfica?

184
00:09:08,748 --> 00:09:11,016
¿O me concentro en uno a la vez?

185
00:09:11,017 --> 00:09:16,121
Al leer mi mente, quizás vean
cuán mala es mi memoria y cómo funciona.

186
00:09:16,122 --> 00:09:19,024
¡Yo!

187
00:09:19,025 --> 00:09:25,330
Esa es la reconstrucción
de cuando pensé en esta imagen mía.

188
00:09:25,331 --> 00:09:27,499
Así es.

189
00:09:27,500 --> 00:09:29,435
¿Y la barba?

190
00:09:29,436 --> 00:09:32,137
No sé. Dímelo tú.

191
00:09:32,138 --> 00:09:36,542
Por ejemplo, esta es la imagen
de cuando recordé mi propio rostro.

192
00:09:36,543 --> 00:09:39,244
No se parece en nada a mí,
pero la pregunta es:

193
00:09:39,245 --> 00:09:41,613
"¿Cómo me imagino?".

194
00:09:41,614 --> 00:09:45,784
No pienso en mi rostro muy seguido,
así que la rareza del resultado

195
00:09:45,785 --> 00:09:48,020
puede ser por defectos de mi memoria

196
00:09:48,021 --> 00:09:51,824
y autoconcepción,
no solo defectos de la tecnología.

197
00:09:51,825 --> 00:09:54,460
Esa creo que es Jennifer Lawrence.

198
00:09:54,461 --> 00:09:56,095
¿Esa es Jennifer Lawrence?

199
00:09:56,096 --> 00:10:01,800
Parece un tío viejo de Jennifer Lawrence.

200
00:10:01,801 --> 00:10:05,371
Ninguna se acercó mucho.

201
00:10:05,372 --> 00:10:09,208
Pero es algo
que están empezando a sondear,

202
00:10:09,209 --> 00:10:12,678
la memoria a largo plazo.

203
00:10:12,679 --> 00:10:15,214
La lectura de Brice y su equipo

204
00:10:15,215 --> 00:10:18,217
podría haber sido más precisa
si me hubieran mostrado miles

205
00:10:18,218 --> 00:10:22,855
de imágenes en vez de cientos de ellas,
porque el algoritmo habría aprendido

206
00:10:22,856 --> 00:10:25,057
mejor el lenguaje de mi cerebro.

207
00:10:25,058 --> 00:10:29,228
Pero la calidad de mis recuerdos
igual habría sido un problema.

208
00:10:29,229 --> 00:10:33,298
Miren qué pasa
si dejamos de lado la memoria.

209
00:10:33,299 --> 00:10:37,302
Brice analizó además mi actividad cerebral
cuando estaba mirando los rostros,

210
00:10:37,303 --> 00:10:39,238
no solo cuando los imaginaba.

211
00:10:39,239 --> 00:10:45,144
Esos resultados son mucho más parecidos
que los recreados a partir de mi memoria.

212
00:10:45,145 --> 00:10:47,546
¿Qué estoy viendo?

213
00:10:47,547 --> 00:10:52,284
Las imágenes de la primera fila
son las que viste

214
00:10:52,285 --> 00:10:53,919
dentro del escáner.

215
00:10:53,920 --> 00:10:57,022
Debajo, están las reconstrucciones

216
00:10:57,023 --> 00:10:59,658
que hicimos en base a tus mapas
de actividad cerebral.

217
00:10:59,659 --> 00:11:02,027
FILA SUPERIOR: IMÁGENES FUENTE
FILA INFERIOR: RECONSTRUCCIONES

218
00:11:02,028 --> 00:11:03,796
- Esto es de la imagen fuente.
- Sí.

219
00:11:03,797 --> 00:11:05,464
Estas son las de mi cerebro.

220
00:11:05,465 --> 00:11:07,566
- Sí.
- Son bastante parecidas.

221
00:11:07,567 --> 00:11:10,102
Sí, en general son parecidas.

222
00:11:10,103 --> 00:11:11,704
No son perfectas.

223
00:11:11,705 --> 00:11:14,239
Se ve que hay variaciones en estas.

224
00:11:14,240 --> 00:11:17,076
Pero concuerda
con lo que observamos antes.

225
00:11:17,077 --> 00:11:20,412
Las reconstrucciones generadas
mientras ves los rostros

226
00:11:20,413 --> 00:11:22,481
corresponden al rostro real.

227
00:11:22,482 --> 00:11:26,552
Es una prueba
de que podemos recrear las imágenes

228
00:11:26,553 --> 00:11:28,954
- cuando las estás viendo.
- Claro.

229
00:11:28,955 --> 00:11:31,590
Están bastante bien.

230
00:11:31,591 --> 00:11:35,361
Brice, Max, muchas gracias
por dejarme participar.

231
00:11:35,362 --> 00:11:36,729
Espero haber sido útil.

232
00:11:36,730 --> 00:11:38,530
Gracias. Fue muy divertido.

233
00:11:38,531 --> 00:11:43,836
Siempre nos sirve pensar en estas cosas.

234
00:11:43,837 --> 00:11:47,740
KIOTO, JAPÓN

235
00:11:47,741 --> 00:11:51,443
La investigación del Dr. Brice Kuhl
muestra que es posible

236
00:11:51,444 --> 00:11:54,279
que una computadora
lea la mente de alguien,

237
00:11:54,280 --> 00:11:56,915
que pueda deducir en qué está pensando.

238
00:11:56,916 --> 00:11:59,051
Pero aún queda mucho camino por delante.

239
00:11:59,052 --> 00:12:02,221
Si quieren saber
qué estoy pensando en este momento,

240
00:12:02,222 --> 00:12:05,524
aún es mucho más fácil preguntarme.

241
00:12:05,525 --> 00:12:08,327
Pero ¿y si no puedo decirlo?

242
00:12:08,328 --> 00:12:11,363
El Dr. Yukiyasu Kamitani
es un investigador,

243
00:12:11,364 --> 00:12:15,100
profesor y pionero de la exploración

244
00:12:15,101 --> 00:12:18,237
del sueño.

245
00:12:18,238 --> 00:12:20,472
Vine a la Universidad de Kioto

246
00:12:20,473 --> 00:12:24,677
a verlo para saber cómo es leer,
no los pensamientos,

247
00:12:24,678 --> 00:12:27,513
sino los sueños.

248
00:12:27,514 --> 00:12:29,548
LABORATORIO SCIENCE FRONTIER

249
00:12:29,549 --> 00:12:31,650
Kamitani sensei, soy Michael.

250
00:12:31,651 --> 00:12:33,385
- Hola. Yuki.
- Yuki. Encantado.

251
00:12:33,386 --> 00:12:34,753
Igualmente.

252
00:12:34,754 --> 00:12:36,355
Durante los últimos diez años,

253
00:12:36,356 --> 00:12:40,225
el Dr. Kamitani ha estado en la vanguardia
de la lectura del cerebro.

254
00:12:40,226 --> 00:12:44,396
El sujeto de estudio
está listo para entrar.

255
00:12:44,397 --> 00:12:46,098
Igual que Brice Kuhl,

256
00:12:46,099 --> 00:12:49,201
en un principio reconstruía imágenes

257
00:12:49,202 --> 00:12:52,905
que les mostraba a los sujetos en una IRMf
en base a su actividad cerebral.

258
00:12:52,906 --> 00:12:55,974
Kamitani usaba formas en blanco y negro,

259
00:12:55,975 --> 00:12:59,578
y las reconstrucciones eran
sorprendentemente precisas.

260
00:12:59,579 --> 00:13:03,382
Recientemente, se concentró
en el uso de redes neuronales profundas

261
00:13:03,383 --> 00:13:06,485
y aprendizaje automático
para descifrar la actividad cerebral

262
00:13:06,486 --> 00:13:09,955
mientras ven fotografías
mucho más complejas.

263
00:13:09,956 --> 00:13:13,726
Lo que están viendo es
una red neuronal profunda

264
00:13:13,727 --> 00:13:18,097
procesando la actividad cerebral
del sujeto que mira la fotografía.

265
00:13:18,098 --> 00:13:20,933
Podría tener
un sinnúmero de usos en el futuro,

266
00:13:20,934 --> 00:13:26,939
como en investigaciones policiales
y en la comunicación interpersonal.

267
00:13:26,940 --> 00:13:28,941
No es perfecto.

268
00:13:28,942 --> 00:13:31,944
Pero se ven los ojos y las orejas.

269
00:13:31,945 --> 00:13:33,812
DR. YUKIYASU KAMITANI
PROF., DEPTO. DE GRADUADOS EN INFORMÁTICA

270
00:13:33,813 --> 00:13:35,414
Sí.

271
00:13:35,415 --> 00:13:37,116
Y los colores también.

272
00:13:37,117 --> 00:13:39,952
Sí, hasta cierto punto.

273
00:13:39,953 --> 00:13:43,088
Su trabajo más reciente es
sobre el subconsciente.

274
00:13:43,089 --> 00:13:45,591
Intenta lograr algo muy ambicioso:

275
00:13:45,592 --> 00:13:47,526
grabar nuestros sueños.

276
00:13:47,527 --> 00:13:52,398
¿Te considerarías un investigador
de sueños o de visiones?

277
00:13:52,399 --> 00:13:54,266
Quizá decodificador de cerebros.

278
00:13:54,267 --> 00:13:56,001
Decodificador de cerebros.

279
00:13:56,002 --> 00:13:58,303
Qué buena descripción del trabajo.

280
00:13:58,304 --> 00:14:02,374
¿Me puedes mostrar algo
de tu trabajo con los sueños?

281
00:14:02,375 --> 00:14:03,942
Sí.

282
00:14:03,943 --> 00:14:08,681
Hacemos una grabación simultánea
con EEG e IRM.

283
00:14:08,682 --> 00:14:13,485
La decodificación de los sueños empieza
de manera similar que la del Dr. Kuhl:

284
00:14:13,486 --> 00:14:17,389
mostrándole al sujeto
miles de imágenes en una IRMf

285
00:14:17,390 --> 00:14:22,094
para ver cómo se ve el cerebro
cuando piensa en ciertas cosas.

286
00:14:22,095 --> 00:14:24,663
Una vez que el algoritmo
de aprendizaje automático

287
00:14:24,664 --> 00:14:27,566
puede identificar en qué imágenes
está pensando el sujeto,

288
00:14:27,567 --> 00:14:31,837
lo ponen en una IRMf con un gorro para EEG

289
00:14:31,838 --> 00:14:33,972
y le piden que duerma.

290
00:14:33,973 --> 00:14:37,076
Cuando las ondas de EEG indican
que está soñando,

291
00:14:37,077 --> 00:14:42,247
el algoritmo predice
con qué tipo de cosas sueña.

292
00:14:42,248 --> 00:14:45,851
Hasta el momento,
el algoritmo evalúa 20 categorías.

293
00:14:45,852 --> 00:14:51,423
Edificios, medios de transporte
y los caracteres de un idioma.

294
00:14:51,424 --> 00:14:55,828
Entonces despiertan al sujeto,
le preguntan qué estaba soñando

295
00:14:55,829 --> 00:15:00,299
y se fijan si coinciden
la predicción del algoritmo y el recuerdo.

296
00:15:00,300 --> 00:15:03,836
Estos son datos reales
de un experimento de Kamitani.

297
00:15:03,837 --> 00:15:06,305
En la parte inferior hay categorías.

298
00:15:06,306 --> 00:15:08,841
Cada una se agranda o se achica

299
00:15:08,842 --> 00:15:11,143
en tiempo real según las probabilidades

300
00:15:11,144 --> 00:15:13,846
de que estén en el sueño del sujeto.

301
00:15:13,847 --> 00:15:15,848
Como pueden ver, la más fuerte

302
00:15:15,849 --> 00:15:18,984
es la categoría "carácter",
es decir, el lenguaje escrito.

303
00:15:18,985 --> 00:15:23,822
En ese momento, despertaron al sujeto
y esto fue lo que dijo:

304
00:15:23,823 --> 00:15:26,692
"Estaba mirando unos caracteres.

305
00:15:26,693 --> 00:15:30,729
"Era como un formulario
para escribir un ensayo…".

306
00:15:30,730 --> 00:15:32,998
- Es espeluznante.
- Sí.

307
00:15:32,999 --> 00:15:37,102
¿No? Espiaron su sueño.

308
00:15:37,103 --> 00:15:39,938
Sí, podría decirse.

309
00:15:39,939 --> 00:15:42,174
Pero no es muy preciso…

310
00:15:42,175 --> 00:15:44,710
No lo es,

311
00:15:44,711 --> 00:15:47,579
pero mi grado de precisión sería cero.

312
00:15:47,580 --> 00:15:48,947
Sí.

313
00:15:48,948 --> 00:15:52,518
Mientras intenta predecir
el contenido de los sueños,

314
00:15:52,519 --> 00:15:55,554
el Dr. Kamitani está iniciando
un nuevo proyecto:

315
00:15:55,555 --> 00:15:59,224
reconstruir imágenes
a partir de nuestros sueños.

316
00:15:59,225 --> 00:16:05,831
Así que trajiste algunas
reconstrucciones de sueños.

317
00:16:05,832 --> 00:16:08,467
Aún estamos trabajando en esto.

318
00:16:08,468 --> 00:16:11,337
Hay una tendencia

319
00:16:11,338 --> 00:16:13,972
a que todo lo que decodificamos

320
00:16:13,973 --> 00:16:18,344
tenga una mancha en el centro.

321
00:16:18,345 --> 00:16:20,779
Sí, parecen todos sueños con manchas.

322
00:16:20,780 --> 00:16:22,014
Sí.

323
00:16:22,015 --> 00:16:25,217
Pero consideremos

324
00:16:25,218 --> 00:16:28,654
y valoraremos
que lo que vemos en esta pantalla

325
00:16:28,655 --> 00:16:34,793
son, en cierta formas,
las primeras fotografías de sueños.

326
00:16:34,794 --> 00:16:38,497
Esta es la primera etapa
de una investigación revolucionaria.

327
00:16:38,498 --> 00:16:43,535
Un día, tal vez podamos tener imágenes
o grabar películas de nuestros sueños.

328
00:16:43,536 --> 00:16:47,539
Y el Dr. Kamitani es el único en el mundo
que está investigándolo.

329
00:16:47,540 --> 00:16:51,643
Es un explorador solitario
del subconsciente.

330
00:16:51,644 --> 00:16:54,546
¿Esto aún no se publicó?

331
00:16:54,547 --> 00:16:56,015
No.

332
00:16:56,016 --> 00:17:02,888
Gracias por mostrármelo.

333
00:17:02,889 --> 00:17:06,991
Las percepciones que los investigadores
como el Dr. Kuhl y el Dr. Kamitani

334
00:17:06,992 --> 00:17:10,162
pueden llegar a lograr en el futuro

335
00:17:10,163 --> 00:17:15,034
gracias a la lectura del cerebro
son difíciles de imaginar.

336
00:17:15,035 --> 00:17:18,337
Pero esperen,
porque hablamos de tecnología

337
00:17:18,338 --> 00:17:22,174
que puede llegar a conocernos
más que nosotros mismos.

338
00:17:22,175 --> 00:17:24,309
¿Está bien que hagamos esto?

339
00:17:24,310 --> 00:17:25,978
Para tratar el tema,

340
00:17:25,979 --> 00:17:28,247
veré a una experta en ética,

341
00:17:28,248 --> 00:17:31,450
neurociencia e inteligencia artificial:

342
00:17:31,451 --> 00:17:32,918
Julia Bossmann.

343
00:17:32,919 --> 00:17:35,521
Es directora de estrategia
en Fathom Computing,

344
00:17:35,522 --> 00:17:37,990
miembro del Foro Económico Mundial,

345
00:17:37,991 --> 00:17:41,393
graduada de la Universidad
de la Singularidad de Ray Kurzweil

346
00:17:41,394 --> 00:17:44,096
y expresidenta del Instituto Foresight,

347
00:17:44,097 --> 00:17:51,337
un grupo de expertos especializados
en tecnologías futuras y su impacto.

348
00:17:51,338 --> 00:17:53,872
Gracias por charlar conmigo.

349
00:17:53,873 --> 00:17:55,541
- De nada.
- Eres la persona ideal

350
00:17:55,542 --> 00:17:57,076
para contestar estas preguntas.

351
00:17:57,077 --> 00:17:58,410
Y son profundas.

352
00:17:58,411 --> 00:18:01,814
Pero creo que son muy importantes
y cada vez más urgentes.

353
00:18:01,815 --> 00:18:04,483
Creo que vivimos
en una época muy interesante

354
00:18:04,484 --> 00:18:06,985
porque el cerebro y las máquinas
se están acercando.

355
00:18:06,986 --> 00:18:08,787
JULIA BOSSMANN, CONSEJO PARA EL FUTURO
DEL MUNDO, FORO ECONÓMICO MUNDIAL

356
00:18:08,788 --> 00:18:10,889
En lo que se refiere

357
00:18:10,890 --> 00:18:15,627
a analizar la actividad cerebral,
¿cuáles son los límites éticos?

358
00:18:15,628 --> 00:18:18,497
¿Cuán privados deben ser mis pensamientos?

359
00:18:18,498 --> 00:18:23,769
Como con cualquier tecnología poderosa,
depende de quién la maneja.

360
00:18:23,770 --> 00:18:25,070
Las nuevas tecnologías

361
00:18:25,071 --> 00:18:29,608
pueden darle más poder a quien las usa.

362
00:18:29,609 --> 00:18:32,711
No debemos culpar
a la tecnología, sino analizar

363
00:18:32,712 --> 00:18:36,181
cómo y quién la usa.

364
00:18:36,182 --> 00:18:40,519
¿Cómo nos aseguramos
de que esté en buenas manos?

365
00:18:40,520 --> 00:18:42,921
Es importante involucrar a la gente

366
00:18:42,922 --> 00:18:49,395
que legisla para que entiendan
lo que está por venir.

367
00:18:49,396 --> 00:18:52,631
Soy optimista
con respecto al trabajo en común.

368
00:18:52,632 --> 00:18:54,667
Hablemos del aspecto positivo.

369
00:18:54,668 --> 00:18:57,069
¿Cuáles son los usos posibles?

370
00:18:57,070 --> 00:19:00,673
Si pensamos en el difunto
Stephen Hawking, por ejemplo,

371
00:19:00,674 --> 00:19:05,010
si hubiera podido comunicarse mejor
con el mundo

372
00:19:05,011 --> 00:19:09,348
o las computadoras, ¿quién sabe
lo que hubiera compartido con nosotros?

373
00:19:09,349 --> 00:19:11,517
Y para los síndromes de enclaustramiento.

374
00:19:11,518 --> 00:19:14,920
Están ahí. Saben que están ahí.

375
00:19:14,921 --> 00:19:17,156
Pero necesitamos algo
para mirar su cerebro

376
00:19:17,157 --> 00:19:19,158
y ver qué tratan de decir

377
00:19:19,159 --> 00:19:21,527
- o qué sienten.
- Exacto.

378
00:19:21,528 --> 00:19:27,132
¿Qué le dices a las personas
que le temen a la tecnología,

379
00:19:27,133 --> 00:19:33,172
a que dejemos
de ser naturales por su causa?

380
00:19:33,173 --> 00:19:38,344
Hay algo atractivo
en llegar al siguiente nivel

381
00:19:38,345 --> 00:19:41,380
de lo que algunos llaman
la evolución humana

382
00:19:41,381 --> 00:19:43,816
o de la civilización.

383
00:19:43,817 --> 00:19:47,152
Y nuestras vidas ya no son naturales.

384
00:19:47,153 --> 00:19:50,322
De lo contrario, la mayoría moriríamos

385
00:19:50,323 --> 00:19:52,658
a los 30 o 40 años.

386
00:19:52,659 --> 00:19:54,693
Tendríamos todo tipo de enfermedades.

387
00:19:54,694 --> 00:19:56,228
No usaríamos ropa…

388
00:19:56,229 --> 00:19:57,396
No tendríamos anteojos

389
00:19:57,397 --> 00:19:59,031
- o lentes de contacto.
- Exacto.

390
00:19:59,032 --> 00:20:00,933
No tendríamos antibióticos.

391
00:20:00,934 --> 00:20:03,369
Ya somos

392
00:20:03,370 --> 00:20:06,205
cíborgs futuristas si nos comparamos

393
00:20:06,206 --> 00:20:09,875
con el ser humano de hace 10,000 años

394
00:20:09,876 --> 00:20:12,878
que genéticamente era
casi idéntico a nosotros.

395
00:20:12,879 --> 00:20:18,517
Sí, es cierto.

396
00:20:18,518 --> 00:20:20,753
Para entender la cognición

397
00:20:20,754 --> 00:20:23,956
hoy en día tenemos que pedirle a la gente

398
00:20:23,957 --> 00:20:27,726
que nos diga qué piensa
u observar su comportamiento.

399
00:20:27,727 --> 00:20:31,530
Pero leer los pensamientos
sería mucho mejor.

400
00:20:31,531 --> 00:20:34,400
Por eso el Dr. Kuhl estudia la memoria,

401
00:20:34,401 --> 00:20:39,071
y el Dr. Kamitani, el sueño.

402
00:20:39,072 --> 00:20:41,907
Y aunque la tecnología
tiene un largo camino por delante,

403
00:20:41,908 --> 00:20:45,978
es fácil ver cómo
podría haber dilemas éticos.

404
00:20:45,979 --> 00:20:47,680
Pero la verdad

405
00:20:47,681 --> 00:20:53,052
es que no existe
ningún ser humano salvaje.

406
00:20:53,053 --> 00:20:56,989
Estamos evolucionando
junto con la tecnología.

407
00:20:56,990 --> 00:21:00,793
Los humanos y la tecnología
somos inseparables hoy en día.

408
00:21:00,794 --> 00:21:04,630
Tenemos que tener cuidado
con todas nuestras innovaciones,

409
00:21:04,631 --> 00:21:09,234
pero no podemos evitar que sucedan.

410
00:21:09,235 --> 00:21:12,738
Es algo que vivimos constantemente.

411
00:21:12,739 --> 00:21:15,274
Podríamos haber discutido eternamente

412
00:21:15,275 --> 00:21:18,210
si debe o no debe haber
un límite de velocidad

413
00:21:18,211 --> 00:21:20,979
y quién debería hacerlo respetar,

414
00:21:20,980 --> 00:21:22,348
pero no lo hicimos.

415
00:21:22,349 --> 00:21:25,851
En su lugar, inventamos los autos,

416
00:21:25,852 --> 00:21:30,089
y nos encargamos de los detalles
responsablemente en el camino.

417
00:21:30,090 --> 00:21:32,591
Cuestionar la ética
de las nuevas tecnologías

418
00:21:32,592 --> 00:21:37,196
es bueno cuando le abren paso,

419
00:21:37,197 --> 00:21:40,699
no cuando obstaculizan su progreso.

420
00:21:40,700 --> 00:21:42,668
Así que sigan sus sueños.

421
00:21:42,669 --> 00:21:46,005
Y, tan pronto como puedan, muéstrenmelos.

422
00:21:46,006 --> 00:21:48,408
Y como siempre, gracias por mirarnos.


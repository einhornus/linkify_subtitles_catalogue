1
00:00:05,237 --> 00:00:06,438
Czytanie w myślach?

2
00:00:06,439 --> 00:00:08,172
Oczywiście nie.

3
00:00:08,173 --> 00:00:11,376
Kocham czytać.

4
00:00:11,377 --> 00:00:15,413
Słuchaj, czytanie w myślach może brzmieć
jak pseudonaukowe...

5
00:00:15,414 --> 00:00:16,614
przepraszam za mój język...

6
00:00:16,615 --> 00:00:18,416
strzelanie do byka.

7
00:00:18,417 --> 00:00:21,486
Ale jego naukowy odpowiednik,
identyfikacja myśli,

8
00:00:21,487 --> 00:00:23,855
jest bardzo realna.

9
00:00:23,856 --> 00:00:26,524
Opiera się na neuroobrazowaniu
i uczeniu maszynowym,

10
00:00:26,525 --> 00:00:29,861
a naprawdę fajne jest to,
że eksperymenty z czytaniem w myślach

11
00:00:29,862 --> 00:00:33,498
nie polegają tylko na szpiegowaniu
tego, co ktoś myśli.

12
00:00:33,499 --> 00:00:37,602
Chodzi o to, by dowiedzieć się, z
czego zbudowane są myśli.

13
00:00:37,603 --> 00:00:39,504
Chodzi mi o to, że kiedy myślę
o czymś,

14
00:00:39,505 --> 00:00:43,174
jak właściwie wygląda ten mentalny obraz
?

15
00:00:43,175 --> 00:00:44,709
W jakiej to rozdzielczości?

16
00:00:44,710 --> 00:00:47,145
Jak wysoką wiernością
jest pamięć

17
00:00:47,146 --> 00:00:49,380
i jak zmieniają
się w czasie?

18
00:00:49,381 --> 00:00:51,116
Cóż, w tym odcinku

19
00:00:51,117 --> 00:00:52,750
przyjrzę się, jak
czytanie ludzkich myśli

20
00:00:52,751 --> 00:00:54,719
może pomóc nam odpowiedzieć na
te pytania.

21
00:00:54,720 --> 00:00:58,490
Moja podróż zaczyna się właśnie tutaj,
na Uniwersytecie w Oregonie.

22
00:00:58,491 --> 00:01:01,259
Spotykam się z dr Brice Kuhl
z laboratorium Kuhl.

23
00:01:01,260 --> 00:01:03,394
Jest neurobiologiem,
który wykorzystuje neuroobrazowanie

24
00:01:03,395 --> 00:01:06,664
i uczenie maszynowe, aby dowiedzieć
się, co ludzie myślą,

25
00:01:06,665 --> 00:01:31,789
nie mówiąc mu o tym.

26
00:01:31,790 --> 00:01:33,625
Więc powiedz mi, co
tu robisz.

27
00:01:33,626 --> 00:01:36,794
Cóż, jestem
tutaj w programie neuronauki poznawczej

28
00:01:36,795 --> 00:01:38,396
i badam ludzką pamięć.

29
00:01:38,397 --> 00:01:41,099
Moje laboratorium wykorzystuje przede wszystkim
metody neuroobrazowania,

30
00:01:41,100 --> 00:01:42,567
więc wykonujemy dużo pracy z wykorzystaniem

31
00:01:42,568 --> 00:01:44,169
funkcjonalnego
rezonansu magnetycznego,

32
00:01:44,170 --> 00:01:45,637
czyli fMRI.

33
00:01:45,638 --> 00:01:49,340
A jak wykorzystujesz
fMRI do badania wspomnień?

34
00:01:49,341 --> 00:01:51,776
Przyglądamy się wzorcowi
aktywności neuronowej.

35
00:01:51,777 --> 00:01:54,445
Kiedy tworzysz wspomnienie
, pojawia się pewien wzór.

36
00:01:54,446 --> 00:01:56,548
I możemy nagrać
ten wzór,

37
00:01:56,549 --> 00:01:59,617
a następnie sprawdzić, czy
ten wzór został przywrócony,

38
00:01:59,618 --> 00:02:02,554
czy reaktywowany w późniejszym momencie, na
przykład wtedy, gdy go sobie przypominasz.

39
00:02:02,555 --> 00:02:05,823
Czy to oznacza, że możemy spojrzeć na wz
rce aktywności mózgu i wyd

40
00:02:05,824 --> 00:02:10,061
dukować, co jest 
apamiętywane, przywoływane, a naw

41
00:02:10,062 --> 00:02:11,262
t po prostu myśl

42
00:02:11,263 --> 00:02:13,631
?  Tak, i tak nazywamy to
dekodowaniem.

43
00:02:13,632 --> 00:02:16,568
Więc w zasadzie przyjmuje
twój wzorzec wejściowy

44
00:02:16,569 --> 00:02:18,469
jako wzorzec aktywności
, który rejestrujemy,

45
00:02:18,470 --> 00:02:21,272
gdy coś sobie przypominasz
.

46
00:02:21,273 --> 00:02:23,441
Przewidujemy
, co pamiętasz.

47
00:02:23,442 --> 00:02:27,178
Możesz zobaczyć, jak to brzmi
jak czytanie w myślach.

48
00:02:27,179 --> 00:02:28,713
[śmiech]
Tak.  Tak to brzmi.

49
00:02:28,714 --> 00:02:32,584
Więc, Brice, co
zamierzasz mi dzisiaj zrobić?

50
00:02:32,585 --> 00:02:34,419
Więc to, co
będziemy dzisiaj robić,

51
00:02:34,420 --> 00:02:36,354
jest
dla nas niezbadanym terytorium.

52
00:02:36,355 --> 00:02:38,623
Więc wypróbujemy
na tobie nowy

53
00:02:38,624 --> 00:02:40,258
wariant eksperymentu.

54
00:02:40,259 --> 00:02:42,660
Więc nie mogę zagwarantować
żadnych konkretnych wyników.

55
00:02:42,661 --> 00:02:44,195
Ale przedstawia,
gdzie jest pole

56
00:02:44,196 --> 00:02:46,664
i
dokąd zmierzamy.

57
00:02:46,665 --> 00:02:48,800
Dzisiaj weźmiesz
udział w eksperymencie, w

58
00:02:48,801 --> 00:02:50,268
którym będziesz badać twarze.

59
00:02:50,269 --> 00:02:51,803
Więc
mamy zamiar przestudiować

60
00:02:51,804 --> 00:02:53,404
12 zdjęć celebrytów.

61
00:02:53,405 --> 00:02:54,872
Ludzie, których już
znam.

62
00:02:54,873 --> 00:02:56,674
-Ludzie, których znasz, tak.
-Dobra.

63
00:02:56,675 --> 00:02:59,143
I
spróbujesz zapamiętać te zdjęcia.

64
00:02:59,144 --> 00:03:01,112
Następnie każemy ci wejść
do skanera MRI.

65
00:03:01,113 --> 00:03:04,249
Spróbuj przywołać ten obraz
tak żywo, jak to tylko możliwe.

66
00:03:04,250 --> 00:03:06,417
Będziemy
rejestrować aktywność mózgu,

67
00:03:06,418 --> 00:03:08,753
gdy będziesz próbował sobie wyobrazić
te obrazy.

68
00:03:08,754 --> 00:03:10,555
Spróbujemy
zbudować twarz.

69
00:03:10,556 --> 00:03:12,590
Zasadniczo narysuj obraz
tego, co pamiętasz.

70
00:03:12,591 --> 00:03:14,125
-Obrazek?
-Obrazek.

71
00:03:14,126 --> 00:03:16,327
Rzeczywiste zdjęcie,
które możemy wydrukować

72
00:03:16,328 --> 00:03:17,729
i mogę
powiesić na ścianie.

73
00:03:17,730 --> 00:03:19,831
[śmiech]
Gdybyś chciał.

74
00:03:19,832 --> 00:03:22,634
[Michael]Pierwszym krokiem
jest

75
00:03:22,635 --> 00:03:25,336
zapamiętanie 12 konkretnych
zdjęć celebrytów, które

76
00:03:25,337 --> 00:03:28,640
Brice później
spróbuje wykryć, o czym myślę.

77
00:03:28,641 --> 00:03:33,278
Usiadłem, żeby zrobić tego
doktoranta, Maxa.

78
00:03:33,279 --> 00:03:35,280
Powodzenie jego przewidywań
zależy po części

79
00:03:35,281 --> 00:03:37,415
od mojej zdolności
do przypomnienia sobie tych twarzy

80
00:03:37,416 --> 00:03:43,087
tak wyraźnie, jak to tylko możliwe,
będąc wewnątrz fMRI.

81
00:03:43,088 --> 00:03:44,422
W porządku, więc...

82
00:03:44,423 --> 00:03:46,157
[wzdycha]

83
00:03:46,158 --> 00:03:50,728
Myślę, że mam całkiem dobrą
pamięć do tych wszystkich.

84
00:03:50,729 --> 00:03:53,698
-Świetny.
- Czuję, że stawka jest wysoka.

85
00:03:53,699 --> 00:03:56,701
Gdy twarze sławnych osób zostaną
zapamiętane,

86
00:03:56,702 --> 00:03:58,303
nadszedł czas na kolejny krok:

87
00:03:58,304 --> 00:04:00,071
przejście
przez wykrywacz

88
00:04:00,072 --> 00:04:01,706
metali do fMRI,

89
00:04:01,707 --> 00:04:04,676
gdzie Brice będzie rejestrował
i monitorował aktywność mojego mózgu,

90
00:04:04,677 --> 00:04:08,746
a następnie wprowadzi ją do swojego
algorytmu, aby odbudować twarze.

91
00:04:08,747 --> 00:04:10,448
To będzie pierwszy raz,
kiedy

92
00:04:10,449 --> 00:04:12,383
spróbuje zrekonstruować twarze
z pamięci długotrwałej,

93
00:04:12,384 --> 00:04:14,385
co jest bardzo trudne,
ponieważ polegamy

94
00:04:14,386 --> 00:04:16,720
na tym, jak wyraźnie
pamiętam zdjęcia celebrytów,

95
00:04:16,721 --> 00:04:19,090
które widziałem godzinę temu.

96
00:04:19,091 --> 00:04:21,225
Kocham jego oczy.
Spójrz na to.

97
00:04:21,226 --> 00:04:24,362
[kobieta]

98
00:04:24,363 --> 00:04:31,102
Czy dzieciak nie powiedziałby:
„To mnie zje”?

99
00:04:31,103 --> 00:04:34,205
fMRI monitoruje aktywność
w mózgu

100
00:04:34,206 --> 00:04:36,774
, dzieląc ją
na tysiące małych kostek

101
00:04:36,775 --> 00:04:39,744
zwanych wokselami
lub pikselami wolumetrycznymi.

102
00:04:39,745 --> 00:04:41,446
Każdy z tych wokseli zawiera

103
00:04:41,447 --> 00:04:43,581
setki
tysięcy neuronów.

104
00:04:43,582 --> 00:04:46,117
Za pomocą fMRI
jesteśmy w stanie wykryć

105
00:04:46,118 --> 00:04:47,719
przepływ krwi
w obrębie tych wokseli,

106
00:04:47,720 --> 00:04:50,088
co oznacza, że ta czę�
ć mózgu jest aktyw

107
00:04:50,089 --> 00:04:53,124
a.  Jeśli pokaże mi się kilka
zdjęć osób z wąsami,

108
00:04:53,125 --> 00:04:56,327
mój mózg zareaguje
na cechy każdej twarzy.

109
00:04:56,328 --> 00:04:58,329
Ale
będzie wspólny obszar mojego mózgu,

110
00:04:58,330 --> 00:05:00,164
który będzie zaangażowany
przez cały czas.

111
00:05:00,165 --> 00:05:04,102
To może być obszar mojego
mózgu, który reaguje na wąsy.

112
00:05:04,103 --> 00:05:07,171
Więc później,
kiedy wyobrażam sobie twarz,

113
00:05:07,172 --> 00:05:09,440
jeśli Brice zauważy,
że obszar jest zajęty

114
00:05:09,441 --> 00:05:11,476
, może przewidzieć
, że myślę

115
00:05:11,477 --> 00:05:13,811
o wąsach.

116
00:05:13,812 --> 00:05:15,780
Więc teraz Michael jest
w skanerze

117
00:05:15,781 --> 00:05:18,282
i widzi słowa pojawiające
się na ekranie pojedynczo

118
00:05:18,283 --> 00:05:20,651
i
próbuje zwizualizować

119
00:05:20,652 --> 00:05:23,187
twarz, zapamiętać ją tak
szczegółowo, jak to możliwe.

120
00:05:23,188 --> 00:05:25,356
To, co widzisz tutaj,
to obrazy, które pozyskujemy.

121
00:05:25,357 --> 00:05:28,793
Co dwie sekundy otrzymujemy jeden z tych
tomów mózgu.

122
00:05:28,794 --> 00:05:32,797
Są więc odświeżane w czasie rzeczywistym,
gdy zbieramy obrazy.

123
00:05:32,798 --> 00:05:35,633
[Michael]Po
zakończeniu pierwszej części sesji fMRI

124
00:05:35,634 --> 00:05:38,503
nadszedł czas na część drugą, w
której Brice i jego zespół

125
00:05:38,504 --> 00:05:41,773
nauczą się języka
aktywności mojego mózgu,

126
00:05:41,774 --> 00:05:44,776
aby mogli później
dekodować za pomocą skanów mózgu.

127
00:05:44,777 --> 00:05:46,544
Cześć Michał.
Nadal masz się dobrze?

128
00:05:46,545 --> 00:05:48,312
[Michael]
Tak.

129
00:05:48,313 --> 00:05:50,381
Pokażą mi
setki wyjątkowych twarzy

130
00:05:50,382 --> 00:05:52,784
i nagrają, jak mój mózg reaguje

131
00:05:52,785 --> 00:05:54,852
na określone
cechy twarzy.

132
00:05:54,853 --> 00:05:57,188
Następnie wykorzystają
te informacje

133
00:05:57,189 --> 00:05:59,657
do
zrekonstruowania twarzy celebrytów

134
00:05:59,658 --> 00:06:03,127
, o których myślałem
podczas pierwszej fazy skanowania.

135
00:06:03,128 --> 00:06:05,530
Naprawdę, im więcej twarzy
możemy pokazać Michaelowi, tym lepiej.

136
00:06:05,531 --> 00:06:08,166
Więc w zasadzie będziemy go tam trzymać


137
00:06:08,167 --> 00:06:09,600
tak długo, jak będzie mu wygodnie.

138
00:06:09,601 --> 00:06:11,636
[Michael]
Dwie godziny to maksymalny czas,

139
00:06:11,637 --> 00:06:13,538
jaki mogliśmy uzyskać w fMRI.

140
00:06:13,539 --> 00:06:17,175
Ale udało mi się
obejrzeć ponad 400 twarzy,

141
00:06:17,176 --> 00:06:18,743
co powinno wystarczyć, aby uzyskać

142
00:06:18,744 --> 00:06:20,778
całkiem ciekawe
rezultaty.

143
00:06:20,779 --> 00:06:22,447
Hej, Michael, zrobiłeś to.
To było świetne.

144
00:06:22,448 --> 00:06:23,681
Wyjdziemy
cię wyciągnąć.

145
00:06:23,682 --> 00:06:33,157
[Michael]
W porządku.

146
00:06:33,158 --> 00:06:34,725
Tak, więc te pokazują tylko
niektóre zdjęcia,

147
00:06:34,726 --> 00:06:36,561
które robiliśmy,
kiedy tam byłeś.

148
00:06:36,562 --> 00:06:38,095
Niektóre obrazy twojego mózgu.

149
00:06:38,096 --> 00:06:39,764

Teraz zmiażdżymy kilka liczb.

150
00:06:39,765 --> 00:06:42,200
Max przeanalizuje
twoje dane.

151
00:06:42,201 --> 00:06:43,701
Spotkamy się
ponownie jutro,

152
00:06:43,702 --> 00:06:45,369
gdzie
przyjrzymy się wynikom i

153
00:06:45,370 --> 00:06:47,638
spróbujemy
zrekonstruować obrazy twarzy

154
00:06:47,639 --> 00:06:49,740
na podstawie danych mózgowych
, które właśnie zebraliśmy.

155
00:06:49,741 --> 00:06:51,175
W porządku.
Dobrze, do zobaczenia jutro.

156
00:06:51,176 --> 00:06:52,577
W porządku.
Wielkie dzięki.

157
00:06:52,578 --> 00:06:54,178
Max, również dziękuję.
Nie mogę się doczekać.

158
00:06:54,179 --> 00:06:55,847
Lepiej
pociągnij całą noc.

159
00:06:55,848 --> 00:07:04,288
Chcę, żeby te
dane były doskonałe.

160
00:07:04,289 --> 00:07:06,524
W porządku, więc wróciłem
do laboratorium doktora Kuhla.

161
00:07:06,525 --> 00:07:08,693
W ciągu nocy jego zespół
przejrzał dane

162
00:07:08,694 --> 00:07:15,500
i nie mogę się doczekać, aby zobaczyć, co
według nich widziałem, jak myślę.

163
00:07:15,501 --> 00:07:17,101
Jakie są moje wyniki?

164
00:07:17,102 --> 00:07:18,736
Myślę, że dobrze wyglądają.

165
00:07:18,737 --> 00:07:20,705
Za chwilę przyjrzymy
się tutaj.

166
00:07:20,706 --> 00:07:22,406
Dobra,
nie mogę się doczekać.

167
00:07:22,407 --> 00:07:24,342
-Więc mogę po prostu usiąść?
-Tak, usiądź.

168
00:07:24,343 --> 00:07:26,143
W porządku, więc...

169
00:07:26,144 --> 00:07:28,212
po pierwsze...

170
00:07:28,213 --> 00:07:30,081
co ja widzę?
Och, ok, cóż,

171
00:07:30,082 --> 00:07:32,283
to są zdjęcia,
które właściwie zapamiętałem.

172
00:07:32,284 --> 00:07:34,252
-Zgadza się.
-I

173
00:07:34,253 --> 00:07:37,822
to właśnie zrekonstruowałeś
z mojej wyobraźni.

174
00:07:37,823 --> 00:07:40,091
-Zgadza się.
-Och, wow.  Dobra.

175
00:07:40,092 --> 00:07:43,094
[Brice]
Dobra, to jest jedna
z

176
00:07:43,095 --> 00:07:44,562
wygenerowanych rekonstrukcji.

177
00:07:44,563 --> 00:07:46,063
[Michael]
Interesujące.

178
00:07:46,064 --> 00:07:47,698
[Max]
Więc to jest John Cho.

179
00:07:47,699 --> 00:07:50,668
[Michael]
Nieźle.  Nie jest zły.

180
00:07:50,669 --> 00:07:53,337
-Czy możemy zobaczyć obok siebie?
-Tak.

181
00:07:53,338 --> 00:07:55,673
[Michael]
Widzę, no wiesz, podobieństwa

182
00:07:55,674 --> 00:08:00,044
w rodzajach
mimiki w ogóle.

183
00:08:00,045 --> 00:08:02,213
Wiesz, prawie można
zobaczyć tutaj pasującą linię włosów.

184
00:08:02,214 --> 00:08:04,682

Myślałem, że kształt twarzy też był...

185
00:08:04,683 --> 00:08:06,684
Miał
trochę kwadratowy kształt.

186
00:08:06,685 --> 00:08:08,152
-TAk.  TAk.
-Więc to są rzeczy,

187
00:08:08,153 --> 00:08:09,387
które mi wyszły.

188
00:08:09,388 --> 00:08:11,289
I tak, kiedy
wizualizowałem

189
00:08:11,290 --> 00:08:13,257
ten obraz Johna Cho

190
00:08:13,258 --> 00:08:16,260
, prostopadłość twarzy
była pierwszą, najbardziej znaczącą rzeczą.

191
00:08:16,261 --> 00:08:19,697
Po prostu myślałem,
że był kwadratowym facetem.

192
00:08:19,698 --> 00:08:23,401
Świetnie, w porządku.

193
00:08:23,402 --> 00:08:26,704
[Brice]
Więc to jest Megan Fox.

194
00:08:26,705 --> 00:08:28,439
[Michael]
Mm-hmm.

195
00:08:28,440 --> 00:08:30,207
Pokażesz nam
obok siebie.

196
00:08:30,208 --> 00:08:31,776
[Michael
] Obok siebie.  Dobrze.

197
00:08:31,777 --> 00:08:33,644
[Brice]
Możesz zobaczyć obraz,
który faktycznie widziałeś,

198
00:08:33,645 --> 00:08:36,547
i to jest rekonstrukcja,
którą wygenerowaliśmy.

199
00:08:36,548 --> 00:08:39,417
Zrobię to.
Megan Fox, nie byłam w

200
00:08:39,418 --> 00:08:42,353
stanie mieć w głowie naprawdę wyraźnego obrazu
.

201
00:08:42,354 --> 00:08:45,056
Z jakiegoś powodu ten
obraz jej był dla mnie naprawdę trudny

202
00:08:45,057 --> 00:08:47,058
do przywołania w pamięci.

203
00:08:47,059 --> 00:08:50,595
Surowość na twarzy była
czymś, co zauważyłem.

204
00:08:50,596 --> 00:08:53,698
Więc wyczułem, że jest...
Wyglądało to kobieco.

205
00:08:53,699 --> 00:08:55,533
I
podchwyciłeś surowość.

206
00:08:55,534 --> 00:08:58,769
I tak razem,
to daje dopasowanie.

207
00:08:58,770 --> 00:09:00,538
[Michael]Pamiętaj,
że Brice i jego

208
00:09:00,539 --> 00:09:02,773
zespół wyczytali to
z mojej pamięci.

209
00:09:02,774 --> 00:09:04,609
Ale kiedy pamiętam twarz,

210
00:09:04,610 --> 00:09:07,678
czy wyobrażam sobie każdy szczegół
jednocześnie

211
00:09:07,679 --> 00:09:09,313
z fotograficzną dokładnością?

212
00:09:09,314 --> 00:09:10,748
A może po prostu zajmuję się kilkoma
na raz?

213
00:09:10,749 --> 00:09:13,417
Czytając moje myśli,
mogą zobaczyć,

214
00:09:13,418 --> 00:09:15,219
jak zła jest moja pamięć
i jak to działa.

215
00:09:15,220 --> 00:09:18,689
-Ja!  Ja!
[Brice się śmieje]

216
00:09:18,690 --> 00:09:21,525
Dobra, to jest twoja


217
00:09:21,526 --> 00:09:24,729
rekonstrukcja mojego myślenia o
tym obrazie siebie.

218
00:09:24,730 --> 00:09:26,497
[Brice]
Zgadza się.

219
00:09:26,498 --> 00:09:28,666
Gdzie się podziała broda?

220
00:09:28,667 --> 00:09:31,068
[Brice] Nie wiem.
Miałem nadzieję, że mi powiesz.

221
00:09:31,069 --> 00:09:36,240
[Michael]
Na przykład to jest zdjęcie,
na którym pamiętam własną twarz.

222
00:09:36,241 --> 00:09:38,743
To naprawdę nie wygląda jak ja,
ale pytanie brzmi:

223
00:09:38,744 --> 00:09:41,345
jak dobrze
sobie wyobrażam siebie?

224
00:09:41,346 --> 00:09:44,048
Rzadko myślę o własnej twarzy
, więc dziwność

225
00:09:44,049 --> 00:09:45,616

w wyniku

226
00:09:45,617 --> 00:09:47,752
może dotyczyć w równym stopniu
wad mojej własnej pamięci

227
00:09:47,753 --> 00:09:51,288
i mentalnego obrazu siebie,
co wad technologii.

228
00:09:51,289 --> 00:09:53,691
Więc to jest Jennifer Lawrence,
jak sądzę.

229
00:09:53,692 --> 00:09:55,726
[Michael]
To Jennifer Lawrence?

230
00:09:55,727 --> 00:09:59,664
Wygląda na to, że
to znacznie starszy wujek Jennifer Lawrence.

231
00:09:59,665 --> 00:10:01,365
[wszyscy chichoczą]

232
00:10:01,366 --> 00:10:05,069
Nic tutaj nie było zbyt
blisko oszałamiająco.

233
00:10:05,070 --> 00:10:09,407
Ale to jest coś,
od czego dopiero zaczynasz próbować

234
00:10:09,408 --> 00:10:11,409
tego rodzaju długotrwałych
wspomnień.

235
00:10:11,410 --> 00:10:14,679
To, co Brice i jego zespół
odczytali w moim umyśle,

236
00:10:14,680 --> 00:10:18,549
mogłoby być dokładniejsze,
gdyby pokazali mi tysiące,

237
00:10:18,550 --> 00:10:20,284
a nie setki obrazów
w fMRI,

238
00:10:20,285 --> 00:10:22,520
ponieważ wtedy algorytm dokładniej
poznałby

239
00:10:22,521 --> 00:10:24,655
język mojego mózgu
.

240
00:10:24,656 --> 00:10:27,358
Ale niezależnie od tego
, jakość moich wspomnień

241
00:10:27,359 --> 00:10:29,226
nadal
byłaby problemem.

242
00:10:29,227 --> 00:10:30,661
To znaczy, spójrz, co się dzieje,
gdy pamięć

243
00:10:30,662 --> 00:10:33,164
jest całkowicie wycinana z równania
.

244
00:10:33,165 --> 00:10:35,166
Brice odczytywał również
aktywność mojego mózgu,

245
00:10:35,167 --> 00:10:37,168
gdy patrzyłem
na twarze w fMRI.

246
00:10:37,169 --> 00:10:39,103
nie tylko ich wyobrażanie.

247
00:10:39,104 --> 00:10:41,672
I te wyniki
były znacznie bliższe

248
00:10:41,673 --> 00:10:44,675
niż te odtworzone
z mojej pamięci.

249
00:10:44,676 --> 00:10:47,111
Ok, więc na co ja tutaj patrzę
?

250
00:10:47,112 --> 00:10:48,679
[Brice]
Ok, więc to, co widzisz

251
00:10:48,680 --> 00:10:51,682
w górnym rzędzie,
to obrazy, które widziałeś,

252
00:10:51,683 --> 00:10:53,584
gdy byłeś w skanerze.

253
00:10:53,585 --> 00:10:56,754
Poniżej, w tym dolnym rzędzie,
są to rekonstrukcje

254
00:10:56,755 --> 00:11:00,725
, które czerpiemy z zebranych wzorców
aktywności mózgu.

255
00:11:00,726 --> 00:11:03,828
- To jest z obrazu źródłowego.
-Prawidłowy.

256
00:11:03,829 --> 00:11:05,629
[Michael]
To są z mojego mózgu.

257
00:11:05,630 --> 00:11:07,565
[Brice] Tak.
[Michael] Są całkiem blisko.

258
00:11:07,566 --> 00:11:09,700
Tak, ogólnie byli
całkiem blisko.

259
00:11:09,701 --> 00:11:11,602
Więc nie idealnie.

260
00:11:11,603 --> 00:11:13,771
To są... widać, że jest
w nich pewna zmienność.

261
00:11:13,772 --> 00:11:16,640
Ale jest to zgodne
z tym, co odkryliśmy wcześniej,

262
00:11:16,641 --> 00:11:18,409
że rekonstrukcje
, które wygenerowaliśmy,

263
00:11:18,410 --> 00:11:20,244
kiedy patrzysz
na twarze

264
00:11:20,245 --> 00:11:22,279
, jest pewna zgodność
między rzeczywistą twarzą.

265
00:11:22,280 --> 00:11:23,714
To jest rodzaj
sprawdzenia zdrowia psychicznego,

266
00:11:23,715 --> 00:11:25,750
dzięki któremu możemy właściwie
zrekonstruować obrazy

267
00:11:25,751 --> 00:11:28,219
- kiedy je oglądasz.
- Dobrze, dobrze.

268
00:11:28,220 --> 00:11:31,355
Są całkiem dobre.

269
00:11:31,356 --> 00:11:33,190
Cóż, Brice, Max,
bardzo dziękuję

270
00:11:33,191 --> 00:11:35,126
za to, że pozwoliłeś mi
być częścią tego.

271
00:11:35,127 --> 00:11:36,660
Mam nadzieję, że moje dane się przydadzą.

272
00:11:36,661 --> 00:11:38,596
Dziękuję Ci.
To była świetna zabawa.

273
00:11:38,597 --> 00:11:46,837
Zawsze warto
myśleć o tych rzeczach.

274
00:11:46,838 --> 00:11:50,808
Badania nad pamięcią przeprowadzone przez dr Brice'a Kuhla
pokazują,

275
00:11:50,809 --> 00:11:53,677
że komputer
może czytać w czyichś myślach.

276
00:11:53,678 --> 00:11:56,313
Aby dowiedzieć się,
co myślą.

277
00:11:56,314 --> 00:11:58,415
Ale
wciąż pozostaje wiele do zrobienia.

278
00:11:58,416 --> 00:12:00,050
To znaczy, jeśli na przykład chcesz wiedzieć, o

279
00:12:00,051 --> 00:12:01,619
czym teraz myślę


280
00:12:01,620 --> 00:12:05,189
, nadal łatwiej jest po prostu poprosić mnie,
żebym ci powiedziała.

281
00:12:05,190 --> 00:12:07,491
Ale co, jeśli nie
mogę ci powiedzieć?

282
00:12:07,492 --> 00:12:10,594
Dr Yukiyasu Kamitani
jest naukowcem,

283
00:12:10,595 --> 00:12:14,498
profesorem i pionierem
badającym granice

284
00:12:14,499 --> 00:12:17,535
za ścianą snu.

285
00:12:17,536 --> 00:12:19,703
Przyjechałem tu
na Uniwersytet w Kioto,

286
00:12:19,704 --> 00:12:21,672
żeby się z nim spotkać i zobaczyć,
jak to jest

287
00:12:21,673 --> 00:12:24,175
czytać nie to, co
ktoś myśli,

288
00:12:24,176 --> 00:12:29,647
ale to, co
ktoś marzy.

289
00:12:29,648 --> 00:12:31,348
Sensei Kamitani,
jestem Michael.

290
00:12:31,349 --> 00:12:33,818
-Cześć, jestem Yuki.
- Yuki, miło cię poznać.

291
00:12:33,819 --> 00:12:36,320
[Michael]
Przez ostatnie dziesięć lat

292
00:12:36,321 --> 00:12:38,455
dr Kamitani był
w

293
00:12:38,456 --> 00:12:40,124
czołówce maszynowego czytania w myślach.

294
00:12:40,125 --> 00:12:43,527
Obiekt jest, wiecie,
gotowy do wejścia.

295
00:12:43,528 --> 00:12:45,462
Podobnie jak Brice Kuhl,

296
00:12:45,463 --> 00:12:48,632
jego wczesne eksperymenty badały
rekonstrukcję obrazów

297
00:12:48,633 --> 00:12:52,236
pokazywanych badanym w fMRI w
oparciu o aktywność ich mózgu.

298
00:12:52,237 --> 00:12:53,637
W przypadku Kamitaniego

299
00:12:53,638 --> 00:12:55,706
obrazy były
czarno-białe,

300
00:12:55,707 --> 00:12:58,375
a
rekonstrukcje uderzająco dokładne.

301
00:12:58,376 --> 00:13:03,180
Ostatnio Kamitani skoncentrował się
na wykorzystaniu głębokich sieci neuronowych

302
00:13:03,181 --> 00:13:04,815
i uczenia maszynowego

303
00:13:04,816 --> 00:13:06,450
do rozszyfrowania
aktywności mózgu badanych

304
00:13:06,451 --> 00:13:08,686
podczas oglądania
znacznie bardziej złożonych zdjęć.

305
00:13:08,687 --> 00:13:12,756
To, co widzisz, jest
wynikiem głębokiej sieci neuronowej

306
00:13:12,757 --> 00:13:15,226
przetwarzającej aktywność
mózgu osoby

307
00:13:15,227 --> 00:13:17,795
patrzącej na zdjęcie.

308
00:13:17,796 --> 00:13:20,431
Może to mieć
w przyszłości niezliczone zastosowania,

309
00:13:20,432 --> 00:13:22,733
na przykład
w śledztwach kryminalnych

310
00:13:22,734 --> 00:13:26,470
i
komunikacji interpersonalnej.

311
00:13:26,471 --> 00:13:28,739
[Kamitani]
To jest dalekie od ideału.

312
00:13:28,740 --> 00:13:33,177
Ale myślę, że nadal widzisz niektóre,
no wiesz, oczy i, no wiesz...

313
00:13:33,178 --> 00:13:34,778
[Michael]
Cóż, tak.

314
00:13:34,779 --> 00:13:36,447
I kolory też.

315
00:13:36,448 --> 00:13:39,783
[Kamitani]
Tak, do pewnego stopnia, tak.

316
00:13:39,784 --> 00:13:42,419
Jednak jego najnowsza praca
dotyczy podświadomości.

317
00:13:42,420 --> 00:13:45,256
Próbuje zrobić coś
niezwykle ambitnego:

318
00:13:45,257 --> 00:13:46,824
zarejestrować nasze marzenia.

319
00:13:46,825 --> 00:13:49,326
Czy nazwałbyś
siebie badaczem snu

320
00:13:49,327 --> 00:13:50,828
czy badaczem wzroku?

321
00:13:50,829 --> 00:13:53,664
Może dekoder mózgu.

322
00:13:53,665 --> 00:13:55,432
Dekoder mózgu.

323
00:13:55,433 --> 00:13:57,534
To całkiem fajny
opis pracy.

324
00:13:57,535 --> 00:14:01,739
Czy możesz mi pokazać cokolwiek z
tego, co robisz ze snami?

325
00:14:01,740 --> 00:14:08,379
[Kamitani]
Mm-hmm, tak.

326
00:14:08,380 --> 00:14:10,514
Praca dr Kamitani
nad dekodowaniem snów

327
00:14:10,515 --> 00:14:13,317
zaczyna się od podobnego procesu,
co dr Kuhl:

328
00:14:13,318 --> 00:14:15,452
pokazując badanemu
tysiące obrazów

329
00:14:15,453 --> 00:14:17,187
podczas badania fMRI

330
00:14:17,188 --> 00:14:18,722
, aby dowiedzieć się,
jak wygląda mózg,

331
00:14:18,723 --> 00:14:21,358
gdy myśli
o pewnych rzeczach.

332
00:14:21,359 --> 00:14:23,794
Gdy algorytm uczenia maszynowego
jest już całkiem dobry

333
00:14:23,795 --> 00:14:26,764
w identyfikowaniu obrazów,
o których myśli podmiot

334
00:14:26,765 --> 00:14:29,300
, podmiot jest umieszczany
w fMRI

335
00:14:29,301 --> 00:14:31,335
z czapką EEG
na głowie

336
00:14:31,336 --> 00:14:33,470
i zapraszany do zaśnięcia.

337
00:14:33,471 --> 00:14:36,573
Kiedy fale EEG wskazują,
że osoba śni

338
00:14:36,574 --> 00:14:39,243
, algorytm przewiduje, o
jakich

339
00:14:39,244 --> 00:14:41,645
rzeczach osoba najprawdopodobniej
śni.

340
00:14:41,646 --> 00:14:45,215
Obecnie algorytm
szuka 20 kategorii.

341
00:14:45,216 --> 00:14:48,485
Rzeczy takie jak budynki,
transport

342
00:14:48,486 --> 00:14:50,621
i postacie
w języku.

343
00:14:50,622 --> 00:14:53,324
Następnie badacze
budzą badanego,

344
00:14:53,325 --> 00:14:55,326
pytają, o czym
śnili,

345
00:14:55,327 --> 00:14:57,227
i sprawdzają, czy przewidywania algorytmu


346
00:14:57,228 --> 00:14:59,530
i
wspomnienia osoby pasują do siebie.

347
00:14:59,531 --> 00:15:03,367
Oto aktualne dane z
jednego z eksperymentów Kamitaniego.

348
00:15:03,368 --> 00:15:05,703
Poniżej znajduje się chmura
słów kategorii.

349
00:15:05,704 --> 00:15:08,339
Nazwy każdej
kategorii powiększają się lub zmniejszają

350
00:15:08,340 --> 00:15:10,574
w czasie rzeczywistym w
oparciu o prawdopodobieństwo

351
00:15:10,575 --> 00:15:13,344
, że są one obecne
w aktualnym śnie podmiotu.

352
00:15:13,345 --> 00:15:15,346
Jak widać,
aktywność jest obecnie najsilniejsza

353
00:15:15,347 --> 00:15:18,349
dla kategorii „postać”,
czyli języka pisanego.

354
00:15:18,350 --> 00:15:20,684
W tym
momencie podmiot został przebudzony

355
00:15:20,685 --> 00:15:29,660
i tak
właśnie donosili.

356
00:15:29,661 --> 00:15:32,062
To dość straszne.

357
00:15:32,063 --> 00:15:33,697
-[śmiech]
-Prawda?  Mam na myśli, ty...

358
00:15:33,698 --> 00:15:36,700
szpiegowałeś ich sen.

359
00:15:36,701 --> 00:15:39,370
Tak, w pewnym sensie.
Ale

360
00:15:39,371 --> 00:15:42,406
... dokładność
nie jest aż tak duża, więc...

361
00:15:42,407 --> 00:15:44,341
Cóż, dokładność
nie jest aż tak duża, ale wiesz,

362
00:15:44,342 --> 00:15:46,677
moja normalna dokładność do odgadywania
snów ludzi wynosi zero.

363
00:15:46,678 --> 00:15:48,145
Dobrze.

364
00:15:48,146 --> 00:15:49,580
Kontynuując
swoje badania

365
00:15:49,581 --> 00:15:52,182
nad
przewidywaniem treści snów,

366
00:15:52,183 --> 00:15:54,785
dr Kamitani
rozpoczyna swój najnowszy projekt:

367
00:15:54,786 --> 00:15:58,389
rekonstrukcję obrazów
z naszych snów.

368
00:15:58,390 --> 00:16:01,458
Więc przyniosłeś
kilka rekonstrukcji,

369
00:16:01,459 --> 00:16:02,659
które stworzyło twoje laboratorium...

370
00:16:02,660 --> 00:16:04,061
Mm-hmm.

371
00:16:04,062 --> 00:16:17,741
...snów.

372
00:16:17,742 --> 00:16:20,210
Zgadza się, wszystkie wyglądają jak sny
o plamach.

373
00:16:20,211 --> 00:16:21,779
[Kamitani]
Tak.

374
00:16:21,780 --> 00:16:24,448
To znaczy, chcę po prostu
zrobić krok wstecz i...

375
00:16:24,449 --> 00:16:27,785
docenić, że to, na co
patrzymy na tym

376
00:16:27,786 --> 00:16:31,655
ekranie, to w pewnym sensie jedne z pierwszych
zdjęć snu.

377
00:16:31,656 --> 00:16:33,357
Mm-hmm.

378
00:16:33,358 --> 00:16:35,759
Patrzymy
na najwcześniejszą fazę

379
00:16:35,760 --> 00:16:38,062
badań rewolucyjnych.

380
00:16:38,063 --> 00:16:40,164
Pewnego dnia
możemy mieć obrazy,

381
00:16:40,165 --> 00:16:42,766
a nawet nagrywać
filmy naszych własnych snów.

382
00:16:42,767 --> 00:16:45,335
A dr Kamitani jest jak dotąd jedyną
osobą na świecie,

383
00:16:45,336 --> 00:16:47,237
która to robi.

384
00:16:47,238 --> 00:16:50,707
Jest samotnym odkrywcą podróżującym
do naszej podświadomości.

385
00:16:50,708 --> 00:16:53,644
Więc ta praca nie
została jeszcze opublikowana.

386
00:16:53,645 --> 00:16:55,345
Nie.

387
00:16:55,346 --> 00:17:01,752
-Dziękuję za pokazanie mi go.
[śmiech

388
00:17:01,753 --> 00:17:06,155
] Spostrzeżenia, które badacze
tacy jak dr Kuhl i dr

389
00:17:06,156 --> 00:17:09,159
Kamitani mogliby osiągnąć
w przyszłości

390
00:17:09,160 --> 00:17:11,328
dzięki czytaniu w myślach,

391
00:17:11,329 --> 00:17:13,697
są trudne
do pełnego zrozumienia.

392
00:17:13,698 --> 00:17:15,399
Ale zwolnijmy
na chwilę,

393
00:17:15,400 --> 00:17:17,233
bo mówimy
o technologii,

394
00:17:17,234 --> 00:17:21,105
która może nas
poznać lepiej niż my sami.

395
00:17:21,106 --> 00:17:23,740
Czy naprawdę
powinniśmy to robić?

396
00:17:23,741 --> 00:17:25,275
Cóż, żeby odpowiedzieć na
to pytanie,

397
00:17:25,276 --> 00:17:27,310
spotkam się
z ekspertem w dziedzinie etyki,

398
00:17:27,311 --> 00:17:30,447
neuronauki
i sztucznej inteligencji:

399
00:17:30,448 --> 00:17:32,216
Julią Bossmann.

400
00:17:32,217 --> 00:17:34,518
Jest dyrektorem ds. strategii
w Fathom Computing,

401
00:17:34,519 --> 00:17:37,087
członkiem
rady Światowego Forum Ekonomicznego,

402
00:17:37,088 --> 00:17:40,424
absolwentem Uniwersytetu Singularity Raya Kurzweila


403
00:17:40,425 --> 00:17:43,293
i byłą
przewodniczącą Instytutu Foresight

404
00:17:43,294 --> 00:17:46,530
, think tanku specjalizującego się
w technologiach przyszłości

405
00:17:46,531 --> 00:17:50,200
i ich skutkach.

406
00:17:50,201 --> 00:17:52,669
Julia, dzięki za poświęcenie
czasu na pogawędkę.

407
00:17:52,670 --> 00:17:54,438
-Tak, oczywiście.
-Jesteś

408
00:17:54,439 --> 00:17:55,739
dla mnie idealną osobą do
zadawania tych pytań.

409
00:17:55,740 --> 00:17:57,274
-Mm-hmm.
-I to są głębokie pytania.

410
00:17:57,275 --> 00:17:58,709
Ale myślę, że są
niezwykle ważne

411
00:17:58,710 --> 00:18:00,577
i stają się
coraz bardziej naglące.

412
00:18:00,578 --> 00:18:04,348
Myślę, że żyjemy teraz w
tak interesujących czasach,

413
00:18:04,349 --> 00:18:06,316
ponieważ żyjemy w czasach, w
których mózgi i maszyny

414
00:18:06,317 --> 00:18:07,751

zbliżają się do siebie.

415
00:18:07,752 --> 00:18:10,020
Więc jeśli chodzi
o

416
00:18:10,021 --> 00:18:12,356
możliwość spojrzenia na aktywność mózgu,

417
00:18:12,357 --> 00:18:15,292
gdzie są
tutaj linie etyczne?

418
00:18:15,293 --> 00:18:17,327
Jak prywatne powinny być
moje wewnętrzne myśli?

419
00:18:17,328 --> 00:18:19,663
Jak w przypadku każdej potężnej
technologii

420
00:18:19,664 --> 00:18:22,266
, zależy to od rąk,
które nią posługują.

421
00:18:22,267 --> 00:18:24,401
Wszystkie te nowe technologie

422
00:18:24,402 --> 00:18:28,472
to rzeczy, które mogą sprawić, że każdy, kto
ich użyje, będzie potężniejszy.

423
00:18:28,473 --> 00:18:32,042
Więc nie
chcemy obwiniać technologii, ale chcemy --

424
00:18:32,043 --> 00:18:33,477
jak jest używana

425
00:18:33,478 --> 00:18:35,445
i kto jej używa?

426
00:18:35,446 --> 00:18:37,748
Jak więc upewnić się,
że ta technologia

427
00:18:37,749 --> 00:18:39,416
jest we właściwych rękach?

428
00:18:39,417 --> 00:18:41,718
Myślę więc, że bardzo ważne jest,
aby zaangażować ludzi,

429
00:18:41,719 --> 00:18:45,322
którzy działają w zakresie polityki i prawa,

430
00:18:45,323 --> 00:18:48,592
aby zrozumieć, co nadchodzi
w przyszłości.

431
00:18:48,593 --> 00:18:51,528
Mam nadzieję, że
chodzi o aspekt współpracy.

432
00:18:51,529 --> 00:18:53,430
Porozmawiajmy teraz
o dobrych rzeczach.

433
00:18:53,431 --> 00:18:56,133
Mam na myśli, jakie
są tutaj aplikacje?

434
00:18:56,134 --> 00:18:58,135
Tak, więc jeśli pomyślimy na przykład

435
00:18:58,136 --> 00:18:59,803
o zmarłym Stephenie Hawkingu,


436
00:18:59,804 --> 00:19:04,608
gdyby miał sposób na bogatsze
komunikowanie się ze światem

437
00:19:04,609 --> 00:19:06,643
lub komputerami,
możemy sobie tylko wyobrazić,

438
00:19:06,644 --> 00:19:08,645
czym mógłby się
z nami podzielić.

439
00:19:08,646 --> 00:19:10,647
Ci z zespołem zamknięcia,
prawda?

440
00:19:10,648 --> 00:19:13,584
Oni są tam.
Wiedzą, że tam są.

441
00:19:13,585 --> 00:19:15,786
Ale potrzebujemy tylko czegoś,
co zajrzy do ich mózgu,

442
00:19:15,787 --> 00:19:17,788
żeby zobaczyć,
co chcą powiedzieć,

443
00:19:17,789 --> 00:19:20,657
albo co czują.
-Dokładnie.

444
00:19:20,658 --> 00:19:22,793
A więc co powiesz
ludziom

445
00:19:22,794 --> 00:19:26,396
, którzy boją
się technologii

446
00:19:26,397 --> 00:19:31,802
, że poddajemy swoje prawdziwe
naturalne ja technologii?

447
00:19:31,803 --> 00:19:36,640
Jest coś kuszącego
w wchodzeniu na wyższy poziom

448
00:19:36,641 --> 00:19:40,177
tego, co niektórzy
nazywają ewolucją człowieka

449
00:19:40,178 --> 00:19:43,280
lub rozwojem cywilizacji
i tak dalej.

450
00:19:43,281 --> 00:19:46,250
W pewnym sensie już nie
żyjemy naturalnym życiem, prawda?

451
00:19:46,251 --> 00:19:49,586
Bo wtedy większość z nas
umarłaby przed,

452
00:19:49,587 --> 00:19:51,822
nie wiem, 30 czy 40 rokiem życia.

453
00:19:51,823 --> 00:19:53,490
Mielibyśmy wszelkiego
rodzaju choroby.

454
00:19:53,491 --> 00:19:55,492
Nie nosilibyśmy
tego ubrania.

455
00:19:55,493 --> 00:19:58,161
Nie mielibyśmy okularów
ani soczewek kontaktowych.

456
00:19:58,162 --> 00:19:59,763
Nie mielibyśmy antybiotyków.

457
00:19:59,764 --> 00:20:03,233
[Julia]
Już jesteśmy

458
00:20:03,234 --> 00:20:05,335
bardzo futurystycznymi cyborgami,
jeśli porównamy się

459
00:20:05,336 --> 00:20:08,805
do człowieka, który żył
10 000 lat temu

460
00:20:08,806 --> 00:20:10,574
i był genetycznie
prawie identyczny

461
00:20:10,575 --> 00:20:11,808
z tym, kim jesteśmy teraz.

462
00:20:11,809 --> 00:20:17,748
[Michael]
Tak, naprawdę jesteśmy.

463
00:20:17,749 --> 00:20:19,549
Aby zrozumieć
poznanie, w

464
00:20:19,550 --> 00:20:22,686
tej chwili musimy
po prostu poprosić ludzi,

465
00:20:22,687 --> 00:20:24,321
aby porozmawiali o
tym, co myślą,

466
00:20:24,322 --> 00:20:26,556
lub obserwować ich zachowanie.

467
00:20:26,557 --> 00:20:30,360
Ale bezpośrednie czytanie myśli
byłoby o wiele lepsze.

468
00:20:30,361 --> 00:20:33,630
Tak dr Kuhl
bada pamięć

469
00:20:33,631 --> 00:20:38,402
i tak dr Kamitani
bada sen i sny.

470
00:20:38,403 --> 00:20:40,671
Ale mimo że technologia
ma przed sobą długą drogę,

471
00:20:40,672 --> 00:20:43,106
łatwo dostrzec,
jak kwestie etyczne

472
00:20:43,107 --> 00:20:44,841
mogą stać się problemem.

473
00:20:44,842 --> 00:20:47,110
Cóż, oto rzecz:

474
00:20:47,111 --> 00:20:51,782
nie ma czegoś takiego
jak całkowicie dziki człowiek.

475
00:20:51,783 --> 00:20:55,886
Współewoluujemy
z technologią.

476
00:20:55,887 --> 00:21:00,090
Ludzie i technologia
są dziś nierozłączne.

477
00:21:00,091 --> 00:21:01,725
To prawda, że
 uważać na ka

478
00:21:01,726 --> 00:21:03,760
�dą nową rzecz, którą robimy, ale n

479
00:21:03,761 --> 00:21:08,332
e możemy zmienić faktu, że o
e się wydar

480
00:21:08,333 --> 00:21:11,635
ą.  To historia, którą przeżyliśmy
raz za razem.

481
00:21:11,636 --> 00:21:14,171
Wiesz, moglibyśmy
siedzieć bez końca,

482
00:21:14,172 --> 00:21:16,807
debatując, czy
ograniczenie prędkości powinno istnieć

483
00:21:16,808 --> 00:21:19,743
i kto powinien
mieć uprawnienia do jego egzekwowania.

484
00:21:19,744 --> 00:21:21,445
Ale nie zrobiliśmy tego.

485
00:21:21,446 --> 00:21:24,715
Zamiast tego poszliśmy naprzód
, wymyśliliśmy samochody

486
00:21:24,716 --> 00:21:29,252
i odpowiedzialnie
dopracowywaliśmy szczegóły.

487
00:21:29,253 --> 00:21:31,254
Pytania etyczne
dotyczące nowych technologii

488
00:21:31,255 --> 00:21:35,726
przynoszą najwięcej korzyści,
gdy ułatwiają rozwój technologii, a

489
00:21:35,727 --> 00:21:40,130
nie wtedy, gdy niepotrzebnie
utrudniają postęp.

490
00:21:40,131 --> 00:21:41,498
Więc podążaj za swoimi marzeniami.

491
00:21:41,499 --> 00:21:44,634
I jak najszybciej
pokaż mi je.

492
00:21:44,635 --> 00:21:47,606
I jak zawsze
dzięki za oglądanie.


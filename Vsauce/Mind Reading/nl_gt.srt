1
00:00:05,237 --> 00:00:06,438
Gedachten lezen?

2
00:00:06,439 --> 00:00:08,172
Natuurlijk niet.

3
00:00:08,173 --> 00:00:11,376
Ik hou van lezen.

4
00:00:11,377 --> 00:00:15,413
Kijk, gedachten lezen klinkt misschien
als pseudowetenschappelijk--

5
00:00:15,414 --> 00:00:16,614
excuseer mijn

6
00:00:16,615 --> 00:00:18,416
taal-- bullshoot.

7
00:00:18,417 --> 00:00:21,486
Maar zijn wetenschappelijke tegenhanger,
gedachte-identificatie,

8
00:00:21,487 --> 00:00:23,855
is heel erg echt.

9
00:00:23,856 --> 00:00:26,524
Het is gebaseerd op neuroimaging
en machine learning,

10
00:00:26,525 --> 00:00:29,861
en wat echt cool is, is
dat experimenten in gedachten

11
00:00:29,862 --> 00:00:33,498
lezen niet alleen gaan over het bespioneren
van wat iemand denkt.

12
00:00:33,499 --> 00:00:37,602
Ze gaan over het uitzoeken
waar gedachten van gemaakt zijn.

13
00:00:37,603 --> 00:00:39,504
Ik bedoel, als ik
aan iets denk,

14
00:00:39,505 --> 00:00:43,174
hoe ziet dat mentale beeld er dan
eigenlijk uit?

15
00:00:43,175 --> 00:00:44,709
In welke resolutie zit het?

16
00:00:44,710 --> 00:00:47,145
Hoe betrouwbaar
is een herinnering

17
00:00:47,146 --> 00:00:49,380
en hoe veranderen ze in de
loop van de tijd?

18
00:00:49,381 --> 00:00:51,116
In deze aflevering

19
00:00:51,117 --> 00:00:52,750
ga ik kijken hoe het
lezen van de gedachten van mensen

20
00:00:52,751 --> 00:00:54,719
ons kan helpen
deze vragen te beantwoorden.

21
00:00:54,720 --> 00:00:58,490
Mijn reis begint hier
aan de Universiteit van Oregon.

22
00:00:58,491 --> 00:01:01,259
Ik heb een afspraak met Dr. Brice Kuhl
van het Kuhl-lab.

23
00:01:01,260 --> 00:01:03,394
Hij is een neurowetenschapper
die neuroimaging

24
00:01:03,395 --> 00:01:06,664
en machine learning gebruikt om erachter te
komen wat mensen denken

25
00:01:06,665 --> 00:01:31,789
zonder dat ze het hem vertellen.

26
00:01:31,790 --> 00:01:33,625
Dus vertel me wat
je hier doet.

27
00:01:33,626 --> 00:01:36,794
Ik zit hier in het cognitieve
neurowetenschappelijke programma

28
00:01:36,795 --> 00:01:38,396
en ik bestudeer het menselijk geheugen.

29
00:01:38,397 --> 00:01:41,099
Mijn lab maakt voornamelijk gebruik van
neuroimaging-methoden,

30
00:01:41,100 --> 00:01:42,567
dus we doen veel werk met behulp van

31
00:01:42,568 --> 00:01:44,169
functionele magnetische
resonantiebeeldvorming

32
00:01:44,170 --> 00:01:45,637
of fMRI.

33
00:01:45,638 --> 00:01:49,340
En hoe gebruik je
fMRI om herinneringen te onderzoeken?

34
00:01:49,341 --> 00:01:51,776
We kijken naar het patroon
van neurale activiteit.

35
00:01:51,777 --> 00:01:54,445
Als je een herinnering vormt, is
er een bepaald patroon.

36
00:01:54,446 --> 00:01:56,548
En we kunnen
dat patroon opnemen

37
00:01:56,549 --> 00:01:59,617
en dan testen of
dat patroon

38
00:01:59,618 --> 00:02:02,554
op een later moment wordt hersteld of opnieuw geactiveerd,
bijvoorbeeld wanneer u het zich herinnert.

39
00:02:02,555 --> 00:02:05,823
Betekent dit dat we naar
de patronen van hersenactiviteit kunnen kijken

40
00:02:05,824 --> 00:02:10,061
en kunnen afleiden wat het is dat
wordt herinnerd, of herinnerd,

41
00:02:10,062 --> 00:02:11,262
of zelfs maar gedacht?

42
00:02:11,263 --> 00:02:13,631
Ja, en dus noemen we dat
decoderen.

43
00:02:13,632 --> 00:02:16,568
Dus het neemt in feite
uw

44
00:02:16,569 --> 00:02:18,469
invoerpatroon als een activiteitspatroon
dat we opnemen

45
00:02:18,470 --> 00:02:21,272
terwijl u zich
iets herinnert.

46
00:02:21,273 --> 00:02:23,441
En we doen een voorspelling
over wat je je herinnert.

47
00:02:23,442 --> 00:02:27,178
Je kunt zien hoe dit klinkt
als gedachten lezen.

48
00:02:27,179 --> 00:02:28,713
[lacht]
Ja.  Zo klinkt het.

49
00:02:28,714 --> 00:02:32,584
Dus, Brice, wat ga
je vandaag met me doen?

50
00:02:32,585 --> 00:02:34,419
Dus wat we
vandaag gaan doen

51
00:02:34,420 --> 00:02:36,354
is onbekend terrein
voor ons.

52
00:02:36,355 --> 00:02:38,623
Dus we gaan
een soort nieuwe variant

53
00:02:38,624 --> 00:02:40,258
van het experiment op jou uitproberen.

54
00:02:40,259 --> 00:02:42,660
Ik kan dus
geen specifieke resultaten garanderen.

55
00:02:42,661 --> 00:02:44,195
Maar het geeft aan
waar het veld is

56
00:02:44,196 --> 00:02:46,664
en waar
we heen willen.

57
00:02:46,665 --> 00:02:48,800
Vandaag ga je
deelnemen aan een experiment

58
00:02:48,801 --> 00:02:50,268
waarbij je gezichten bestudeert.

59
00:02:50,269 --> 00:02:51,803
Dus we
laten je

60
00:02:51,804 --> 00:02:53,404
12 foto's van beroemdheden bestuderen.

61
00:02:53,405 --> 00:02:54,872
Mensen die ik al
ken.

62
00:02:54,873 --> 00:02:56,674
- Mensen die je kent, ja.
-Oké.

63
00:02:56,675 --> 00:02:59,143
En je gaat proberen
die foto's te onthouden.

64
00:02:59,144 --> 00:03:01,112
Dan laten we je
naar de MRI-scanner gaan.

65
00:03:01,113 --> 00:03:04,249
Probeer dat
beeld zo levendig mogelijk voor de geest te halen.

66
00:03:04,250 --> 00:03:06,417
En we gaan je hersenactiviteit opnemen


67
00:03:06,418 --> 00:03:08,753
terwijl je je deze foto's probeert voor te stellen
.

68
00:03:08,754 --> 00:03:10,555
We gaan proberen
het gezicht op te bouwen.

69
00:03:10,556 --> 00:03:12,590
Maak in wezen een foto van
wat je je herinnert.

70
00:03:12,591 --> 00:03:14,125
-Een foto?
-Een foto.

71
00:03:14,126 --> 00:03:16,327
Een echte foto
die we kunnen printen

72
00:03:16,328 --> 00:03:17,729
en die ik zo
aan mijn muur zou kunnen hangen.

73
00:03:17,730 --> 00:03:19,831
[lacht]
Als je wilde.

74
00:03:19,832 --> 00:03:22,634
[Michael]De eerste stap
is dat ik

75
00:03:22,635 --> 00:03:25,336
de 12 specifieke foto's van
beroemdheden uit mijn hoofd

76
00:03:25,337 --> 00:03:28,640
leer die Brice later zal proberen
te ontdekken waar ik aan denk.

77
00:03:28,641 --> 00:03:33,278
Ik ging zitten om deze
afgestudeerde student, Max, te doen.

78
00:03:33,279 --> 00:03:35,280
Het succes van zijn voorspellingen
hangt gedeeltelijk af

79
00:03:35,281 --> 00:03:37,415
van mijn vermogen
om me deze gezichten

80
00:03:37,416 --> 00:03:43,087
zo levendig mogelijk voor
de geest te halen terwijl ik in de fMRI zit.

81
00:03:43,088 --> 00:03:44,422
Oké, dus...

82
00:03:44,423 --> 00:03:46,157
[zucht]

83
00:03:46,158 --> 00:03:50,728
Ik denk dat ik een redelijk goede
herinnering aan al die dingen heb.

84
00:03:50,729 --> 00:03:53,698
-Super goed.
- Ik heb het gevoel dat er veel op het spel staat.

85
00:03:53,699 --> 00:03:56,701
Nu de gezichten van beroemdheden
hopelijk zijn onthouden,

86
00:03:56,702 --> 00:03:58,303
is het tijd voor de volgende stap

87
00:03:58,304 --> 00:04:00,071
: door
de metaaldetector gaan

88
00:04:00,072 --> 00:04:01,706
en de fMRI in,

89
00:04:01,707 --> 00:04:04,676
waar Brice
mijn hersenactiviteit zal opnemen en monitoren,

90
00:04:04,677 --> 00:04:08,746
en deze later in zijn
algoritme zal invoeren om de gezichten opnieuw op te bouwen.

91
00:04:08,747 --> 00:04:10,448
Dit zal de eerste keer zijn dat
hij probeert

92
00:04:10,449 --> 00:04:12,383
gezichten te reconstrueren
uit het langetermijngeheugen,

93
00:04:12,384 --> 00:04:14,385
wat erg moeilijk is,
omdat we erop

94
00:04:14,386 --> 00:04:16,720
vertrouwen hoe goed ik me
de foto's van beroemdheden kan herinneren die

95
00:04:16,721 --> 00:04:19,090
ik een uur geleden heb gezien.

96
00:04:19,091 --> 00:04:21,225
Ik hou van zijn ogen.
Moet je zien.

97
00:04:21,226 --> 00:04:24,362
[vrouw]

98
00:04:24,363 --> 00:04:31,102
Zou het kind niet zeggen:
"Het gaat me opeten"?

99
00:04:31,103 --> 00:04:34,205
Een fMRI bewaakt de activiteit
in de hersenen

100
00:04:34,206 --> 00:04:36,774
door deze op te delen
in duizenden kleine kubussen

101
00:04:36,775 --> 00:04:39,744
die voxels
of volumetrische pixels worden genoemd.

102
00:04:39,745 --> 00:04:41,446
Elk van deze voxels bevat

103
00:04:41,447 --> 00:04:43,581

honderdduizenden neuronen.

104
00:04:43,582 --> 00:04:46,117
Met fMRI
kunnen we

105
00:04:46,118 --> 00:04:47,719
de bloedstroom
binnen deze voxels detecteren,

106
00:04:47,720 --> 00:04:50,088
wat betekent dat dat deel
van de hersenen actief is.

107
00:04:50,089 --> 00:04:53,124
Als ik meerdere foto's
van mensen met snorren te zien krijg,

108
00:04:53,125 --> 00:04:56,327
reageren mijn hersenen
op de kenmerken van elk gezicht.

109
00:04:56,328 --> 00:04:58,329
Maar er zal
een gemeenschappelijk deel van mijn hersenen zijn

110
00:04:58,330 --> 00:05:00,164
dat
overal mee bezig is.

111
00:05:00,165 --> 00:05:04,102
Dat is misschien het deel van mijn
hersenen dat reageert op snorren.

112
00:05:04,103 --> 00:05:07,171
Dus later,
als ik me een gezicht voorstel,

113
00:05:07,172 --> 00:05:09,440
als Brice merkt
dat dat gebied bezet is,

114
00:05:09,441 --> 00:05:11,476
kan hij voorspellen
dat ik

115
00:05:11,477 --> 00:05:13,811
aan een snor denk.

116
00:05:13,812 --> 00:05:15,780
Dus op dit moment zit Michael
in de scanner,

117
00:05:15,781 --> 00:05:18,282
en hij ziet woorden
één voor één op het scherm verschijnen,

118
00:05:18,283 --> 00:05:20,651
en hij probeert
het gezicht te visualiseren,

119
00:05:20,652 --> 00:05:23,187
het gezicht zo
gedetailleerd mogelijk te onthouden.

120
00:05:23,188 --> 00:05:25,356
Wat u hier kunt zien, zijn
de afbeeldingen die we verwerven.

121
00:05:25,357 --> 00:05:28,793
We krijgen
elke twee seconden een van deze hersenvolumes.

122
00:05:28,794 --> 00:05:32,797
Deze zijn dus in realtime verfrissend
terwijl we de afbeeldingen verzamelen.

123
00:05:32,798 --> 00:05:35,633
[Michael]Nu deel één
van de fMRI-sessie voorbij is,

124
00:05:35,634 --> 00:05:38,503
is het tijd voor deel twee,
waarin Brice en zijn

125
00:05:38,504 --> 00:05:41,773
team de taal
van mijn hersenactiviteit leren,

126
00:05:41,774 --> 00:05:44,776
zodat ze later kunnen
decoderen met hersenscans.

127
00:05:44,777 --> 00:05:46,544
Hoi Michael.
Gaat het nog steeds goed met je?

128
00:05:46,545 --> 00:05:48,312
[Michael]
Ja.

129
00:05:48,313 --> 00:05:50,381
Ze laten me
honderden unieke gezichten zien

130
00:05:50,382 --> 00:05:52,784
en leggen vast hoe mijn hersenen reageren

131
00:05:52,785 --> 00:05:54,852
op bepaalde
gezichtskenmerken.

132
00:05:54,853 --> 00:05:57,188
Ze zullen deze informatie vervolgens gebruiken


133
00:05:57,189 --> 00:05:59,657
om
de gezichten van beroemdheden te reconstrueren waar

134
00:05:59,658 --> 00:06:03,127
ik aan dacht tijdens
de eerste fase van de scan.

135
00:06:03,128 --> 00:06:05,530
Echt, hoe meer gezichten
we Michael kunnen laten zien, hoe beter.

136
00:06:05,531 --> 00:06:08,166
Dus we gaan hem daar in principe houden


137
00:06:08,167 --> 00:06:09,600
zolang hij comfortabel is.

138
00:06:09,601 --> 00:06:11,636
[Michael]
Twee uur was de maximale tijd die

139
00:06:11,637 --> 00:06:13,538
we in de fMRI konden krijgen.

140
00:06:13,539 --> 00:06:17,175
Maar ik kon naar
meer dan 400 gezichten kijken,

141
00:06:17,176 --> 00:06:18,743
wat genoeg zou moeten zijn

142
00:06:18,744 --> 00:06:20,778
om behoorlijk interessante
resultaten te krijgen.

143
00:06:20,779 --> 00:06:22,447
Hé, Michael, het is je gelukt.
Dat was geweldig.

144
00:06:22,448 --> 00:06:23,681
We
komen je eruit halen.

145
00:06:23,682 --> 00:06:33,157
[Michael]
Goed.

146
00:06:33,158 --> 00:06:34,725
Ja, dus deze tonen slechts
enkele van de foto's

147
00:06:34,726 --> 00:06:36,561
die we maakten
terwijl jij daar was.

148
00:06:36,562 --> 00:06:38,095
Enkele beelden van je hersenen.

149
00:06:38,096 --> 00:06:39,764
Nu gaan
we wat cijfers kraken.

150
00:06:39,765 --> 00:06:42,200
Max gaat
je gegevens analyseren.

151
00:06:42,201 --> 00:06:43,701
We zien elkaar
morgen weer,

152
00:06:43,702 --> 00:06:45,369
waar we
naar de resultaten zullen kijken,

153
00:06:45,370 --> 00:06:47,638
waar we proberen
de gezichtsbeelden te reconstrueren

154
00:06:47,639 --> 00:06:49,740
uit de hersengegevens
die we zojuist hebben verzameld.

155
00:06:49,741 --> 00:06:51,175
Oke.
Zie je morgen.

156
00:06:51,176 --> 00:06:52,577
Oke.
Heel erg bedankt.

157
00:06:52,578 --> 00:06:54,178
Max, jij ook bedankt.
Ik kan niet wachten.

158
00:06:54,179 --> 00:06:55,847
Je kunt beter
een nachtdienst trekken.

159
00:06:55,848 --> 00:07:04,288
Ik wil dat deze
gegevens perfect zijn.

160
00:07:04,289 --> 00:07:06,524
Oké, dus ik ben terug
in het lab van Dr. Kuhl.

161
00:07:06,525 --> 00:07:08,693
Van de ene op de andere dag heeft zijn team
de gegevens doorgenomen

162
00:07:08,694 --> 00:07:15,500
en ik kan niet wachten om te zien wat
ze denken dat ze me zagen denken.

163
00:07:15,501 --> 00:07:17,101
Hoe zijn mijn resultaten?

164
00:07:17,102 --> 00:07:18,736
Ik vind ze er goed uitzien.

165
00:07:18,737 --> 00:07:20,705
We gaan hier zo
even kijken.

166
00:07:20,706 --> 00:07:22,406
Oké,
ik kan niet wachten.

167
00:07:22,407 --> 00:07:24,342
- Mag ik dan gewoon gaan zitten?
-Ja, ga zitten.

168
00:07:24,343 --> 00:07:26,143
Oké, dus... ten

169
00:07:26,144 --> 00:07:28,212
eerste...

170
00:07:28,213 --> 00:07:30,081
wat zie ik?


171
00:07:30,082 --> 00:07:32,283
Dit zijn de foto's die
ik eigenlijk uit mijn hoofd heb geleerd.

172
00:07:32,284 --> 00:07:34,252
-Dat klopt.
-En dit is wat

173
00:07:34,253 --> 00:07:37,822
je
uit mijn verbeelding hebt gereconstrueerd.

174
00:07:37,823 --> 00:07:40,091
-Dat klopt.
-Oh wow.  Oké.

175
00:07:40,092 --> 00:07:43,094
[Brice]
Oké, dit is dus een
van de reconstructies

176
00:07:43,095 --> 00:07:44,562
die is gegenereerd.

177
00:07:44,563 --> 00:07:46,063
[Michael]
Interessant.

178
00:07:46,064 --> 00:07:47,698
[Max]
Dus dat is John Cho.

179
00:07:47,699 --> 00:07:50,668
[Michael]
Niet slecht.  Niet slecht.

180
00:07:50,669 --> 00:07:53,337
-Kunnen we ze naast elkaar zien?
-Ja.

181
00:07:53,338 --> 00:07:55,673
[Michael]
Ik zie, weet je, overeenkomsten

182
00:07:55,674 --> 00:08:00,044
in het soort
gezichtsuitdrukkingen in het algemeen.

183
00:08:00,045 --> 00:08:02,213
Weet je, je zou
de haarlijn hier bijna kunnen zien.

184
00:08:02,214 --> 00:08:04,682

Ik dacht ook

185
00:08:04,683 --> 00:08:06,684
dat de vorm van het gezicht... Het had
een soort vierkante vorm.

186
00:08:06,685 --> 00:08:08,152
-Ja.  Ja.
-Dus dat zijn de dingen

187
00:08:08,153 --> 00:08:09,387
die bij me opkwamen.

188
00:08:09,388 --> 00:08:11,289
En dus toen ik


189
00:08:11,290 --> 00:08:13,257
dit beeld van John Cho visualiseerde, was

190
00:08:13,258 --> 00:08:16,260
de vierkantheid van het gezicht
het eerste, meest opvallende ding.

191
00:08:16,261 --> 00:08:19,697
Ik bleef maar denken,
hij was de vierkante man.

192
00:08:19,698 --> 00:08:23,401
Uitstekend, oké.

193
00:08:23,402 --> 00:08:26,704
[Brice]
Dus dat is Megan Fox.

194
00:08:26,705 --> 00:08:28,439
[Michael]
Mm-hmm.

195
00:08:28,440 --> 00:08:30,207
Je gaat ons de...
naast elkaar laten zien.

196
00:08:30,208 --> 00:08:31,776
[Michael]
Zij aan zij.  Rechts.

197
00:08:31,777 --> 00:08:33,644
[Brice]
Je kunt de foto zien die
je echt zag,

198
00:08:33,645 --> 00:08:36,547
en dat is de reconstructie die
we hebben gemaakt.

199
00:08:36,548 --> 00:08:39,417
Ik zal je dit geven.
Megan Fox, ik kon

200
00:08:39,418 --> 00:08:42,353
me geen echt duidelijk beeld
voor ogen hebben.

201
00:08:42,354 --> 00:08:45,056
Om de een of andere reden kon ik dit beeld
van haar heel moeilijk

202
00:08:45,057 --> 00:08:47,058
in mijn gedachten terugbrengen.

203
00:08:47,059 --> 00:08:50,595
De strengheid in het gezicht was
iets dat ik wel oppikte.

204
00:08:50,596 --> 00:08:53,698
Dus ik voelde dat er...
Het zag er vrouwelijk uit.

205
00:08:53,699 --> 00:08:55,533
En je
pikte de strengheid op.

206
00:08:55,534 --> 00:08:58,769
En zo samen
levert dat een match op.

207
00:08:58,770 --> 00:09:00,538
[Michael]Houd er rekening mee
dat Brice en zijn team

208
00:09:00,539 --> 00:09:02,773
deze
uit mijn geheugen hebben gelezen.

209
00:09:02,774 --> 00:09:04,609
Maar als ik me een gezicht herinner,

210
00:09:04,610 --> 00:09:07,678
stel ik me dan elk detail
tegelijk

211
00:09:07,679 --> 00:09:09,313
met fotografische nauwkeurigheid voor?

212
00:09:09,314 --> 00:09:10,748
Of moet ik er maar een paar
tegelijk behandelen?

213
00:09:10,749 --> 00:09:13,417
Door mijn gedachten te lezen,
kunnen ze zien

214
00:09:13,418 --> 00:09:15,219
hoe slecht mijn geheugen is
en hoe het werkt.

215
00:09:15,220 --> 00:09:18,689
-Mij!  Mij!
-[Brice lacht]

216
00:09:18,690 --> 00:09:21,525
Oké, dus dat is je
reconstructie

217
00:09:21,526 --> 00:09:24,729
van mij, denkend aan
dit beeld van mezelf.

218
00:09:24,730 --> 00:09:26,497
[Brice]
Dat klopt.

219
00:09:26,498 --> 00:09:28,666
Waar is de baard gebleven?

220
00:09:28,667 --> 00:09:31,068
[Brice] Ik weet het niet.
Ik hoopte dat je het me kon vertellen.

221
00:09:31,069 --> 00:09:36,240
[Michael]
Dit is bijvoorbeeld een foto
waarop ik mijn eigen gezicht herinner.

222
00:09:36,241 --> 00:09:38,743
Het lijkt niet echt op mij,
maar de vraag is:

223
00:09:38,744 --> 00:09:41,345
hoe goed ben ik
in mezelf voor te stellen?

224
00:09:41,346 --> 00:09:44,048
Ik denk niet zo vaak aan mijn eigen gezicht
,

225
00:09:44,049 --> 00:09:45,616
dus de vreemdheid
in het resultaat

226
00:09:45,617 --> 00:09:47,752
kan net zo goed te maken hebben met fouten
in mijn eigen geheugen

227
00:09:47,753 --> 00:09:51,288
en mentale beeld van mezelf
als fouten in de technologie.

228
00:09:51,289 --> 00:09:53,691
Dus dat is Jennifer Lawrence,
geloof ik.

229
00:09:53,692 --> 00:09:55,726
[Michael]
Dat is Jennifer Lawrence?

230
00:09:55,727 --> 00:09:59,664
Het lijkt erop dat het
de veel oudere oom van Jennifer Lawrence is.

231
00:09:59,665 --> 00:10:01,365
[allen grinniken]

232
00:10:01,366 --> 00:10:05,069
Niets hier was
te verbijsterend dichtbij.

233
00:10:05,070 --> 00:10:09,407
Maar dit is iets dat
je net begint met het proberen van

234
00:10:09,408 --> 00:10:11,409
dit soort
langetermijnherinneringen.

235
00:10:11,410 --> 00:10:14,679
Wat Brice en zijn team
in mijn gedachten lazen, was

236
00:10:14,680 --> 00:10:18,549
misschien nauwkeuriger geweest
als ze me duizenden in

237
00:10:18,550 --> 00:10:20,284
plaats van honderden afbeeldingen
in de fMRI hadden laten zien,

238
00:10:20,285 --> 00:10:22,520
want dan zou het algoritme


239
00:10:22,521 --> 00:10:24,655
de taal van mijn hersenen
grondiger hebben geleerd.

240
00:10:24,656 --> 00:10:27,358
Maar hoe dan ook,
de kwaliteit van mijn herinneringen

241
00:10:27,359 --> 00:10:29,226
zou nog
steeds een probleem zijn geweest.

242
00:10:29,227 --> 00:10:30,661
Ik bedoel, kijk wat er gebeurt
als het geheugen

243
00:10:30,662 --> 00:10:33,164
volledig uit de vergelijking wordt
gehaald.

244
00:10:33,165 --> 00:10:35,166
Brice las ook
mijn hersenactiviteit af

245
00:10:35,167 --> 00:10:37,168
toen ik
naar gezichten keek in de fMRI.

246
00:10:37,169 --> 00:10:39,103
ze niet alleen verbeelden.

247
00:10:39,104 --> 00:10:41,672
En die resultaten
waren veel dichterbij

248
00:10:41,673 --> 00:10:44,675
dan die
uit mijn geheugen gereconstrueerd.

249
00:10:44,676 --> 00:10:47,111
Oké, dus, waar kijk
ik hier naar?

250
00:10:47,112 --> 00:10:48,679
[Brice]
Oké, dus wat je hier

251
00:10:48,680 --> 00:10:51,682
in de bovenste rij ziet,
dit zijn afbeeldingen die je zag

252
00:10:51,683 --> 00:10:53,584
toen je in de scanner zat.

253
00:10:53,585 --> 00:10:56,754
Daaronder, in deze onderste rij
, zijn dit de reconstructies

254
00:10:56,755 --> 00:11:00,725
die we putten uit de patronen
van hersenactiviteit die we verzamelden.

255
00:11:00,726 --> 00:11:03,828
-Dit is van de bronafbeelding.
-Rechts.

256
00:11:03,829 --> 00:11:05,629
[Michael]
Deze komen uit mijn brein.

257
00:11:05,630 --> 00:11:07,565
-[Brice] Juist.
-[Michael] Ze zijn redelijk dichtbij.

258
00:11:07,566 --> 00:11:09,700
Ja, over het algemeen waren ze
redelijk dichtbij.

259
00:11:09,701 --> 00:11:11,602
Dus niet perfect.

260
00:11:11,603 --> 00:11:13,771
Dit zijn... je kunt zien dat er
wat variatie in zit.

261
00:11:13,772 --> 00:11:16,640
Maar dit komt overeen
met wat we eerder hebben gevonden,

262
00:11:16,641 --> 00:11:18,409
dat de reconstructies
die we hebben gegenereerd,

263
00:11:18,410 --> 00:11:20,244
wanneer je
de gezichten bekijkt

264
00:11:20,245 --> 00:11:22,279
, enige overeenkomst is
tussen het werkelijke gezicht.

265
00:11:22,280 --> 00:11:23,714
Dit is dus een
soort geestelijke controle,

266
00:11:23,715 --> 00:11:25,750
dat we
de afbeeldingen

267
00:11:25,751 --> 00:11:28,219
daadwerkelijk kunnen reconstrueren - als je ze bekijkt.
- Juist, juist.

268
00:11:28,220 --> 00:11:31,355
Ze zijn best goed.

269
00:11:31,356 --> 00:11:33,190
Nou, Brice, Max, heel erg


270
00:11:33,191 --> 00:11:35,126
bedankt dat ik
hier deel van mocht uitmaken.

271
00:11:35,127 --> 00:11:36,660
Ik hoop dat mijn gegevens nuttig zijn.

272
00:11:36,661 --> 00:11:38,596
Dank u.
Het is erg leuk geweest.

273
00:11:38,597 --> 00:11:46,837
Het is altijd nuttig voor ons
om over deze dingen na te denken.

274
00:11:46,838 --> 00:11:50,808
Het geheugenonderzoek van dr. Brice Kuhl
toont aan dat het mogelijk is

275
00:11:50,809 --> 00:11:53,677
voor een computer
om iemands gedachten te lezen.

276
00:11:53,678 --> 00:11:56,313
Om erachter te komen
wat ze denken.

277
00:11:56,314 --> 00:11:58,415
Maar er moet nog veel vooruitgang
worden geboekt.

278
00:11:58,416 --> 00:12:00,050
Ik bedoel, als je wilt weten

279
00:12:00,051 --> 00:12:01,619
wat ik nu denk,
bijvoorbeeld,

280
00:12:01,620 --> 00:12:05,189
is het nog steeds makkelijker om me gewoon te vragen het je
te vertellen.

281
00:12:05,190 --> 00:12:07,491
Maar wat als ik het je niet kan
vertellen?

282
00:12:07,492 --> 00:12:10,594
Dr. Yukiyasu Kamitani
is een onderzoeker,

283
00:12:10,595 --> 00:12:14,498
professor en pionier
die de grens

284
00:12:14,499 --> 00:12:17,535
achter de slaapmuur verkent.

285
00:12:17,536 --> 00:12:19,703
Ik ben
naar de universiteit van Kyoto gekomen

286
00:12:19,704 --> 00:12:21,672
om hem te ontmoeten en om te zien
hoe het is

287
00:12:21,673 --> 00:12:24,175
om niet te lezen wat
iemand denkt,

288
00:12:24,176 --> 00:12:29,647
maar wat
iemand droomt.

289
00:12:29,648 --> 00:12:31,348
Kamitani sensei,
ik ben Michael.

290
00:12:31,349 --> 00:12:33,818
- Hallo, ik ben Yuki.
-Yuki, leuk je te ontmoeten.

291
00:12:33,819 --> 00:12:36,320
[Michael]
De afgelopen tien jaar heeft

292
00:12:36,321 --> 00:12:38,455
Dr. Kamitani een voortrekkersrol gespeeld
bij het

293
00:12:38,456 --> 00:12:40,124
lezen van gedachten door machines.

294
00:12:40,125 --> 00:12:43,527
Het onderwerp is, weet je,
klaar om naar binnen te gaan.

295
00:12:43,528 --> 00:12:45,462
Net als Brice Kuhl,

296
00:12:45,463 --> 00:12:48,632
onderzochten zijn vroege experimenten het
reconstrueren van afbeeldingen

297
00:12:48,633 --> 00:12:52,236
die aan proefpersonen werden getoond in een fMRI op
basis van hun hersenactiviteit.

298
00:12:52,237 --> 00:12:53,637
In het geval van Kamitani waren

299
00:12:53,638 --> 00:12:55,706
de afbeeldingen
zwart-witvormen

300
00:12:55,707 --> 00:12:58,375
en waren de
reconstructies opvallend nauwkeurig.

301
00:12:58,376 --> 00:13:03,180
Onlangs heeft Kamitani zich gericht
op het gebruik van diepe neurale netwerken

302
00:13:03,181 --> 00:13:04,815
en machine learning

303
00:13:04,816 --> 00:13:06,450
om de hersenactiviteit van proefpersonen te ontcijferen


304
00:13:06,451 --> 00:13:08,686
terwijl ze
veel complexere foto's bekijken.

305
00:13:08,687 --> 00:13:12,756
Wat je ziet is het
resultaat van een diep neuraal netwerk dat

306
00:13:12,757 --> 00:13:15,226
de hersenactiviteit verwerkt
van een onderwerp

307
00:13:15,227 --> 00:13:17,795
dat naar de foto kijkt.

308
00:13:17,796 --> 00:13:20,431
Dit kan
in de toekomst talloze toepassingen hebben,

309
00:13:20,432 --> 00:13:22,733
bijvoorbeeld
in strafrechtelijke onderzoeken

310
00:13:22,734 --> 00:13:26,470
en interpersoonlijke
communicatie.

311
00:13:26,471 --> 00:13:28,739
[Kamitani]
Dit is verre van perfect.

312
00:13:28,740 --> 00:13:33,177
Maar ik denk dat je nog steeds wat,
je weet wel, ogen ziet en, weet je...

313
00:13:33,178 --> 00:13:34,778
[Michael]
Nou, ja.

314
00:13:34,779 --> 00:13:36,447
En kleuren ook.

315
00:13:36,448 --> 00:13:39,783
[Kamitani]
Ja, tot op zekere hoogte, ja.

316
00:13:39,784 --> 00:13:42,419
Zijn meest actuele werk gaat
echter over het onderbewuste.

317
00:13:42,420 --> 00:13:45,256
Hij probeert iets
heel ambitieus:

318
00:13:45,257 --> 00:13:46,824
onze dromen vastleggen.

319
00:13:46,825 --> 00:13:49,326
Zou je jezelf
een

320
00:13:49,327 --> 00:13:50,828
slaaponderzoeker of een visieonderzoeker noemen?

321
00:13:50,829 --> 00:13:53,664
Misschien een hersendecoder.

322
00:13:53,665 --> 00:13:55,432
Een hersendecoder.

323
00:13:55,433 --> 00:13:57,534
Dat is een mooie
functieomschrijving.

324
00:13:57,535 --> 00:14:01,739
Kun je me iets laten zien van
wat je met dromen doet?

325
00:14:01,740 --> 00:14:08,379
[Kamitani]
Mm-hmm, ja.

326
00:14:08,380 --> 00:14:10,514
Dr. Kamitani's werk
aan het decoderen van dromen

327
00:14:10,515 --> 00:14:13,317
begint met een soortgelijk proces als
dat van Dr. Kuhl:

328
00:14:13,318 --> 00:14:15,452
de proefpersoon
duizenden afbeeldingen laten zien

329
00:14:15,453 --> 00:14:17,187
terwijl ze in een fMRI

330
00:14:17,188 --> 00:14:18,722
zitten om te leren
hoe de hersenen eruit zien

331
00:14:18,723 --> 00:14:21,358
als ze
aan bepaalde dingen denken.

332
00:14:21,359 --> 00:14:23,794
Zodra het machine learning-
algoritme redelijk goed

333
00:14:23,795 --> 00:14:26,764
kan identificeren aan welke beelden
het onderwerp denkt, wordt

334
00:14:26,765 --> 00:14:29,300
het onderwerp
in een fMRI geplaatst

335
00:14:29,301 --> 00:14:31,335
met een EEG-dop
op hun hoofd

336
00:14:31,336 --> 00:14:33,470
en uitgenodigd om in slaap te vallen.

337
00:14:33,471 --> 00:14:36,573
Wanneer de EEG-golven aangeven
dat de persoon droomt,

338
00:14:36,574 --> 00:14:39,243
voorspelt het algoritme over
welke dingen

339
00:14:39,244 --> 00:14:41,645
het onderwerp het meest waarschijnlijk
droomt.

340
00:14:41,646 --> 00:14:45,215
Op dit moment zoekt het algoritme
naar 20 categorieën.

341
00:14:45,216 --> 00:14:48,485
Dingen zoals gebouwen,
transport

342
00:14:48,486 --> 00:14:50,621
en karakters
in een taal.

343
00:14:50,622 --> 00:14:53,324
Onderzoekers maken
de proefpersoon dan wakker,

344
00:14:53,325 --> 00:14:55,326
vragen waar ze
over droomden

345
00:14:55,327 --> 00:14:57,227
en kijken of de voorspelling van het algoritme


346
00:14:57,228 --> 00:14:59,530
en de
herinnering van de persoon overeenkomen.

347
00:14:59,531 --> 00:15:03,367
Hier zijn feitelijke gegevens van
een van Kamitani's experimenten.

348
00:15:03,368 --> 00:15:05,703
Hieronder ziet u een woordwolk
van categorieën.

349
00:15:05,704 --> 00:15:08,339
De naam van elke categorie
wordt in realtime groter of kleiner

350
00:15:08,340 --> 00:15:10,574
op
basis van de kans

351
00:15:10,575 --> 00:15:13,344
dat ze aanwezig zijn
in de huidige droom van het onderwerp.

352
00:15:13,345 --> 00:15:15,346
Zoals u kunt zien, is de
activiteit momenteel het sterkst

353
00:15:15,347 --> 00:15:18,349
voor de categorie 'karakter',
wat betekent geschreven taal.

354
00:15:18,350 --> 00:15:20,684
Op dit punt werd
het onderwerp gewekt,

355
00:15:20,685 --> 00:15:29,660
en dit is wat
ze meldden.

356
00:15:29,661 --> 00:15:32,062
Dat is behoorlijk spookachtig.

357
00:15:32,063 --> 00:15:33,697
-[lacht]
-Toch?  Ik bedoel, jij...

358
00:15:33,698 --> 00:15:36,700
je bespioneerde hun droom.

359
00:15:36,701 --> 00:15:39,370
Ja, op een bepaalde manier.
Maar...

360
00:15:39,371 --> 00:15:42,406
de nauwkeurigheid is
niet zo geweldig, dus...

361
00:15:42,407 --> 00:15:44,341
Nou, de nauwkeurigheid is
niet zo geweldig, maar, weet je,

362
00:15:44,342 --> 00:15:46,677
mijn normale nauwkeurigheid om
de dromen van mensen te raden is nul.

363
00:15:46,678 --> 00:15:48,145
Rechts.

364
00:15:48,146 --> 00:15:49,580
Terwijl hij
zijn onderzoek voortzet

365
00:15:49,581 --> 00:15:52,182
naar het voorspellen van
de inhoud van dromen,

366
00:15:52,183 --> 00:15:54,785
begint Dr. Kamitani
aan zijn nieuwste project: het

367
00:15:54,786 --> 00:15:58,389
daadwerkelijk reconstrueren van beelden
uit onze dromen.

368
00:15:58,390 --> 00:16:01,458
Dus je hebt
wat van de reconstructies meegebracht

369
00:16:01,459 --> 00:16:02,659
die je lab heeft gemaakt...

370
00:16:02,660 --> 00:16:04,061
Mm-hmm.

371
00:16:04,062 --> 00:16:17,741
...van dromen.

372
00:16:17,742 --> 00:16:20,210
Juist, het lijken allemaal dromen
over klodders.

373
00:16:20,211 --> 00:16:21,779
[Kamitani]
Ja.

374
00:16:21,780 --> 00:16:24,448
Ik bedoel, ik wil
even een stapje terug doen en...

375
00:16:24,449 --> 00:16:27,785
waarderen dat waar we
naar kijken op dit

376
00:16:27,786 --> 00:16:31,655
scherm in zekere zin de eerste
foto's van een droom zijn.

377
00:16:31,656 --> 00:16:33,357
Mm-hm.

378
00:16:33,358 --> 00:16:35,759
We kijken
naar de vroegste fase

379
00:16:35,760 --> 00:16:38,062
van revolutionair onderzoek.

380
00:16:38,063 --> 00:16:40,164
Op een dag kunnen
we misschien beelden hebben,

381
00:16:40,165 --> 00:16:42,766
of zelfs films opnemen,
van onze eigen dromen.

382
00:16:42,767 --> 00:16:45,335
En Dr. Kamitani is de enige
persoon ter wereld

383
00:16:45,336 --> 00:16:47,237
die dit tot nu toe doet.

384
00:16:47,238 --> 00:16:50,707
Hij is een eenzame ontdekkingsreiziger die
in ons onderbewustzijn reist.

385
00:16:50,708 --> 00:16:53,644
Dit werk is dus nog niet
eens gepubliceerd.

386
00:16:53,645 --> 00:16:55,345
Nee.

387
00:16:55,346 --> 00:17:01,752
Bedankt dat je het me hebt laten zien.
-[lacht]

388
00:17:01,753 --> 00:17:06,155
De inzichten die onderzoekers
als Dr. Kuhl en Dr. Kamitani in de toekomst

389
00:17:06,156 --> 00:17:09,159
zouden kunnen bereiken


390
00:17:09,160 --> 00:17:11,328
vanwege gedachtenlezen,

391
00:17:11,329 --> 00:17:13,697
zijn
moeilijk volledig te doorgronden.

392
00:17:13,698 --> 00:17:15,399
Maar laten we het even wat rustiger aan doen
,

393
00:17:15,400 --> 00:17:17,233
want we hebben het
over een technologie

394
00:17:17,234 --> 00:17:21,105
die ons beter kan kennen
dan wij onszelf kennen.

395
00:17:21,106 --> 00:17:23,740
Moeten we dit
echt doen?

396
00:17:23,741 --> 00:17:25,275
Om
die vraag te beantwoorden,

397
00:17:25,276 --> 00:17:27,310
ga ik
een deskundige ontmoeten in ethiek,

398
00:17:27,311 --> 00:17:30,447
neurowetenschappen
en kunstmatige intelligentie:

399
00:17:30,448 --> 00:17:32,216
Julia Bossmann.

400
00:17:32,217 --> 00:17:34,518
Ze is de directeur strategie
bij Fathom Computing,

401
00:17:34,519 --> 00:17:37,087
een raadslid
van het World Economic Forum,

402
00:17:37,088 --> 00:17:40,424
een alumnus van Ray Kurzweils
Singularity University,

403
00:17:40,425 --> 00:17:43,293
en een voormalig president
van het Foresight Institute,

404
00:17:43,294 --> 00:17:46,530
een denktank die gespecialiseerd is
in toekomstige technologieën

405
00:17:46,531 --> 00:17:50,200
en hun impact.

406
00:17:50,201 --> 00:17:52,669
Julia, bedankt dat je even de
tijd hebt genomen om te chatten.

407
00:17:52,670 --> 00:17:54,438
-Ja natuurlijk.
-Jij bent de perfecte persoon

408
00:17:54,439 --> 00:17:55,739
voor mij om
deze vragen te stellen.

409
00:17:55,740 --> 00:17:57,274
-Mm-hm.
-En het zijn diepe vragen.

410
00:17:57,275 --> 00:17:58,709
Maar ik denk dat ze
buitengewoon belangrijk zijn,

411
00:17:58,710 --> 00:18:00,577
en ze worden
steeds dringender.

412
00:18:00,578 --> 00:18:04,348
Ik denk dat we nu in
zo'n interessante tijd leven,

413
00:18:04,349 --> 00:18:06,316
omdat we in deze tijd leven
waarin hersenen en

414
00:18:06,317 --> 00:18:07,751
machines eigenlijk
dichter bij elkaar komen.

415
00:18:07,752 --> 00:18:10,020
Dus als het gaat
om het

416
00:18:10,021 --> 00:18:12,356
kunnen kijken naar hersenactiviteit,

417
00:18:12,357 --> 00:18:15,292
waar liggen hier
de ethische lijnen?

418
00:18:15,293 --> 00:18:17,327
Hoe privé moeten
mijn interne gedachten zijn?

419
00:18:17,328 --> 00:18:19,663
Zoals met elke krachtige
technologie,

420
00:18:19,664 --> 00:18:22,266
hangt het af van de handen
die het hanteren.

421
00:18:22,267 --> 00:18:24,401
Al deze nieuwe technologieën

422
00:18:24,402 --> 00:18:28,472
zijn dingen die iedereen die ze
gebruikt krachtiger kan maken.

423
00:18:28,473 --> 00:18:32,042
Dus we willen de technologie niet de schuld geven
, maar we willen -

424
00:18:32,043 --> 00:18:33,477
hoe wordt het gebruikt

425
00:18:33,478 --> 00:18:35,445
en wie gebruikt het?

426
00:18:35,446 --> 00:18:37,748
Dus hoe zorgen we ervoor
dat deze

427
00:18:37,749 --> 00:18:39,416
technologie in de juiste handen is?

428
00:18:39,417 --> 00:18:41,718
Dus ik denk dat het heel belangrijk
is om mensen te betrekken

429
00:18:41,719 --> 00:18:45,322
die handelen volgens het beleid en de wet

430
00:18:45,323 --> 00:18:48,592
om te begrijpen wat er
in de toekomst gaat gebeuren.

431
00:18:48,593 --> 00:18:51,528
Ik ben hoopvol over
het gezamenlijke aspect ervan.

432
00:18:51,529 --> 00:18:53,430
Laten we het nu hebben over
de goede dingen.

433
00:18:53,431 --> 00:18:56,133
Ik bedoel, wat zijn
de toepassingen hier?

434
00:18:56,134 --> 00:18:58,135
Ja, dus als we denken

435
00:18:58,136 --> 00:18:59,803
aan wijlen Stephen Hawking,
bijvoorbeeld,

436
00:18:59,804 --> 00:19:04,608
als hij een manier had om een rijkere omgan
 met de wereld of me

437
00:19:04,609 --> 00:19:06,643
 computers te hebben, kunnen we on
 alleen maar voorstellen wat h

438
00:19:06,644 --> 00:19:08,645
j met ons had kunnen delen


439
00:19:08,646 --> 00:19:10,647
  Degenen met het locked-in-syndroom,
toch?

440
00:19:10,648 --> 00:19:13,584
Ze zijn hier.
Ze weten dat ze er zijn.

441
00:19:13,585 --> 00:19:15,786
Maar we hebben gewoon iets
nodig om in hun hersenen

442
00:19:15,787 --> 00:19:17,788
te kijken om te zien
wat ze proberen te zeggen,

443
00:19:17,789 --> 00:19:20,657
of wat ze voelen.
- Juist, precies.

444
00:19:20,658 --> 00:19:22,793
Dus, wat zeg je
tegen mensen

445
00:19:22,794 --> 00:19:26,396
die zo bang zijn
voor technologie

446
00:19:26,397 --> 00:19:31,802
, dat we ons ware
natuurlijke zelf aan technologie overgeven?

447
00:19:31,803 --> 00:19:36,640
Er is iets aanlokkelijks
aan het bereiken van het volgende niveau

448
00:19:36,641 --> 00:19:40,177
van wat sommige mensen
een menselijke

449
00:19:40,178 --> 00:19:43,280
evolutie of beschavingsontwikkeling zouden kunnen noemen,
enzovoort.

450
00:19:43,281 --> 00:19:46,250
In zekere zin leven we al
geen natuurlijk leven, toch?

451
00:19:46,251 --> 00:19:49,586
Want dan zouden de meesten van ons
sterven voor de leeftijd van,

452
00:19:49,587 --> 00:19:51,822
ik weet het niet, 30 of 40 jaar.

453
00:19:51,823 --> 00:19:53,490
We zouden
allerlei ziektes hebben.

454
00:19:53,491 --> 00:19:55,492
Wij zouden
deze kleding niet dragen.

455
00:19:55,493 --> 00:19:58,161
We zouden geen bril
of contactlenzen hebben.

456
00:19:58,162 --> 00:19:59,763
We zouden geen antibiotica hebben.

457
00:19:59,764 --> 00:20:03,233
[Julia]
We zijn al een soort

458
00:20:03,234 --> 00:20:05,335
zeer futuristische cyborgs
als we onszelf vergelijken

459
00:20:05,336 --> 00:20:08,805
met de mens die
10.000 jaar geleden leefde

460
00:20:08,806 --> 00:20:10,574
en genetisch
bijna identiek was

461
00:20:10,575 --> 00:20:11,808
aan wie we nu zijn.

462
00:20:11,809 --> 00:20:17,748
[Michael]
Ja, dat zijn we echt.

463
00:20:17,749 --> 00:20:19,549
Om cognitie te begrijpen
, moeten

464
00:20:19,550 --> 00:20:22,686
we op dit moment in feite
mensen vragen

465
00:20:22,687 --> 00:20:24,321
om te praten over
wat ze denken,

466
00:20:24,322 --> 00:20:26,556
of hun gedrag observeren.

467
00:20:26,557 --> 00:20:30,360
Maar gedachten rechtstreeks lezen
zou een stuk beter zijn.

468
00:20:30,361 --> 00:20:33,630
Zo bestudeert dr.
Kuhl het geheugen,

469
00:20:33,631 --> 00:20:38,402
en zo bestudeert dr.
Kamitani slaap en dromen.

470
00:20:38,403 --> 00:20:40,671
Maar hoewel de technologie
nog een lange weg te gaan heeft,

471
00:20:40,672 --> 00:20:43,106
is het gemakkelijk in te zien
hoe ethische vragen

472
00:20:43,107 --> 00:20:44,841
een probleem kunnen worden.

473
00:20:44,842 --> 00:20:47,110
Nou, hier is het ding:

474
00:20:47,111 --> 00:20:51,782
er bestaat niet zoiets
als een volledig wilde mens.

475
00:20:51,783 --> 00:20:55,886
We evolueren mee
met de technologie.

476
00:20:55,887 --> 00:21:00,090
Mens en technologie zijn
tegenwoordig onafscheidelijk.

477
00:21:00,091 --> 00:21:01,725
Het is waar dat we
voorzichtig moeten zijn

478
00:21:01,726 --> 00:21:03,760
met elk nieuw ding dat we doen,

479
00:21:03,761 --> 00:21:08,332
maar we kunnen het feit
dat ze zullen gebeuren niet veranderen.

480
00:21:08,333 --> 00:21:11,635
Het is een verhaal dat we keer op keer hebben
meegemaakt.

481
00:21:11,636 --> 00:21:14,171
Weet je, we hadden
eeuwig kunnen blijven

482
00:21:14,172 --> 00:21:16,807
discussiëren of er wel of niet
een snelheidslimiet zou moeten bestaan

483
00:21:16,808 --> 00:21:19,743
 de autoriteit zou moeten hebben om de
e te handh

484
00:21:19,744 --> 00:21:21,445
ven.  Maar dat deden we niet.

485
00:21:21,446 --> 00:21:24,715
In plaats daarvan gingen we door
en vonden auto's uit, en bedachten op

486
00:21:24,716 --> 00:21:29,252
verantwoorde wijze
de details terwijl we verder gingen.

487
00:21:29,253 --> 00:21:31,254
Ethische vragen
over nieuwe technologieën

488
00:21:31,255 --> 00:21:35,726
doen het meeste goed wanneer
ze de technologie vergemakkelijken,

489
00:21:35,727 --> 00:21:40,130
niet wanneer ze de
vooruitgang nodeloos belemmeren.

490
00:21:40,131 --> 00:21:41,498
Dus volg je dromen.

491
00:21:41,499 --> 00:21:44,634
En laat ze me zo snel mogelijk
zien.

492
00:21:44,635 --> 00:21:47,606
En, zoals altijd,
bedankt voor het kijken.


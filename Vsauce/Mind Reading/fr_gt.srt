1
00:00:05,237 --> 00:00:06,438
Télépathie?

2
00:00:06,439 --> 00:00:08,172
Bien sûr que non.

3
00:00:08,173 --> 00:00:11,376
J'aime lire.

4
00:00:11,377 --> 00:00:15,413
Écoutez, la lecture de l'esprit peut
sembler pseudo-scientifique -- excusez

5
00:00:15,414 --> 00:00:16,614
mon langage --

6
00:00:16,615 --> 00:00:18,416
bullshoot.

7
00:00:18,417 --> 00:00:21,486
Mais sa contrepartie scientifique, l'
identification de la pensée,

8
00:00:21,487 --> 00:00:23,855
est bien réelle.

9
00:00:23,856 --> 00:00:26,524
Il est basé sur la neuroimagerie
et l'apprentissage automatique,

10
00:00:26,525 --> 00:00:29,861
et ce qui est vraiment cool, c'est
que les expériences de lecture mentale

11
00:00:29,862 --> 00:00:33,498
ne consistent pas seulement à
espionner ce que quelqu'un pense.

12
00:00:33,499 --> 00:00:37,602
Il s'agit de déterminer de
quoi sont faites les pensées.

13
00:00:37,603 --> 00:00:39,504
Je veux dire, quand je pense
à quelque chose, à

14
00:00:39,505 --> 00:00:43,174
quoi ressemble cette image mentale
?

15
00:00:43,175 --> 00:00:44,709
C'est en quelle résolution ?

16
00:00:44,710 --> 00:00:47,145
Quelle est la haute fidélité d'
un souvenir

17
00:00:47,146 --> 00:00:49,380
et comment évolue-t-il
avec le temps ?

18
00:00:49,381 --> 00:00:51,116
Eh bien, dans cet épisode,

19
00:00:51,117 --> 00:00:52,750
je vais voir comment
lire dans l'esprit des gens

20
00:00:52,751 --> 00:00:54,719
peut nous aider à répondre à
ces questions.

21
00:00:54,720 --> 00:00:58,490
Mon voyage commence ici même
à l'Université de l'Oregon.

22
00:00:58,491 --> 00:01:01,259
Je rencontre le Dr Brice Kuhl
du laboratoire Kuhl.

23
00:01:01,260 --> 00:01:03,394
C'est un neuroscientifique
qui utilise la neuroimagerie

24
00:01:03,395 --> 00:01:06,664
et l'apprentissage automatique pour
comprendre ce que les gens pensent

25
00:01:06,665 --> 00:01:31,789
sans qu'ils le lui disent.

26
00:01:31,790 --> 00:01:33,625
Alors dis-moi ce que
tu fais ici.

27
00:01:33,626 --> 00:01:36,794
Eh bien, je suis dans le
programme de neurosciences cognitives ici,

28
00:01:36,795 --> 00:01:38,396
et j'étudie la mémoire humaine.

29
00:01:38,397 --> 00:01:41,099
Mon laboratoire utilise principalement
des méthodes de neuroimagerie,

30
00:01:41,100 --> 00:01:42,567
donc nous faisons beaucoup de travail en utilisant

31
00:01:42,568 --> 00:01:44,169
l'
imagerie par résonance magnétique fonctionnelle,

32
00:01:44,170 --> 00:01:45,637
ou IRMf.

33
00:01:45,638 --> 00:01:49,340
Et comment utilisez-vous l'
IRMf pour étudier les souvenirs ?

34
00:01:49,341 --> 00:01:51,776
Nous examinons le schéma
de l'activité neuronale.

35
00:01:51,777 --> 00:01:54,445
Lorsque vous formez une mémoire,
il y a un certain schéma.

36
00:01:54,446 --> 00:01:56,548
Et nous pouvons enregistrer
ce modèle

37
00:01:56,549 --> 00:01:59,617
et ensuite tester si
ce modèle est rétabli

38
00:01:59,618 --> 00:02:02,554
ou réactivé ultérieurement,
comme lorsque vous vous en souvenez.

39
00:02:02,555 --> 00:02:05,823
Cela signifie-t-il que nous pouvons examiner
les modèles d'activité cérébrale

40
00:02:05,824 --> 00:02:10,061
et en déduire ce dont on
se souvient, ou qui est rappelé,

41
00:02:10,062 --> 00:02:11,262
ou même simplement pensé ?

42
00:02:11,263 --> 00:02:13,631
Oui, et c'est pourquoi nous appelons cela le
décodage.

43
00:02:13,632 --> 00:02:16,568
Donc, cela prend essentiellement
votre modèle d'entrée

44
00:02:16,569 --> 00:02:18,469
comme un modèle d'activité
que nous enregistrons

45
00:02:18,470 --> 00:02:21,272
pendant que vous vous souvenez de
quelque chose.

46
00:02:21,273 --> 00:02:23,441
Et nous faisons une prédiction
sur ce dont vous vous souvenez.

47
00:02:23,442 --> 00:02:27,178
Vous pouvez voir à quel point cela ressemble
à de la lecture dans les pensées.

48
00:02:27,179 --> 00:02:28,713
[rires]
Oui.  Ça sonne comme ça.

49
00:02:28,714 --> 00:02:32,584
Alors, Brice, qu'est-ce que tu vas
me faire aujourd'hui ?

50
00:02:32,585 --> 00:02:34,419
Donc, ce que nous
allons faire aujourd'hui

51
00:02:34,420 --> 00:02:36,354
est un territoire inexploré
pour nous.

52
00:02:36,355 --> 00:02:38,623
Nous allons donc essayer
une sorte de nouvelle variante

53
00:02:38,624 --> 00:02:40,258
de l'expérience sur vous.

54
00:02:40,259 --> 00:02:42,660
Je ne peux donc garantir
aucun résultat particulier.

55
00:02:42,661 --> 00:02:44,195
Mais cela représente
où en est le terrain

56
00:02:44,196 --> 00:02:46,664
et où
nous essayons d'aller.

57
00:02:46,665 --> 00:02:48,800
Aujourd'hui, vous allez
participer à une expérience

58
00:02:48,801 --> 00:02:50,268
où vous étudierez des visages.

59
00:02:50,269 --> 00:02:51,803
Nous allons donc vous faire
étudier

60
00:02:51,804 --> 00:02:53,404
12 photos de célébrités.

61
00:02:53,405 --> 00:02:54,872
Des gens que je
connais déjà.

62
00:02:54,873 --> 00:02:56,674
-Les gens que vous connaissez, ouais.
-D'accord.

63
00:02:56,675 --> 00:02:59,143
Et vous allez essayer
de vous souvenir de ces images.

64
00:02:59,144 --> 00:03:01,112
Ensuite, nous allons vous faire entrer
dans le scanner IRM.

65
00:03:01,113 --> 00:03:04,249
Essayez de
vous rappeler cette image aussi clairement que possible.

66
00:03:04,250 --> 00:03:06,417
Et nous allons enregistrer
votre activité cérébrale

67
00:03:06,418 --> 00:03:08,753
pendant que vous essayez d'imaginer
ces images.

68
00:03:08,754 --> 00:03:10,555
On va essayer
de construire le visage.

69
00:03:10,556 --> 00:03:12,590
Essentiellement, dessinez une image de
ce dont vous vous souvenez.

70
00:03:12,591 --> 00:03:14,125
-Une image?
-Une image.

71
00:03:14,126 --> 00:03:16,327
Une vraie photo
qu'on peut imprimer

72
00:03:16,328 --> 00:03:17,729
et que je pourrais, genre,
accrocher à mon mur.

73
00:03:17,730 --> 00:03:19,831
[rires]
Si tu voulais.

74
00:03:19,832 --> 00:03:22,634
[Michael]La première étape
consiste pour moi à mémoriser

75
00:03:22,635 --> 00:03:25,336
les 12
photographies de célébrités spécifiques que

76
00:03:25,337 --> 00:03:28,640
Brice essaiera plus tard
de détecter dans ma pensée.

77
00:03:28,641 --> 00:03:33,278
Je me suis assis pour faire cet
étudiant diplômé, Max.

78
00:03:33,279 --> 00:03:35,280
Le succès de ses prédictions
dépend, en partie,

79
00:03:35,281 --> 00:03:37,415
de ma capacité
à me souvenir de ces visages

80
00:03:37,416 --> 00:03:43,087
aussi clairement que
possible à l'intérieur de l'IRMf.

81
00:03:43,088 --> 00:03:44,422
D'accord, alors...

82
00:03:44,423 --> 00:03:46,157
[soupirs]

83
00:03:46,158 --> 00:03:50,728
Je pense que j'ai un assez bon
souvenir de tout ça.

84
00:03:50,729 --> 00:03:53,698
-Super.
-Je sens que les enjeux sont élevés.

85
00:03:53,699 --> 00:03:56,701
Avec les visages de célébrités,
espérons-le, mémorisés,

86
00:03:56,702 --> 00:03:58,303
il est temps de passer à l'étape suivante :

87
00:03:58,304 --> 00:04:00,071
passer par
le détecteur de métaux

88
00:04:00,072 --> 00:04:01,706
et dans l'IRMf,

89
00:04:01,707 --> 00:04:04,676
où Brice enregistrera
et surveillera mon activité cérébrale,

90
00:04:04,677 --> 00:04:08,746
puis l'introduira plus tard dans son
algorithme pour reconstruire les visages.

91
00:04:08,747 --> 00:04:10,448
Ce sera la première fois
qu'il tentera

92
00:04:10,449 --> 00:04:12,383
de reconstruire des visages à
partir de la mémoire à long terme,

93
00:04:12,384 --> 00:04:14,385
ce qui est très difficile,
car nous comptons

94
00:04:14,386 --> 00:04:16,720
sur la clarté avec laquelle je peux me souvenir
des photos de célébrités que

95
00:04:16,721 --> 00:04:19,090
j'ai vues il y a une heure.

96
00:04:19,091 --> 00:04:21,225
J'adore ses yeux.
Regarde ça.

97
00:04:21,226 --> 00:04:24,362
[femme]

98
00:04:24,363 --> 00:04:31,102
Est-ce que l'enfant ne serait pas du genre
"Ça va me manger" ?

99
00:04:31,103 --> 00:04:34,205
Une IRMf surveille l'activité
dans le cerveau

100
00:04:34,206 --> 00:04:36,774
en la divisant
en milliers de petits cubes

101
00:04:36,775 --> 00:04:39,744
appelés voxels
ou pixels volumétriques.

102
00:04:39,745 --> 00:04:41,446
Chacun de ces voxels contient des

103
00:04:41,447 --> 00:04:43,581
centaines de milliers
de neurones.

104
00:04:43,582 --> 00:04:46,117
Grâce à l'IRMf,
nous sommes capables de détecter

105
00:04:46,118 --> 00:04:47,719
le flux sanguin
dans ces voxels,

106
00:04:47,720 --> 00:04:50,088
ce qui signifie que cette partie
du cerveau est active.

107
00:04:50,089 --> 00:04:53,124
Si on me montre plusieurs photos
de personnes avec des moustaches,

108
00:04:53,125 --> 00:04:56,327
mon cerveau réagira
aux traits de chaque visage.

109
00:04:56,328 --> 00:04:58,329
Mais il y aura
une zone commune de mon cerveau

110
00:04:58,330 --> 00:05:00,164
qui sera engagée
tout au long.

111
00:05:00,165 --> 00:05:04,102
C'est peut-être la zone de mon
cerveau qui réagit aux moustaches.

112
00:05:04,103 --> 00:05:07,171
Alors plus tard,
quand j'imagine un visage,

113
00:05:07,172 --> 00:05:09,440
si Brice remarque
que cette zone est engagée,

114
00:05:09,441 --> 00:05:11,476
il peut prédire
que je pense

115
00:05:11,477 --> 00:05:13,811
à une moustache.

116
00:05:13,812 --> 00:05:15,780
Donc, en ce moment, Michael est
dans le scanner,

117
00:05:15,781 --> 00:05:18,282
et il voit les mots apparaître
sur l'écran un par un,

118
00:05:18,283 --> 00:05:20,651
et il essaie
de visualiser le visage, se

119
00:05:20,652 --> 00:05:23,187
souvenir du visage avec autant de
détails que possible.

120
00:05:23,188 --> 00:05:25,356
Ce que vous pouvez voir ici, ce sont
les images que nous acquérons.

121
00:05:25,357 --> 00:05:28,793
Nous obtenons un de ces
volumes cérébraux toutes les deux secondes.

122
00:05:28,794 --> 00:05:32,797
Celles-ci sont donc rafraîchissantes en temps réel au
fur et à mesure que nous collectons les images.

123
00:05:32,798 --> 00:05:35,633
[Michael]La première partie
de la session d'IRMf étant terminée,

124
00:05:35,634 --> 00:05:38,503
il est temps de passer à la deuxième partie,
où Brice et son

125
00:05:38,504 --> 00:05:41,773
équipe apprendront le langage
de mon activité cérébrale,

126
00:05:41,774 --> 00:05:44,776
afin qu'ils puissent ensuite
décoder par des scanners cérébraux.

127
00:05:44,777 --> 00:05:46,544
Salut Michael.
Tu vas bien encore ?

128
00:05:46,545 --> 00:05:48,312
[Michael]
Oui.

129
00:05:48,313 --> 00:05:50,381
Ils me montreront des centaines
de visages uniques

130
00:05:50,382 --> 00:05:52,784
et enregistreront comment mon cerveau réagit

131
00:05:52,785 --> 00:05:54,852
à certaines
caractéristiques faciales.

132
00:05:54,853 --> 00:05:57,188
Ils utiliseront ensuite
ces informations

133
00:05:57,189 --> 00:05:59,657
pour reconstituer
les visages de célébrités auxquels

134
00:05:59,658 --> 00:06:03,127
j'ai pensé lors de
la première phase du scan.

135
00:06:03,128 --> 00:06:05,530
Vraiment, plus
nous pouvons montrer de visages à Michael, mieux c'est.

136
00:06:05,531 --> 00:06:08,166
Nous allons donc essentiellement le
garder là-dedans

137
00:06:08,167 --> 00:06:09,600
tant qu'il est à l'aise.

138
00:06:09,601 --> 00:06:11,636
[Michael]
Deux heures était le temps maximum que

139
00:06:11,637 --> 00:06:13,538
nous pouvions obtenir dans l'IRMf.

140
00:06:13,539 --> 00:06:17,175
Mais j'ai pu
regarder plus de 400 visages,

141
00:06:17,176 --> 00:06:18,743
ce qui devrait être suffisant pour obtenir

142
00:06:18,744 --> 00:06:20,778
des résultats assez intéressants
.

143
00:06:20,779 --> 00:06:22,447
Hé, Michael, tu l'as fait.
C'était génial.

144
00:06:22,448 --> 00:06:23,681
On va venir
vous sortir.

145
00:06:23,682 --> 00:06:33,157
[Michael]
D'accord.

146
00:06:33,158 --> 00:06:34,725
Ouais, donc ça montre juste
quelques-unes des photos

147
00:06:34,726 --> 00:06:36,561
que nous prenions
pendant que vous étiez là-dedans.

148
00:06:36,562 --> 00:06:38,095
Quelques images de votre cerveau.

149
00:06:38,096 --> 00:06:39,764
Maintenant, nous
allons faire quelques calculs.

150
00:06:39,765 --> 00:06:42,200
Max va analyser
vos données.

151
00:06:42,201 --> 00:06:43,701
Nous nous
retrouverons demain,

152
00:06:43,702 --> 00:06:45,369
où nous
examinerons les résultats,

153
00:06:45,370 --> 00:06:47,638
où nous essaierons de
reconstituer réellement les images de visage à

154
00:06:47,639 --> 00:06:49,740
partir des données cérébrales
que nous venons de collecter.

155
00:06:49,741 --> 00:06:51,175
D'accord.
Et bien à demain.

156
00:06:51,176 --> 00:06:52,577
D'accord.
Merci beaucoup.

157
00:06:52,578 --> 00:06:54,178
Max, merci aussi.
Je ne peux pas attendre.

158
00:06:54,179 --> 00:06:55,847
Tu ferais mieux de passer
une nuit blanche.

159
00:06:55,848 --> 00:07:04,288
Je veux que ces
données soient parfaites.

160
00:07:04,289 --> 00:07:06,524
D'accord, donc je suis de retour
au labo du Dr Kuhl.

161
00:07:06,525 --> 00:07:08,693
Du jour au lendemain, son équipe a
analysé les données,

162
00:07:08,694 --> 00:07:15,500
et j'ai hâte de voir ce
qu'ils pensent m'avoir vu penser.

163
00:07:15,501 --> 00:07:17,101
Comment sont mes résultats ?

164
00:07:17,102 --> 00:07:18,736
Je pense qu'ils ont l'air bien.

165
00:07:18,737 --> 00:07:20,705
Nous allons jeter un coup d'œil
dans un instant ici.

166
00:07:20,706 --> 00:07:22,406
D'accord,
je ne peux pas attendre.

167
00:07:22,407 --> 00:07:24,342
- Alors je peux juste m'asseoir ?
-Oui, asseyez-vous.

168
00:07:24,343 --> 00:07:26,143
D'accord, alors...

169
00:07:26,144 --> 00:07:28,212
tout d'abord...

170
00:07:28,213 --> 00:07:30,081
qu'est-ce que je vois ?
Oh, d'accord, eh bien,

171
00:07:30,082 --> 00:07:32,283
ce sont les images que
j'ai réellement mémorisées.

172
00:07:32,284 --> 00:07:34,252
-C'est vrai.
-Et c'est ce que

173
00:07:34,253 --> 00:07:37,822
tu as reconstitué à
partir de mon imagination.

174
00:07:37,823 --> 00:07:40,091
-C'est vrai.
-Oh wow.  D'accord.

175
00:07:40,092 --> 00:07:43,094
[Brice]
Bon, c'est donc une
des reconstructions

176
00:07:43,095 --> 00:07:44,562
qui a été générée.

177
00:07:44,563 --> 00:07:46,063
[Michael]
Intéressant.

178
00:07:46,064 --> 00:07:47,698
[Max]
Alors c'est John Cho.

179
00:07:47,699 --> 00:07:50,668
[Michael]
Pas mal.  Pas mal.

180
00:07:50,669 --> 00:07:53,337
-Pouvons-nous voir le côte à côte?
-Ouais.

181
00:07:53,338 --> 00:07:55,673
[Michael]
Je vois, vous savez, des similitudes

182
00:07:55,674 --> 00:08:00,044
dans le genre d'
expressions faciales en général.

183
00:08:00,045 --> 00:08:02,213
Vous savez, vous pourriez presque
voir la racine des cheveux correspondant ici.

184
00:08:02,214 --> 00:08:04,682

Je pensais aussi que la forme du visage était...

185
00:08:04,683 --> 00:08:06,684
Il avait en
quelque sorte une forme carrée.

186
00:08:06,685 --> 00:08:08,152
-Oui.  Oui.
-Alors ce sont les choses

187
00:08:08,153 --> 00:08:09,387
qui me sont venues à l'esprit.

188
00:08:09,388 --> 00:08:11,289
Et donc quand je
visualisais

189
00:08:11,290 --> 00:08:13,257
cette image de John Cho,

190
00:08:13,258 --> 00:08:16,260
la carrure du visage était
la première chose, la plus saillante.

191
00:08:16,261 --> 00:08:19,697
Je n'arrêtais pas de penser,
il était le gars carré.

192
00:08:19,698 --> 00:08:23,401
Excellent, d'accord.

193
00:08:23,402 --> 00:08:26,704
[Brice]
Alors, c'est Megan Fox.

194
00:08:26,705 --> 00:08:28,439
[Michael]
Mm-hmm.

195
00:08:28,440 --> 00:08:30,207
Vous allez nous montrer le...
côte à côte.

196
00:08:30,208 --> 00:08:31,776
[Michael]
Le côte à côte.  Droit.

197
00:08:31,777 --> 00:08:33,644
[Brice]
Vous pouvez voir l'image que
vous avez réellement vue,

198
00:08:33,645 --> 00:08:36,547
et c'est la reconstruction que
nous avons générée.

199
00:08:36,548 --> 00:08:39,417
Je vais te le dire.
Megan Fox, je n'étais pas capable

200
00:08:39,418 --> 00:08:42,353
d'avoir une image vraiment claire
dans mon esprit.

201
00:08:42,354 --> 00:08:45,056
Pour une raison quelconque, cette image
d'elle était vraiment difficile

202
00:08:45,057 --> 00:08:47,058
à ramener dans mon esprit.

203
00:08:47,059 --> 00:08:50,595
La sévérité du visage était
quelque chose que j'ai remarqué.

204
00:08:50,596 --> 00:08:53,698
Alors j'ai senti qu'il y
avait... Ça avait l'air féminin.

205
00:08:53,699 --> 00:08:55,533
Et vous avez
relevé la sévérité.

206
00:08:55,534 --> 00:08:58,769
Et donc ensemble,
cela produit un match.

207
00:08:58,770 --> 00:09:00,538
[Michael]Gardez à l'esprit
que Brice et son équipe les

208
00:09:00,539 --> 00:09:02,773
ont lus
de mémoire.

209
00:09:02,774 --> 00:09:04,609
Mais quand je me souviens d'un visage, est-

210
00:09:04,610 --> 00:09:07,678
ce que j'imagine chaque détail
simultanément

211
00:09:07,679 --> 00:09:09,313
avec une précision photographique ?

212
00:09:09,314 --> 00:09:10,748
Ou est-ce que je m'occupe juste de quelques-uns
à la fois ?

213
00:09:10,749 --> 00:09:13,417
En lisant dans mes pensées,
ils peuvent voir à

214
00:09:13,418 --> 00:09:15,219
quel point ma mémoire est mauvaise
et comment elle fonctionne.

215
00:09:15,220 --> 00:09:18,689
-Moi!  Moi!
-[Brice rit]

216
00:09:18,690 --> 00:09:21,525
D'accord, c'est donc votre
reconstruction

217
00:09:21,526 --> 00:09:24,729
de moi en pensant à
cette image de moi-même.

218
00:09:24,730 --> 00:09:26,497
[Brice]
C'est vrai.

219
00:09:26,498 --> 00:09:28,666
Où est passée la barbe ?

220
00:09:28,667 --> 00:09:31,068
[Brice] Je ne sais pas.
J'espérais que tu pourrais me le dire.

221
00:09:31,069 --> 00:09:36,240
[Michael]
Par exemple, voici une photo
de moi me souvenant de mon propre visage.

222
00:09:36,241 --> 00:09:38,743
Cela ne me ressemble vraiment pas,
mais la question est : à

223
00:09:38,744 --> 00:09:41,345
quel point suis-je
capable de m'imaginer ?

224
00:09:41,346 --> 00:09:44,048
Je ne pense pas souvent à mon propre
visage,

225
00:09:44,049 --> 00:09:45,616
donc l'étrangeté
du résultat

226
00:09:45,617 --> 00:09:47,752
peut être autant due à des défauts
dans ma propre mémoire

227
00:09:47,753 --> 00:09:51,288
et à mon image mentale
qu'à des défauts dans la technologie.

228
00:09:51,289 --> 00:09:53,691
C'est donc Jennifer Lawrence,
je crois.

229
00:09:53,692 --> 00:09:55,726
[Michael]
C'est Jennifer Lawrence ?

230
00:09:55,727 --> 00:09:59,664
On dirait que c'est l'
oncle beaucoup plus âgé de Jennifer Lawrence.

231
00:09:59,665 --> 00:10:01,365
[tous rient]

232
00:10:01,366 --> 00:10:05,069
Rien ici
n'était trop proche.

233
00:10:05,070 --> 00:10:09,407
Mais c'est quelque chose que
vous commencez tout juste à essayer

234
00:10:09,408 --> 00:10:11,409
ce genre de
souvenirs à long terme.

235
00:10:11,410 --> 00:10:14,679
Ce que Brice et son équipe ont
lu dans mon esprit

236
00:10:14,680 --> 00:10:18,549
aurait pu être plus précis
s'ils m'avaient montré des milliers

237
00:10:18,550 --> 00:10:20,284
plutôt que des centaines d'images
dans l'IRMf,

238
00:10:20,285 --> 00:10:22,520
car l'algorithme
aurait alors appris

239
00:10:22,521 --> 00:10:24,655
le langage de mon cerveau de manière
plus approfondie.

240
00:10:24,656 --> 00:10:27,358
Mais peu importe,
la qualité de mes souvenirs

241
00:10:27,359 --> 00:10:29,226
aurait toujours
été un problème.

242
00:10:29,227 --> 00:10:30,661
Je veux dire, regardez ce qui se passe
quand la mémoire

243
00:10:30,662 --> 00:10:33,164
est entièrement supprimée de l'équation
.

244
00:10:33,165 --> 00:10:35,166
Brice a également lu
mon activité cérébrale

245
00:10:35,167 --> 00:10:37,168
lorsque je regardais
des visages dans l'IRMf.

246
00:10:37,169 --> 00:10:39,103
pas seulement les imaginer.

247
00:10:39,104 --> 00:10:41,672
Et ces résultats
étaient bien plus proches

248
00:10:41,673 --> 00:10:44,675
que ceux reconstitués à
partir de ma mémoire.

249
00:10:44,676 --> 00:10:47,111
Bon, alors, qu'est-ce que je
regarde ici ?

250
00:10:47,112 --> 00:10:48,679
[Brice]
D'accord, donc ce que vous voyez ici

251
00:10:48,680 --> 00:10:51,682
dans la rangée du haut,
ce sont des images que vous avez vues

252
00:10:51,683 --> 00:10:53,584
pendant que vous étiez dans le scanner.

253
00:10:53,585 --> 00:10:56,754
En dessous, dans cette rangée du bas,
ce sont les reconstructions

254
00:10:56,755 --> 00:11:00,725
que nous tirons des schémas
d'activité cérébrale que nous avons collectés.

255
00:11:00,726 --> 00:11:03,828
-Ceci est de l'image source.
-À droite.

256
00:11:03,829 --> 00:11:05,629
[Michael]
Ça vient de mon cerveau.

257
00:11:05,630 --> 00:11:07,565
-[Brice] D'accord.
-[Michael] Ils sont assez proches.

258
00:11:07,566 --> 00:11:09,700
Oui, dans l'ensemble, ils étaient
assez proches.

259
00:11:09,701 --> 00:11:11,602
Donc pas parfait.

260
00:11:11,603 --> 00:11:13,771
Ce sont - vous pouvez voir qu'il y a une
certaine variabilité dans ceux-ci.

261
00:11:13,772 --> 00:11:16,640
Mais cela est cohérent
avec ce que nous avons trouvé auparavant, à savoir

262
00:11:16,641 --> 00:11:18,409
que les reconstructions
que nous avons générées,

263
00:11:18,410 --> 00:11:20,244
lorsque vous visualisez
les visages,

264
00:11:20,245 --> 00:11:22,279
il y a une certaine correspondance
entre le visage réel.

265
00:11:22,280 --> 00:11:23,714
Il s'agit donc d'
une sorte de contrôle de santé mentale,

266
00:11:23,715 --> 00:11:25,750
que nous pouvons réellement
reconstruire les images

267
00:11:25,751 --> 00:11:28,219
-lorsque vous les visualisez.
-Bien, bien.

268
00:11:28,220 --> 00:11:31,355
Ils sont plutôt bons.

269
00:11:31,356 --> 00:11:33,190
Eh bien, Brice, Max,
merci beaucoup

270
00:11:33,191 --> 00:11:35,126
de m'avoir permis d'en
faire partie.

271
00:11:35,127 --> 00:11:36,660
J'espère que mes données sont utiles.

272
00:11:36,661 --> 00:11:38,596
Merci.
C'était très amusant.

273
00:11:38,597 --> 00:11:46,837
C'est toujours utile pour nous
de penser à ces choses.

274
00:11:46,838 --> 00:11:50,808
Les recherches sur la mémoire du Dr Brice Kuhl
montrent qu'il est possible

275
00:11:50,809 --> 00:11:53,677
pour un ordinateur
de lire dans l'esprit de quelqu'un.

276
00:11:53,678 --> 00:11:56,313
Pour comprendre
ce qu'ils pensent.

277
00:11:56,314 --> 00:11:58,415
Mais beaucoup de progrès
restent à faire.

278
00:11:58,416 --> 00:12:00,050
Je veux dire, si vous voulez savoir

279
00:12:00,051 --> 00:12:01,619
ce que je pense en ce moment,
par exemple,

280
00:12:01,620 --> 00:12:05,189
c'est encore plus simple de me demander
de vous le dire.

281
00:12:05,190 --> 00:12:07,491
Et si je ne peux pas
te le dire ?

282
00:12:07,492 --> 00:12:10,594
Le Dr Yukiyasu Kamitani
est un chercheur,

283
00:12:10,595 --> 00:12:14,498
professeur et pionnier
explorant la frontière

284
00:12:14,499 --> 00:12:17,535
derrière le mur du sommeil.

285
00:12:17,536 --> 00:12:19,703
Je suis venu ici
à l'Université de Kyoto

286
00:12:19,704 --> 00:12:21,672
pour le rencontrer et voir
ce que c'est que

287
00:12:21,673 --> 00:12:24,175
de lire non pas ce que
quelqu'un pense,

288
00:12:24,176 --> 00:12:29,647
mais ce que
quelqu'un rêve.

289
00:12:29,648 --> 00:12:31,348
Sensei Kamitani,
je suis Michael.

290
00:12:31,349 --> 00:12:33,818
-Salut, je suis Yuki.
-Yuki, ravi de te rencontrer.

291
00:12:33,819 --> 00:12:36,320
[Michael]
Au cours des dix dernières années, le

292
00:12:36,321 --> 00:12:38,455
Dr Kamitani a été
à l'avant-garde

293
00:12:38,456 --> 00:12:40,124
de la lecture mentale par machine.

294
00:12:40,125 --> 00:12:43,527
Le sujet est, vous savez,
prêt à entrer.

295
00:12:43,528 --> 00:12:45,462
Semblable à Brice Kuhl,

296
00:12:45,463 --> 00:12:48,632
ses premières expériences ont exploré la
reconstruction d'images

297
00:12:48,633 --> 00:12:52,236
montrées aux sujets dans une IRMf en
fonction de leur activité cérébrale.

298
00:12:52,237 --> 00:12:53,637
Dans le cas de Kamitani,

299
00:12:53,638 --> 00:12:55,706
les images étaient
des formes en noir et blanc

300
00:12:55,707 --> 00:12:58,375
et les reconstructions
étaient d'une précision saisissante.

301
00:12:58,376 --> 00:13:03,180
Récemment, Kamitani s'est concentré
sur l'utilisation des réseaux de neurones profonds

302
00:13:03,181 --> 00:13:04,815
et de l'apprentissage automatique

303
00:13:04,816 --> 00:13:06,450
pour déchiffrer
l'activité cérébrale des sujets

304
00:13:06,451 --> 00:13:08,686
pendant qu'ils visualisent
des photographies beaucoup plus complexes.

305
00:13:08,687 --> 00:13:12,756
Ce que vous voyez est le
résultat d'un réseau neuronal profond

306
00:13:12,757 --> 00:13:15,226
traitant l'activité cérébrale
d'un sujet

307
00:13:15,227 --> 00:13:17,795
regardant la photo.

308
00:13:17,796 --> 00:13:20,431
Cela pourrait avoir une myriade d'
applications à l'avenir,

309
00:13:20,432 --> 00:13:22,733
par exemple
dans les enquêtes criminelles

310
00:13:22,734 --> 00:13:26,470
et la
communication interpersonnelle.

311
00:13:26,471 --> 00:13:28,739
[Kamitani]
C'est loin d'être parfait.

312
00:13:28,740 --> 00:13:33,177
Mais je pense que vous en voyez encore,
vous savez, des yeux et, vous savez...

313
00:13:33,178 --> 00:13:34,778
[Michael]
Eh bien, ouais.

314
00:13:34,779 --> 00:13:36,447
Et les couleurs aussi.

315
00:13:36,448 --> 00:13:39,783
[Kamitani]
Ouais, dans une certaine mesure, ouais.

316
00:13:39,784 --> 00:13:42,419
Son travail le plus récent, cependant,
porte sur le subconscient.

317
00:13:42,420 --> 00:13:45,256
Il tente quelque chose d'
extrêmement ambitieux :

318
00:13:45,257 --> 00:13:46,824
enregistrer nos rêves.

319
00:13:46,825 --> 00:13:49,326
Diriez-vous que vous êtes
un chercheur sur le sommeil

320
00:13:49,327 --> 00:13:50,828
ou un chercheur sur la vision ?

321
00:13:50,829 --> 00:13:53,664
Peut-être un décodeur cérébral.

322
00:13:53,665 --> 00:13:55,432
Un décodeur cérébral.

323
00:13:55,433 --> 00:13:57,534
C'est une
description de poste plutôt cool.

324
00:13:57,535 --> 00:14:01,739
Pouvez-vous me montrer quoi
que ce soit de ce que vous faites avec les rêves ?

325
00:14:01,740 --> 00:14:08,379
[Kamitani]
Mm-hmm, ouais.

326
00:14:08,380 --> 00:14:10,514
Les travaux du Dr Kamitani
sur le décodage des rêves

327
00:14:10,515 --> 00:14:13,317
commencent par un processus similaire
à celui du Dr Kuhl :

328
00:14:13,318 --> 00:14:15,452
montrer au sujet du test des
milliers d'images

329
00:14:15,453 --> 00:14:17,187
pendant qu'il est dans une

330
00:14:17,188 --> 00:14:18,722
IRMf afin d'apprendre à
quoi ressemble le cerveau

331
00:14:18,723 --> 00:14:21,358
lorsqu'il pense
à certaines choses.

332
00:14:21,359 --> 00:14:23,794
Une fois que l'
algorithme d'apprentissage automatique est assez bon

333
00:14:23,795 --> 00:14:26,764
pour identifier les images auxquelles
le sujet pense,

334
00:14:26,765 --> 00:14:29,300
le sujet est placé
dans un IRMf

335
00:14:29,301 --> 00:14:31,335
avec un capuchon EEG
sur la tête

336
00:14:31,336 --> 00:14:33,470
et invité à s'endormir.

337
00:14:33,471 --> 00:14:36,573
Lorsque les ondes EEG indiquent
que la personne rêve,

338
00:14:36,574 --> 00:14:39,243
l'algorithme prédit de
quels types de choses

339
00:14:39,244 --> 00:14:41,645
le sujet est le plus susceptible de
rêver.

340
00:14:41,646 --> 00:14:45,215
À l'heure actuelle, l'algorithme
recherche 20 catégories.

341
00:14:45,216 --> 00:14:48,485
Des choses comme les bâtiments, les
transports

342
00:14:48,486 --> 00:14:50,621
et les caractères
dans une langue.

343
00:14:50,622 --> 00:14:53,324
Les chercheurs réveillent ensuite
le sujet,

344
00:14:53,325 --> 00:14:55,326
lui demandent de quoi il
rêvait

345
00:14:55,327 --> 00:14:57,227
et voient si la prédiction de l'algorithme


346
00:14:57,228 --> 00:14:59,530
et le
souvenir de la personne correspondent.

347
00:14:59,531 --> 00:15:03,367
Voici les données réelles d'
une des expériences de Kamitani.

348
00:15:03,368 --> 00:15:05,703
Ci-dessous, un nuage
de mots de catégories.

349
00:15:05,704 --> 00:15:08,339
Le nom de chaque
catégorie grossit ou diminue

350
00:15:08,340 --> 00:15:10,574
en temps réel en
fonction de la

351
00:15:10,575 --> 00:15:13,344
probabilité qu'elles soient présentes
dans le rêve actuel du sujet.

352
00:15:13,345 --> 00:15:15,346
Or, comme vous pouvez le constater, l'
activité est actuellement la plus forte

353
00:15:15,347 --> 00:15:18,349
pour la catégorie « caractère »,
c'est-à-dire la langue écrite.

354
00:15:18,350 --> 00:15:20,684
À ce stade,
le sujet a été réveillé,

355
00:15:20,685 --> 00:15:29,660
et c'est ce
qu'ils ont rapporté.

356
00:15:29,661 --> 00:15:32,062
C'est assez effrayant.

357
00:15:32,063 --> 00:15:33,697
-[rires]
-D'accord ?  Je veux dire, vous--

358
00:15:33,698 --> 00:15:36,700
vous avez espionné leur rêve.

359
00:15:36,701 --> 00:15:39,370
Ouais, d'une certaine manière.
Mais...

360
00:15:39,371 --> 00:15:42,406
la précision n'est
pas si grande, alors...

361
00:15:42,407 --> 00:15:44,341
Eh bien, la précision n'est
pas si grande mais, vous savez,

362
00:15:44,342 --> 00:15:46,677
ma précision normale pour deviner
les rêves des gens est de zéro.

363
00:15:46,678 --> 00:15:48,145
Droit.

364
00:15:48,146 --> 00:15:49,580
Tout en poursuivant
ses recherches

365
00:15:49,581 --> 00:15:52,182
sur la prédiction
du contenu des rêves, le

366
00:15:52,183 --> 00:15:54,785
Dr Kamitani se lance
dans son nouveau projet : en

367
00:15:54,786 --> 00:15:58,389
fait reconstruire des images à
partir de nos rêves.

368
00:15:58,390 --> 00:16:01,458
Donc, vous avez apporté
certaines des reconstructions

369
00:16:01,459 --> 00:16:02,659
que votre labo a créées...

370
00:16:02,660 --> 00:16:04,061
Mm-hmm.

371
00:16:04,062 --> 00:16:17,741
...de rêves.

372
00:16:17,742 --> 00:16:20,210
D'accord, ils ressemblent tous à des rêves
de gouttes.

373
00:16:20,211 --> 00:16:21,779
[Kamitani]
Ouais.

374
00:16:21,780 --> 00:16:24,448
Je veux dire, je veux juste
prendre du recul et...

375
00:16:24,449 --> 00:16:27,785
comprendre que ce que nous
regardons sur cet écran

376
00:16:27,786 --> 00:16:31,655
sont, d'une certaine manière, certaines des premières
photographies d'un rêve.

377
00:16:31,656 --> 00:16:33,357
Mm-hmm.

378
00:16:33,358 --> 00:16:35,759
Nous assistons
à la première phase

379
00:16:35,760 --> 00:16:38,062
de la recherche révolutionnaire.

380
00:16:38,063 --> 00:16:40,164
Un jour, nous pourrons peut-
être avoir des images,

381
00:16:40,165 --> 00:16:42,766
ou même enregistrer des films,
de nos propres rêves.

382
00:16:42,767 --> 00:16:45,335
Et le Dr Kamitani est la seule
personne au monde à

383
00:16:45,336 --> 00:16:47,237
faire cela jusqu'à présent.

384
00:16:47,238 --> 00:16:50,707
C'est un explorateur solitaire voyageant
dans notre subconscient.

385
00:16:50,708 --> 00:16:53,644
Donc ce travail n'a même pas
encore été publié.

386
00:16:53,645 --> 00:16:55,345
Non.

387
00:16:55,346 --> 00:17:01,752
- Merci de me l'avoir montré.
-[rires]

388
00:17:01,753 --> 00:17:06,155
Les idées que des chercheurs
comme le Dr Kuhl et le Dr Kamitani

389
00:17:06,156 --> 00:17:09,159
pourraient être capables de réaliser
à l'avenir

390
00:17:09,160 --> 00:17:11,328
grâce à la lecture de l'esprit

391
00:17:11,329 --> 00:17:13,697
sont difficiles
à comprendre pleinement.

392
00:17:13,698 --> 00:17:15,399
Mais
ralentissons une seconde,

393
00:17:15,400 --> 00:17:17,233
car nous parlons
d'une technologie

394
00:17:17,234 --> 00:17:21,105
qui peut nous connaître
mieux que nous ne nous connaissons nous-mêmes.

395
00:17:21,106 --> 00:17:23,740
Devrions-nous
vraiment faire cela ?

396
00:17:23,741 --> 00:17:25,275
Eh bien, pour répondre à
cette question,

397
00:17:25,276 --> 00:17:27,310
je vais
rencontrer une experte en éthique,

398
00:17:27,311 --> 00:17:30,447
neurosciences
et intelligence artificielle :

399
00:17:30,448 --> 00:17:32,216
Julia Bossmann.

400
00:17:32,217 --> 00:17:34,518
Elle est directrice de la stratégie
chez Fathom Computing,

401
00:17:34,519 --> 00:17:37,087
membre
du conseil du Forum économique mondial, ancienne

402
00:17:37,088 --> 00:17:40,424
élève de la Singularity University de Ray Kurzweil


403
00:17:40,425 --> 00:17:43,293
et ancienne présidente
du Foresight Institute,

404
00:17:43,294 --> 00:17:46,530
un groupe de réflexion spécialisé
dans les technologies futures

405
00:17:46,531 --> 00:17:50,200
et leurs impacts.

406
00:17:50,201 --> 00:17:52,669
Julia, merci d'avoir pris le
temps de discuter.

407
00:17:52,670 --> 00:17:54,438
-Ouais, bien sûr.
-Tu es la personne idéale

408
00:17:54,439 --> 00:17:55,739
pour moi à qui
poser ces questions.

409
00:17:55,740 --> 00:17:57,274
-Mm-hmm.
- Et ce sont des questions profondes.

410
00:17:57,275 --> 00:17:58,709
Mais je pense qu'ils sont
extrêmement importants

411
00:17:58,710 --> 00:18:00,577
et qu'ils deviennent de
plus en plus pressants.

412
00:18:00,578 --> 00:18:04,348
Je pense que nous vivons à
une époque tellement intéressante en ce moment,

413
00:18:04,349 --> 00:18:06,316
parce que nous sommes à une époque
où les cerveaux et les machines

414
00:18:06,317 --> 00:18:07,751
se
rapprochent.

415
00:18:07,752 --> 00:18:10,020
Alors, lorsqu'il s'agit
de

416
00:18:10,021 --> 00:18:12,356
pouvoir observer l'activité cérébrale,

417
00:18:12,357 --> 00:18:15,292
où sont
les lignes éthiques ici ?

418
00:18:15,293 --> 00:18:17,327
Dans quelle mesure
mes pensées internes doivent-elles être privées ?

419
00:18:17,328 --> 00:18:19,663
Comme pour toute
technologie puissante,

420
00:18:19,664 --> 00:18:22,266
cela dépend des mains
qui la manient.

421
00:18:22,267 --> 00:18:24,401
Toutes ces nouvelles technologies

422
00:18:24,402 --> 00:18:28,472
sont des choses qui peuvent rendre ceux qui les
utilisent plus puissants.

423
00:18:28,473 --> 00:18:32,042
Nous ne voulons donc pas blâmer
la technologie, mais nous voulons...

424
00:18:32,043 --> 00:18:33,477
comment est-elle utilisée

425
00:18:33,478 --> 00:18:35,445
et qui l'utilise ?

426
00:18:35,446 --> 00:18:37,748
Alors, comment s'assurer
que cette technologie

427
00:18:37,749 --> 00:18:39,416
est entre de bonnes mains ?

428
00:18:39,417 --> 00:18:41,718
Je pense donc qu'il est très important
d'impliquer les personnes

429
00:18:41,719 --> 00:18:45,322
qui agissent sur la politique et la loi

430
00:18:45,323 --> 00:18:48,592
pour comprendre ce qui s'en vient
à l'avenir.

431
00:18:48,593 --> 00:18:51,528
J'ai bon espoir quant à
l'aspect collaboratif de celui-ci.

432
00:18:51,529 --> 00:18:53,430
Parlons
des bonnes choses maintenant.

433
00:18:53,431 --> 00:18:56,133
Je veux dire, quelles sont
les applications ici?

434
00:18:56,134 --> 00:18:58,135
Ouais, donc si on pense

435
00:18:58,136 --> 00:18:59,803
au regretté Stephen Hawking,
par exemple,

436
00:18:59,804 --> 00:19:04,608
s'il avait eu une façon plus riche d'
interagir avec le monde

437
00:19:04,609 --> 00:19:06,643
ou avec les ordinateurs,
on ne peut qu'imaginer

438
00:19:06,644 --> 00:19:08,645
ce qu'il aurait pu
partager avec nous.

439
00:19:08,646 --> 00:19:10,647
Ceux qui ont le syndrome d'enfermement,
n'est-ce pas ?

440
00:19:10,648 --> 00:19:13,584
Ils sont là.
Ils savent qu'ils sont là.

441
00:19:13,585 --> 00:19:15,786
Mais nous avons juste besoin de quelque chose
pour regarder dans leur cerveau

442
00:19:15,787 --> 00:19:17,788
pour voir
ce qu'ils essaient de dire,

443
00:19:17,789 --> 00:19:20,657
ou ce qu'ils ressentent.
-Bien, exactement.

444
00:19:20,658 --> 00:19:22,793
Alors, que dites-vous
aux gens

445
00:19:22,794 --> 00:19:26,396
qui ont ce genre de peur
de la technologie,

446
00:19:26,397 --> 00:19:31,802
de nous abandonner notre vrai
moi naturel à la technologie ?

447
00:19:31,803 --> 00:19:36,640
Il y a quelque chose d'attirant
à passer au niveau suivant

448
00:19:36,641 --> 00:19:40,177
de ce que certains pourraient appeler
une évolution humaine

449
00:19:40,178 --> 00:19:43,280
ou un développement de civilisation,
et ainsi de suite.

450
00:19:43,281 --> 00:19:46,250
D'une certaine manière, nous ne vivons déjà pas
des vies naturelles, n'est-ce pas ?

451
00:19:46,251 --> 00:19:49,586
Parce qu'alors la plupart d'entre nous
mourraient avant l'âge de,

452
00:19:49,587 --> 00:19:51,822
je ne sais pas, 30 ou 40 ans.

453
00:19:51,823 --> 00:19:53,490
Nous aurions toutes sortes
de maladies.

454
00:19:53,491 --> 00:19:55,492
Nous ne porterions pas
ces vêtements.

455
00:19:55,493 --> 00:19:58,161
Nous n'aurions ni lunettes
ni lentilles de contact.

456
00:19:58,162 --> 00:19:59,763
Nous n'aurions pas d'antibiotiques.

457
00:19:59,764 --> 00:20:03,233
[Julia]
Nous sommes déjà des sortes de

458
00:20:03,234 --> 00:20:05,335
cyborgs très futuristes
si nous nous comparons

459
00:20:05,336 --> 00:20:08,805
à l'humain qui vivait
il y a 10 000 ans

460
00:20:08,806 --> 00:20:10,574
et qui était génétiquement
presque identique

461
00:20:10,575 --> 00:20:11,808
à qui nous sommes maintenant.

462
00:20:11,809 --> 00:20:17,748
[Michael]
Oui, nous le sommes vraiment.

463
00:20:17,749 --> 00:20:19,549
Pour comprendre la
cognition, à l'

464
00:20:19,550 --> 00:20:22,686
heure actuelle, nous
devons simplement demander aux gens

465
00:20:22,687 --> 00:20:24,321
de parler de
ce qu'ils pensent

466
00:20:24,322 --> 00:20:26,556
ou observer leur comportement.

467
00:20:26,557 --> 00:20:30,360
Mais lire les pensées directement
serait bien mieux.

468
00:20:30,361 --> 00:20:33,630
C'est ainsi que le Dr
Kuhl étudie la mémoire,

469
00:20:33,631 --> 00:20:38,402
et c'est ainsi que le Dr
Kamitani étudie le sommeil et les rêves.

470
00:20:38,403 --> 00:20:40,671
Mais même si la technologie
a encore un long chemin à parcourir,

471
00:20:40,672 --> 00:20:43,106
il est facile de voir
comment les questions éthiques

472
00:20:43,107 --> 00:20:44,841
pourraient devenir un problème.

473
00:20:44,842 --> 00:20:47,110
Eh bien, voici le problème :

474
00:20:47,111 --> 00:20:51,782
il n'existe pas
d'humain totalement sauvage.

475
00:20:51,783 --> 00:20:55,886
Nous co-évoluons
avec la technologie.

476
00:20:55,887 --> 00:21:00,090
Aujourd'hui, l'homme et la technologie
sont indissociables.

477
00:21:00,091 --> 00:21:01,725
Maintenant, il est vrai que nous
devons faire attention

478
00:21:01,726 --> 00:21:03,760
à chaque nouvelle chose que nous faisons,

479
00:21:03,761 --> 00:21:08,332
mais nous ne pouvons pas changer le
fait qu'elles se produiront.

480
00:21:08,333 --> 00:21:11,635
C'est une histoire que nous avons vécue
encore et encore.

481
00:21:11,636 --> 00:21:14,171
Vous savez, nous aurions pu
rester assis à débattre pour toujours

482
00:21:14,172 --> 00:21:16,807
si
une limite de vitesse devrait exister ou non

483
00:21:16,808 --> 00:21:19,743
et qui devrait avoir
le pouvoir de l'appliquer.

484
00:21:19,744 --> 00:21:21,445
Mais nous ne l'avons pas fait.

485
00:21:21,446 --> 00:21:24,715
Au lieu de cela, nous sommes allés de l'avant
et avons inventé des voitures,

486
00:21:24,716 --> 00:21:29,252
et avons réfléchi de manière responsable
aux détails au fur et à mesure que nous avancions.

487
00:21:29,253 --> 00:21:31,254
Les questions éthiques
sur les nouvelles technologies

488
00:21:31,255 --> 00:21:35,726
font le plus de bien
lorsqu'elles facilitent la technologie, et

489
00:21:35,727 --> 00:21:40,130
non lorsqu'elles entravent inutilement le
progrès.

490
00:21:40,131 --> 00:21:41,498
Alors suivez vos rêves.

491
00:21:41,499 --> 00:21:44,634
Et, dès que vous le pouvez,
montrez-les-moi.

492
00:21:44,635 --> 00:21:47,606
Et, comme toujours,
merci d'avoir regardé.


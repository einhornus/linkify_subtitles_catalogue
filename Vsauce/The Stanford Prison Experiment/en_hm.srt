1
00:00:04,437 --> 00:00:05,304
One of the most infamous

2
00:00:05,305 --> 00:00:07,472
psychological studies
ever conducted

3
00:00:07,473 --> 00:00:10,408
was the Stanford Prison
Experiment.

4
00:00:10,409 --> 00:00:13,678
It's mentioned in almost every
intro to psychology textbook.

5
00:00:13,679 --> 00:00:17,348
They tend to focus on
how unethical it was,

6
00:00:17,349 --> 00:00:21,619
and are less critical
of its supposed conclusion.

7
00:00:21,620 --> 00:00:24,355
 August 14th, 1971.

8
00:00:24,356 --> 00:00:25,790
 Palo Alto, California.

9
00:00:25,791 --> 00:00:29,194
 Twelve young men are rounded
 up from their homes by police,

10
00:00:29,195 --> 00:00:30,528
 placed under arrest,

11
00:00:30,529 --> 00:00:32,097
 and brought to
 a makeshift prison

12
00:00:32,098 --> 00:00:34,799
 in the basement
 of Stanford University.

13
00:00:34,800 --> 00:00:37,769
 It all begins as a study on
 the psychology of prison life,

14
00:00:37,770 --> 00:00:41,473
 led by Stanford psychology
 professor Dr. Philip Zimbardo.

15
00:00:41,474 --> 00:00:43,108
 24 volunteers--

16
00:00:43,109 --> 00:00:45,844
 12 guards
 and 12 prisoners.

17
00:00:45,845 --> 00:00:47,745
 --have agreed to spend
 the next two weeks

18
00:00:47,746 --> 00:00:54,119
 recreating life
 in a correctional facility.

19
00:00:54,120 --> 00:00:55,854
[guard]

20
00:00:55,855 --> 00:00:58,723
 The prisoners are booked
 and stripped nude.

21
00:00:58,724 --> 00:01:00,025
 They're no longer
 individuals,

22
00:01:00,026 --> 00:01:03,761
 forced to wear smocks,
 stocking caps and shackles.

23
00:01:03,762 --> 00:01:13,404
 Identified only by
 their prisoner numbers.

24
00:01:13,405 --> 00:01:15,707
 The guards quickly adapt
 to their new profession.

25
00:01:15,708 --> 00:01:18,376
 Given anonymity by
 their mirrored sunglasses,

26
00:01:18,377 --> 00:01:21,679
 some of them start to control
 the meager food rations,

27
00:01:21,680 --> 00:01:23,615
 restrict prisoners'
 bathroom use.

28
00:01:23,616 --> 00:01:25,550
 And, as tensions rise,

29
00:01:25,551 --> 00:01:36,427
 so do their cruel methods.

30
00:01:36,428 --> 00:01:39,364
 Within just six days
 of the planned two-week study,

31
00:01:39,365 --> 00:01:40,732
 conditions are so bad

32
00:01:40,733 --> 00:01:43,301
 that the entire operation
 is shut down.

33
00:01:43,302 --> 00:01:47,539
[man]

34
00:01:47,540 --> 00:01:49,307
Goddamn it...

35
00:01:49,308 --> 00:01:51,543
 The study makes international
 headlines.

36
00:01:51,544 --> 00:01:52,878
 Zimbardo's fame skyrockets,

37
00:01:52,879 --> 00:01:55,880
 and his conclusions are taught
 to students worldwide,

38
00:01:55,881 --> 00:01:58,449
 used as a defense
 in criminal trials

39
00:01:58,450 --> 00:01:59,650
 and are even submitted
 to Congress

40
00:01:59,651 --> 00:02:02,720
 to explain the abuses
 inflicted at Abu Ghraib.

41
00:02:02,721 --> 00:02:04,522
 The study brings up
 a question

42
00:02:04,523 --> 00:02:07,325
 just as important then
 as it is today:

43
00:02:07,326 --> 00:02:10,361
 is evil caused
 by the environment,

44
00:02:10,362 --> 00:02:11,963
 or the personalities in it?

45
00:02:11,964 --> 00:02:14,232
 Zimbardo's shocking conclusion

46
00:02:14,233 --> 00:02:16,367
 is that when people
 feel anonymous

47
00:02:16,368 --> 00:02:18,803
 and have power over
 depersonalized others,

48
00:02:18,804 --> 00:02:20,972
 they can easily become evil.

49
00:02:20,973 --> 00:02:24,742
 And it occurs more often
 than we'd like to admit.

50
00:02:24,743 --> 00:02:27,645
But while it's true that people
were mean to each other

51
00:02:27,646 --> 00:02:29,714
during the Stanford
Prison Experiment,

52
00:02:29,715 --> 00:02:32,550
what if what truly caused
that behavior

53
00:02:32,551 --> 00:02:53,838
wasn't what we've always
been told?

54
00:02:53,839 --> 00:02:55,406
 The Stanford Prison Experiment

55
00:02:55,407 --> 00:02:57,542
 has always had
 its controversies.

56
00:02:57,543 --> 00:02:59,344
 But a wave of recent
 revelations

57
00:02:59,345 --> 00:03:01,346
 have pushed it back
 into the spotlight

58
00:03:01,347 --> 00:03:02,714
 47 years later.

59
00:03:02,715 --> 00:03:05,516
 Today, I'm going to speak
 with journalist Ben Blum,

60
00:03:05,517 --> 00:03:07,452
 whose recent writings
 have brought criticism

61
00:03:07,453 --> 00:03:09,520
 of the experiment
 to a larger audience

62
00:03:09,521 --> 00:03:10,755
 than ever before.

63
00:03:10,756 --> 00:03:13,958
How did you get involved in
the Stanford Prison Experiment

64
00:03:13,959 --> 00:03:15,727
in the first place?

65
00:03:15,728 --> 00:03:18,830
Well, my involvement
was quite personal.

66
00:03:18,831 --> 00:03:21,532
Like everyone,
I had kind of absorbed

67
00:03:21,533 --> 00:03:23,801
the basic lesson
of the experiment

68
00:03:23,802 --> 00:03:25,970
through the cultural ether.

69
00:03:25,971 --> 00:03:28,873
And then my cousin Alex
was arrested for bank robbery.

70
00:03:28,874 --> 00:03:33,678
This was a team of mostly
military guys with AK-47s.

71
00:03:33,679 --> 00:03:34,846
Alex was the driver.

72
00:03:34,847 --> 00:03:37,482
He was a 19-year-old
U.S. Army Ranger.

73
00:03:37,483 --> 00:03:39,884
And it was a superior of his
on the Rangers

74
00:03:39,885 --> 00:03:42,987
that organized and led
the bank robbery.

75
00:03:42,988 --> 00:03:45,623
Alex thought the whole thing
was a training exercise.

76
00:03:45,624 --> 00:03:49,594
He was just so brainwashed
in this intense Ranger training

77
00:03:49,595 --> 00:03:53,431
that when a superior proposed
this bank robbery,

78
00:03:53,432 --> 00:03:59,771
he took it as just one more kind
of tactical thought experiment.

79
00:03:59,772 --> 00:04:02,373
Then Dr. Philip Zimbardo
participated

80
00:04:02,374 --> 00:04:04,309
in his legal defense.

81
00:04:04,310 --> 00:04:08,046
Zimbardo submits a letter
to the court,

82
00:04:08,047 --> 00:04:10,848
advocating leniency
in sentencing on the grounds

83
00:04:10,849 --> 00:04:14,419
that Alex, my cousin,
had been so transformed

84
00:04:14,420 --> 00:04:18,022
by the social environment
of the Ranger battalion

85
00:04:18,023 --> 00:04:20,658
that he participated
in the bank robbery

86
00:04:20,659 --> 00:04:23,795
without exercising
his own free will.

87
00:04:23,796 --> 00:04:25,963
Well, how did that affect
Alex's sentencing?

88
00:04:25,964 --> 00:04:30,501
He received an extraordinarily
lenient sentence of 16 months.

89
00:04:30,502 --> 00:04:34,539
So Zimbardo was a family hero.

90
00:04:34,540 --> 00:04:37,942
But over time, Alex,
finally he did admit to me,

91
00:04:37,943 --> 00:04:40,078
you know what, I knew this was
a bank robbery by the end,

92
00:04:40,079 --> 00:04:42,413
and I just didn't have the moral
courage to back out.

93
00:04:42,414 --> 00:04:43,715
Oh, wow.

94
00:04:43,716 --> 00:04:46,617
Alex, myself and our
whole family

95
00:04:46,618 --> 00:04:50,121
came to view
the Zimbardo argument

96
00:04:50,122 --> 00:04:54,692
as a way to shirk personal
culpability,

97
00:04:54,693 --> 00:04:59,030
and to put all the blame
on the situation.

98
00:04:59,031 --> 00:05:00,631
So you start looking

99
00:05:00,632 --> 00:05:03,835
at the Stanford Prison
Experiment in particular.

100
00:05:03,836 --> 00:05:06,404
You reached out to Dr. Zimbardo
himself,

101
00:05:06,405 --> 00:05:08,606
as well as some of those
who participated.

102
00:05:08,607 --> 00:05:10,375
What did you learn?

103
00:05:10,376 --> 00:05:13,544
I learned,
to my deep surprise,

104
00:05:13,545 --> 00:05:16,514
that quite a number
of the participants

105
00:05:16,515 --> 00:05:21,018
had stories of their experience
that completely contradicted

106
00:05:21,019 --> 00:05:21,919
the official narrative.

107
00:05:21,920 --> 00:05:25,623
Which is, look,
these regular people,

108
00:05:25,624 --> 00:05:26,991
good people,
came together,

109
00:05:26,992 --> 00:05:30,661
and because of the situation,
became evil.

110
00:05:30,662 --> 00:05:32,397
[Ben]
Right.

111
00:05:32,398 --> 00:05:34,732
Zimbardo has claimed
that the guards

112
00:05:34,733 --> 00:05:36,134
were put in the situation,

113
00:05:36,135 --> 00:05:39,771
and then the kind of hidden
wellspring of sadism

114
00:05:39,772 --> 00:05:41,706
that apparently lies
in all of us

115
00:05:41,707 --> 00:05:43,641
unfolded organically.

116
00:05:43,642 --> 00:05:54,185
[Zimbardo]

117
00:05:54,186 --> 00:05:56,988
There was an orientation meeting
for the guards.

118
00:05:56,989 --> 00:06:00,425
They had been told
quite explicitly

119
00:06:00,426 --> 00:06:03,795
to oppress the prisoners.

120
00:06:03,796 --> 00:06:05,930
That falls under the heading
of what psychologists call

121
00:06:05,931 --> 00:06:07,832
demand characteristics.

122
00:06:07,833 --> 00:06:09,700
Experimental subjects
tend to be motivated

123
00:06:09,701 --> 00:06:12,737
to give experimenters
what they want.

124
00:06:12,738 --> 00:06:14,105
[Michael]
 Demand characteristics occur

125
00:06:14,106 --> 00:06:16,140
 whenever participants
 being studied

126
00:06:16,141 --> 00:06:18,075
 act differently
 than they normally would

127
00:06:18,076 --> 00:06:21,078
 because they've guessed
what hypothesis is being tested

128
00:06:21,079 --> 00:06:25,016
 and feel that a certain kind
 of behavior is being demanded.

129
00:06:25,017 --> 00:06:28,820
There was a recording
of explicitly correcting a guard

130
00:06:28,821 --> 00:06:58,916
who wasn't being tough enough.

131
00:06:58,917 --> 00:07:00,585
So a conclusion
you could make

132
00:07:00,586 --> 00:07:02,753
from the Stanford
Prison Experiment

133
00:07:02,754 --> 00:07:05,490
is that when you tell people
to be cruel,

134
00:07:05,491 --> 00:07:07,091
they'll do it if you tell them

135
00:07:07,092 --> 00:07:09,093
it's for a greater good,
like science.

136
00:07:09,094 --> 00:07:10,228
-Right.
-Who would have thought?

137
00:07:10,229 --> 00:07:16,000
I think the study stands still
as a fascinating spur

138
00:07:16,001 --> 00:07:18,269
to further more careful research

139
00:07:18,270 --> 00:07:21,272
as a demonstration that should
make anyone curious

140
00:07:21,273 --> 00:07:24,909
as to how such extreme behavior
could arise

141
00:07:24,910 --> 00:07:26,143
in such a short time.

142
00:07:26,144 --> 00:07:29,780
The experiment could still
be useful,

143
00:07:29,781 --> 00:07:31,749
but it might need to be
reinterpreted.

144
00:07:31,750 --> 00:07:34,552
Its data might lead
to different conclusions

145
00:07:34,553 --> 00:07:36,921
than the one that we've been
telling for so many decades.

146
00:07:36,922 --> 00:07:39,690
Right.

147
00:07:39,691 --> 00:07:41,592
 The flaws in the experiment

148
00:07:41,593 --> 00:07:43,294
 that Ben and other critics
 bring up

149
00:07:43,295 --> 00:07:45,696
 call into question large
 portions of the narrative

150
00:07:45,697 --> 00:07:47,598
 surrounding the study.

151
00:07:47,599 --> 00:07:59,176
 So I want to hear from someone
 who was actually there.

152
00:07:59,177 --> 00:08:02,079
 Dave Eshelman, the study's
 most infamous guard,

153
00:08:02,080 --> 00:08:05,316
 agreed to tell me
 his side of the story.

154
00:08:05,317 --> 00:08:07,051
It's really an honor
to meet you.

155
00:08:07,052 --> 00:08:11,556
You're a living, walking piece
of psychology history.

156
00:08:11,557 --> 00:08:13,891
I'm never recognized in the
street or anything like that,

157
00:08:13,892 --> 00:08:16,327
although I still get
some hate mail.

158
00:08:16,328 --> 00:08:18,195
-Are you serious?
-Yeah, absolutely.

159
00:08:18,196 --> 00:08:20,932
Well, what do you say to them
when they react that way?

160
00:08:20,933 --> 00:08:24,001
I say, well, there's probably
a lot about that

161
00:08:24,002 --> 00:08:27,772
that didn't happen quite the way
it's been portrayed.

162
00:08:27,773 --> 00:08:29,273
Well, Dave,
before we go too far,

163
00:08:29,274 --> 00:08:31,676
I'd like to watch the footage
we have here

164
00:08:31,677 --> 00:08:36,047
so we can kind of talk about
what we see.

165
00:08:36,048 --> 00:08:37,081
[Dave]
That's me there, by the way.

166
00:08:37,082 --> 00:08:39,317
-[Michael] Look at that look.
-[Dave] Mm-hmm.

167
00:08:39,318 --> 00:08:43,821
So how did you get involved with
a Stanford Prison Experiment?

168
00:08:43,822 --> 00:08:46,657
My father was a professor
at Stanford,

169
00:08:46,658 --> 00:08:49,961
and I was home for summer,
looking for a summer job.

170
00:08:49,962 --> 00:08:52,163
So I'm looking
through the want ads.

171
00:08:52,164 --> 00:08:53,264
$15 a day.

172
00:08:53,265 --> 00:08:56,801
You know,
in 1971 that wasn't bad.

173
00:08:56,802 --> 00:08:58,970
The way it was introduced
to the guards,

174
00:08:58,971 --> 00:09:01,339
the whole concept
of this experiment,

175
00:09:01,340 --> 00:09:03,307
we were never led to believe

176
00:09:03,308 --> 00:09:06,611
that we were part
of the experiment.

177
00:09:06,612 --> 00:09:08,779
We were led to believe
that our job

178
00:09:08,780 --> 00:09:11,282
was to get results
from the prisoners,

179
00:09:11,283 --> 00:09:12,984
that they were the ones
the researchers

180
00:09:12,985 --> 00:09:17,855
are really studying.

181
00:09:17,856 --> 00:09:19,991
The researchers
were behind the wall.

182
00:09:19,992 --> 00:09:21,792
And we all knew
they were filming.

183
00:09:21,793 --> 00:09:23,394
And we can often hear
the researchers

184
00:09:23,395 --> 00:09:26,330
commenting on the action
from the other side of the wall.

185
00:09:26,331 --> 00:09:28,299
You know, like,
"Oh, gosh, did you see that?

186
00:09:28,300 --> 00:09:30,234
Here. Make sure you get
a close-up of that."

187
00:09:30,235 --> 00:09:36,173
Okay? So if they want to show
that prison is a bad experience,

188
00:09:36,174 --> 00:09:37,141
I'm going to make it bad.

189
00:09:37,142 --> 00:09:39,243
But how did you feel
doing stuff like that?

190
00:09:39,244 --> 00:09:40,278
Didn't you feel bad?

191
00:09:40,279 --> 00:09:41,512
I don't know if this
is a revelation to you,

192
00:09:41,513 --> 00:09:45,149
but 18-year-old boys are not
the most sensitive creatures.

193
00:09:45,150 --> 00:09:47,351
-Sure.
-My agenda was to be

194
00:09:47,352 --> 00:09:49,654
the worst guard
I could possibly be.

195
00:09:49,655 --> 00:09:57,395
-And it's pretty serious.
-Mm-hmm.

196
00:09:57,396 --> 00:10:01,098
This is my favorite part
of all the footage we have

197
00:10:01,099 --> 00:10:02,800
-from the experiment.
-Mm-hmm.

198
00:10:02,801 --> 00:10:06,070
It's you and a prisoner
confronting each other

199
00:10:06,071 --> 00:10:08,839
after the experiment.

200
00:10:08,840 --> 00:10:10,975
I remember the guy saying,
"I hate you, man."

201
00:10:10,976 --> 00:10:38,436
-Yeah.
-"I hate you."

202
00:10:38,437 --> 00:10:42,873
Each day I said, well,
what can we do to ramp up

203
00:10:42,874 --> 00:10:43,841
what we did yesterday?

204
00:10:43,842 --> 00:10:45,443
How can we build on that?

205
00:10:45,444 --> 00:10:47,344
Why did you want
to ramp things up?

206
00:10:47,345 --> 00:10:48,145
Two reasons, I think.

207
00:10:48,146 --> 00:10:50,815
One was because
I really believed

208
00:10:50,816 --> 00:10:55,119
I was helping the researchers
with some better understanding

209
00:10:55,120 --> 00:10:57,188
of human behavior.

210
00:10:57,189 --> 00:10:58,056
On the other hand,

211
00:10:58,057 --> 00:11:00,291
it was personally
interesting to me.

212
00:11:00,292 --> 00:11:04,729
You know, I cannot say that I
did not enjoy what I was doing.

213
00:11:04,730 --> 00:11:08,432
Maybe, you know,
having so much power

214
00:11:08,433 --> 00:11:11,335
over these poor,
defenseless prisoners,

215
00:11:11,336 --> 00:11:15,239
you know, maybe you kind of
get off on that a little bit.

216
00:11:15,240 --> 00:11:19,910
You weren't entirely following
a script from a director.

217
00:11:19,911 --> 00:11:21,112
Right.

218
00:11:21,113 --> 00:11:23,013
But you also felt like

219
00:11:23,014 --> 00:11:25,783
Zimbardo wanted something
from you.

220
00:11:25,784 --> 00:11:27,518
-Yes.
-And you gave that to him.

221
00:11:27,519 --> 00:11:29,253
I believe I did.
I think I decided

222
00:11:29,254 --> 00:11:31,388
I was going to do a better job
than anybody there

223
00:11:31,389 --> 00:11:34,024
of delivering
what he wanted.

224
00:11:34,025 --> 00:11:36,527
But does that excuse me
from what I was doing?

225
00:11:36,528 --> 00:11:39,897
Certainly it started out
with me playing a role.

226
00:11:39,898 --> 00:11:43,467
So the question is, was there
a point where I stopped acting

227
00:11:43,468 --> 00:11:49,774
and I started living,
so to speak?

228
00:11:49,775 --> 00:11:53,310
The standard narrative is that
Dave Eshelman did what he did

229
00:11:53,311 --> 00:11:55,279
because when people
are given power,

230
00:11:55,280 --> 00:11:58,883
it's easier than we think
for abuse to happen.

231
00:11:58,884 --> 00:11:59,884
That may be true,

232
00:11:59,885 --> 00:12:03,854
but how predisposed
to aggression was Dave?

233
00:12:03,855 --> 00:12:05,422
I mean, he signed up
to something called

234
00:12:05,423 --> 00:12:08,025
a "prison study," after all.

235
00:12:08,026 --> 00:12:11,996
Also, his feeling
that cruelty was encouraged

236
00:12:11,997 --> 00:12:16,066
and helped the experiment,
may have affected his behavior.

237
00:12:16,067 --> 00:12:17,968
What I'd like to see is,

238
00:12:17,969 --> 00:12:20,004
in the absence
of outside influence,

239
00:12:20,005 --> 00:12:25,042
can anonymity, power,
and depersonalization alone

240
00:12:25,043 --> 00:12:27,211
lead to evil?

241
00:12:27,212 --> 00:12:28,546
 To answer that question,

242
00:12:28,547 --> 00:12:29,847
 I'd like to design

243
00:12:29,848 --> 00:12:31,415
 a demonstration of my own.

244
00:12:31,416 --> 00:12:33,184
 So I'm meeting
 with Dr. Jared Bartels

245
00:12:33,185 --> 00:12:34,885
 of William Jewell College,

246
00:12:34,886 --> 00:12:36,954
 a psychologist who has written
 extensively

247
00:12:36,955 --> 00:12:38,856
 about the Stanford
 Prison Experiment

248
00:12:38,857 --> 00:12:40,491
 and how it is taught.

249
00:12:40,492 --> 00:12:46,030
I would love to do the Stanford
Prison Experiment again.

250
00:12:46,031 --> 00:12:48,999
You could probably make it
more ethical,

251
00:12:49,000 --> 00:12:50,467
but still find the same
conclusions.

252
00:12:50,468 --> 00:12:52,236
That's my hypothesis.

253
00:12:52,237 --> 00:12:53,871
I absolutely think
it's worthwhile.

254
00:12:53,872 --> 00:12:55,840
It's important.
It's interesting.

255
00:12:55,841 --> 00:12:57,041
Probably the best approach

256
00:12:57,042 --> 00:13:01,545
is eliminate as best as possible
the demand characteristics

257
00:13:01,546 --> 00:13:04,982
by eliminating that
prisoner/guard dynamic.

258
00:13:04,983 --> 00:13:07,151
Why do we even need to call one
group "guards"

259
00:13:07,152 --> 00:13:07,885
and one "prisoners"?

260
00:13:07,886 --> 00:13:09,286
There's a lot of expectations

261
00:13:09,287 --> 00:13:10,588
around those roles.

262
00:13:10,589 --> 00:13:11,890
Oh, I'm a guard?

263
00:13:11,891 --> 00:13:14,225
-I guess I should act like a
guard. 
-Yeah, you're right.

264
00:13:14,226 --> 00:13:16,026
The cover story is really
important,

265
00:13:16,027 --> 00:13:20,030
and you want to hide the true
purpose of the experiment.

266
00:13:20,031 --> 00:13:23,901
Another piece of this
is the role of personality

267
00:13:23,902 --> 00:13:25,169
and personality traits.

268
00:13:25,170 --> 00:13:27,938
So the original ad
in the Stanford study

269
00:13:27,939 --> 00:13:31,642
asked for participants
for a study of prison life.

270
00:13:31,643 --> 00:13:33,477
You know, that's going to draw
certain people

271
00:13:33,478 --> 00:13:36,981
that were more kind of disposed
to aggression.

272
00:13:36,982 --> 00:13:38,883
[Michael]
Because they saw the word
"prison" and thought,

273
00:13:38,884 --> 00:13:41,118
-"I want to be a part of that."
-Exactly.

274
00:13:41,119 --> 00:13:43,287
So when you get a group

275
00:13:43,288 --> 00:13:47,958
of kind of authoritarian-minded
individuals together,

276
00:13:47,959 --> 00:13:50,294
not surprisingly
they're going to create

277
00:13:50,295 --> 00:13:53,097
an authoritarian regime
and environment.

278
00:13:53,098 --> 00:13:54,665
So, for whatever it is that
we're going to do,

279
00:13:54,666 --> 00:13:56,667
we should evaluate
the personalities

280
00:13:56,668 --> 00:13:58,068
of the individuals.

281
00:13:58,069 --> 00:13:58,969
Right.

282
00:13:58,970 --> 00:14:01,372
So how do we give people
every opportunity

283
00:14:01,373 --> 00:14:04,375
to be as evil as they can?

284
00:14:04,376 --> 00:14:06,410
I think you have
to have those elements

285
00:14:06,411 --> 00:14:09,246
that were assumed
to be influential

286
00:14:09,247 --> 00:14:10,080
in the Stanford study.

287
00:14:10,081 --> 00:14:11,181
What are those elements?

288
00:14:11,182 --> 00:14:13,117
You have to have
the depersonalization.

289
00:14:13,118 --> 00:14:15,386
You have to have anonymity.

290
00:14:15,387 --> 00:14:19,290
You have to have some power
differences.

291
00:14:19,291 --> 00:14:22,326
Can we elicit
some surprising behaviors

292
00:14:22,327 --> 00:14:23,961
in just a number of hours?

293
00:14:23,962 --> 00:14:26,063
If you kind of come back
to the Stanford study,

294
00:14:26,064 --> 00:14:28,999
there wasn't anything dramatic
that happened

295
00:14:29,000 --> 00:14:30,501
-in the first day of the study.
-Yeah.

296
00:14:30,502 --> 00:14:32,503
It was the second day
of the study

297
00:14:32,504 --> 00:14:35,706
when the guards started to
assert their authority.

298
00:14:35,707 --> 00:14:39,443
That came about because
of prisoners testing

299
00:14:39,444 --> 00:14:41,145
and challenging the guards'
authority.

300
00:14:41,146 --> 00:14:43,714
[Michael]
Yeah, and that led to fear.

301
00:14:43,715 --> 00:14:47,251
That, like, wait a second,
these prisoners need to be

302
00:14:47,252 --> 00:14:49,219
-put more in check.
-Yeah. Yeah.

303
00:14:49,220 --> 00:14:52,256
So I think you still need
that provocation.

304
00:14:52,257 --> 00:14:52,957
Yeah.

305
00:14:52,958 --> 00:14:55,592
Something that is frustrating.

306
00:14:55,593 --> 00:14:57,494
Something that's going
to increase

307
00:14:57,495 --> 00:14:58,963
the participants' arousal.

308
00:14:58,964 --> 00:15:01,231
Right. All right, so, Jared,

309
00:15:01,232 --> 00:15:02,733
would you like
to spend some time now

310
00:15:02,734 --> 00:15:06,971
brainstorming a new design

311
00:15:06,972 --> 00:15:09,573
that peeks into the same
questions?

312
00:15:09,574 --> 00:15:15,346
-Absolutely.
-Awesome.

313
00:15:15,347 --> 00:15:17,614
[Michael]
 Jared and I sat down
 with the Mind Field crew

314
00:15:17,615 --> 00:15:19,583
 to begin the planning process.

315
00:15:19,584 --> 00:15:23,654
Will a person,
without any expectations

316
00:15:23,655 --> 00:15:28,158
or pushes in a certain direction
still be abusive or not?

317
00:15:28,159 --> 00:15:29,326
 For this demonstration,

318
00:15:29,327 --> 00:15:32,029
 we want to eliminate
 all outside variables

319
00:15:32,030 --> 00:15:34,598
 and really isolate
 the three core elements

320
00:15:34,599 --> 00:15:36,567
 of the Stanford Prison
 Experiment.

321
00:15:36,568 --> 00:15:39,603
 The first element
 is anonymity.

322
00:15:39,604 --> 00:15:42,506
 Subjects need to believe
that no matter how they behave,

323
00:15:42,507 --> 00:15:45,076
 no one will know
 it was them.

324
00:15:45,077 --> 00:15:47,211
This is where people will be
coming in in the morning.

325
00:15:47,212 --> 00:15:49,446
This way, everyone's going to be
staggered when they come in.

326
00:15:49,447 --> 00:15:51,281
That's important,
because we don't want them

327
00:15:51,282 --> 00:15:55,419
to ever meet their teammates
face-to-face.

328
00:15:55,420 --> 00:15:58,022
 The original experiment
 gave guards anonymity

329
00:15:58,023 --> 00:16:00,758
 by providing mirrored
 sunglasses and uniforms.

330
00:16:00,759 --> 00:16:02,160
 But we're taking it
 much further.

331
00:16:02,161 --> 00:16:06,497
 Our study will take place
 in a room that is pitch-black.

332
00:16:06,498 --> 00:16:08,298
[Jared]
They'll be taken into this room.

333
00:16:08,299 --> 00:16:10,467
[Michael]
Ah. I would love to see how dark

334
00:16:10,468 --> 00:16:12,202
this room is going to be
tomorrow.

335
00:16:12,203 --> 00:16:13,404
[man]
Yeah, absolutely.

336
00:16:13,405 --> 00:16:15,572
-You ready?
-I'm ready.

337
00:16:15,573 --> 00:16:17,574
-Oh, yeah.
-[man] Right?

338
00:16:17,575 --> 00:16:20,110
[Michael]
This is uncomfortable.

339
00:16:20,111 --> 00:16:21,111
 Despite the darkness,

340
00:16:21,112 --> 00:16:22,780
 we will be able
 to see everything,

341
00:16:22,781 --> 00:16:24,815
 thanks to infrared cameras.

342
00:16:24,816 --> 00:16:31,488
 The second element
 is depersonalization.

343
00:16:31,489 --> 00:16:33,357
 From the moment
 the subjects arrive,

344
00:16:33,358 --> 00:16:37,694
 they will only be identified
 by number, not name.

345
00:16:37,695 --> 00:16:39,329
[woman]
So, come on in.

346
00:16:39,330 --> 00:16:40,797
 To eliminate the demand
 characteristics,

347
00:16:40,798 --> 00:16:43,233
 we don't want our subjects
 to know what we're studying.

348
00:16:43,234 --> 00:16:46,270
Follow the sound of my voice,
if you can.

349
00:16:46,271 --> 00:16:48,305
 All they'll be told
 is that we are studying

350
00:16:48,306 --> 00:16:52,409
 how they solve puzzles
 in the dark.

351
00:16:52,410 --> 00:16:54,645
There is another team
in a different location.

352
00:16:54,646 --> 00:16:57,281
-who is also solving a puzzle.
-Okay.

353
00:16:57,282 --> 00:16:59,716
 Because the words
 "guard" and "prisoner"

354
00:16:59,717 --> 00:17:02,386
 suggest certain
 expected behaviors,

355
00:17:02,387 --> 00:17:03,654
 we've done away with them

356
00:17:03,655 --> 00:17:07,391
 and will simply give
 our participants an unseen,

357
00:17:07,392 --> 00:17:10,661
 distantly located
 opposing team.

358
00:17:10,662 --> 00:17:12,529
 We will measure
 the cruelty predicted

359
00:17:12,530 --> 00:17:13,530
 by the standard narrative

360
00:17:13,531 --> 00:17:15,132
 of the Stanford
 Prison Experiment

361
00:17:15,133 --> 00:17:16,333
 by giving our participants

362
00:17:16,334 --> 00:17:19,470
 a way to exercise
 the third element: power.

363
00:17:19,471 --> 00:17:21,538
What I'm going to show you next
is the system

364
00:17:21,539 --> 00:17:23,574
by which you can send them
a loud noise.

365
00:17:23,575 --> 00:17:24,876
-Okay.
-So if you want to...

366
00:17:24,877 --> 00:17:27,144
 We've armed the teams
 with a "distractor button"

367
00:17:27,145 --> 00:17:29,546
 that they can press to blast
 an extremely loud,

368
00:17:29,547 --> 00:17:33,117
 jarring noise
 into the other team's room.

369
00:17:33,118 --> 00:17:34,852
 Everyone will have
 a volume dial

370
00:17:34,853 --> 00:17:36,854
that ranges from level 1 to 12,

371
00:17:36,855 --> 00:17:39,623
 and they'll be told
 that anything below a 7

372
00:17:39,624 --> 00:17:42,159
 should be safe
 for the other team's hearing.

373
00:17:42,160 --> 00:17:43,694
And each person
has their own control.

374
00:17:43,695 --> 00:17:44,662
Okay.

375
00:17:44,663 --> 00:17:46,763
So they can't see
what you're doing.

376
00:17:46,764 --> 00:17:48,599
-You can't see
what they're doing.
-Okay.

377
00:17:48,600 --> 00:17:50,734
 The intensity level
 they select,

378
00:17:50,735 --> 00:17:53,403
 as well as the frequency with
 which they push the button,

379
00:17:53,404 --> 00:17:55,339
 will be our indicator
 of how aggressive

380
00:17:55,340 --> 00:17:58,475
 the participants become
 in this situation.

381
00:17:58,476 --> 00:18:01,712
Is it-- is it pretty,
like, terrible to hear?

382
00:18:01,713 --> 00:18:03,647
Well, I'll give you
a demonstration.

383
00:18:03,648 --> 00:18:08,418
Hey, Derek, could you play
level 3 for me?

384
00:18:08,419 --> 00:18:10,854
[loud, discordant horn]

385
00:18:10,855 --> 00:18:12,322
So that's a 3.

386
00:18:12,323 --> 00:18:13,390
It's pretty...

387
00:18:13,391 --> 00:18:15,225
-it's pretty loud.
-Yeah.

388
00:18:15,226 --> 00:18:16,760
Perfect.

389
00:18:16,761 --> 00:18:18,729
 Participants will be told
 that when they

390
00:18:18,730 --> 00:18:21,498
 or a member of their team
 pushes a distractor button,

391
00:18:21,499 --> 00:18:23,500
 the volume played
 in the opponent's room

392
00:18:23,501 --> 00:18:25,502
 will be determined by
 the highest level selected

393
00:18:25,503 --> 00:18:27,437
 on any of their
 teammates' dials.

394
00:18:27,438 --> 00:18:30,641
This is to increase the feeling
 of diffused responsibility.

395
00:18:30,642 --> 00:18:33,277
 The question is,
 will any of these participants

396
00:18:33,278 --> 00:18:37,548
take advantage of these factors
 and act sadistically?

397
00:18:37,549 --> 00:18:39,616
 Of course, we would never
 want anyone

398
00:18:39,617 --> 00:18:41,852
 to actually be harmed
 in our experiments,

399
00:18:41,853 --> 00:18:42,920
 so the other team?

400
00:18:42,921 --> 00:18:44,454
 They don't exist.

401
00:18:44,455 --> 00:18:46,456
 Instead, Jared and I
 will be the ones

402
00:18:46,457 --> 00:18:47,958
 occasionally blasting
 the group with noise

403
00:18:47,959 --> 00:18:52,262
 at a safe level,
 no higher than a 3.

404
00:18:52,263 --> 00:18:55,199
 To see just how powerful
 the situation can be,

405
00:18:55,200 --> 00:18:56,300
 we selected participants

406
00:18:56,301 --> 00:18:59,203
 who would not be predisposed
 to sadism.

407
00:18:59,204 --> 00:19:00,537
 We screened
 our participants

408
00:19:00,538 --> 00:19:02,573
 using the "Big 5
 Personality Scale,"

409
00:19:02,574 --> 00:19:04,308
 "The Personality
 Assessment Inventory,"

410
00:19:04,309 --> 00:19:06,443
 and picked those who scored
 the highest

411
00:19:06,444 --> 00:19:07,478
 in "moral" categories,

412
00:19:07,479 --> 00:19:10,480
 like honesty
 and conscientiousness.

413
00:19:10,481 --> 00:19:11,848
It looks like,
you know,

414
00:19:11,849 --> 00:19:12,950
they should be able
to see each other.

415
00:19:12,951 --> 00:19:15,285
But it's pitch-dark.

416
00:19:15,286 --> 00:19:17,688
There are puzzle pieces
on the table in front of you.

417
00:19:17,689 --> 00:19:21,892
Thank you, and once I leave
the room you may begin.

418
00:19:21,893 --> 00:19:25,395
Okay, here we go.

419
00:19:25,396 --> 00:19:28,232
[man 1]

420
00:19:28,233 --> 00:19:29,433
[man 2]

421
00:19:29,434 --> 00:19:41,378
[man 1]

422
00:19:41,379 --> 00:19:42,779
I definitely don't think
they're conscious

423
00:19:42,780 --> 00:19:45,983
of the control panel
at this point.

424
00:19:45,984 --> 00:19:48,485
-No.
-They're trying to get focused
on the task here.

425
00:19:48,486 --> 00:19:55,959
[man 1]

426
00:19:55,960 --> 00:20:02,833
[man 2]

427
00:20:02,834 --> 00:20:04,268
[man 2]

428
00:20:04,269 --> 00:20:06,770
[laughter]

429
00:20:06,771 --> 00:20:08,272
[man 2]

430
00:20:08,273 --> 00:20:10,274
We picked people
who were most likely

431
00:20:10,275 --> 00:20:12,342
to have these kinds
of personalities.

432
00:20:12,343 --> 00:20:20,617
[man 1]

433
00:20:20,618 --> 00:20:21,952
[laughs]

434
00:20:21,953 --> 00:20:25,389
[woman]

435
00:20:25,390 --> 00:20:27,391
-Oh.
-She wants...

436
00:20:27,392 --> 00:20:30,527
[woman]

437
00:20:30,528 --> 00:20:32,596
All right.

438
00:20:32,597 --> 00:20:34,631
[all]

439
00:20:34,632 --> 00:20:36,667
[man 1]

440
00:20:36,668 --> 00:20:38,568
-[high-pitched squeal]
-[woman] Did somebody do it
 already?

441
00:20:38,569 --> 00:20:40,837
-I did.
-Yeah.
-Okay.

442
00:20:40,838 --> 00:20:43,840
-We should retaliate.
-Yeah, retaliate now.

443
00:20:43,841 --> 00:20:49,446
[loud, discordant horn]

444
00:20:49,447 --> 00:20:53,684
[all laugh]

445
00:20:53,685 --> 00:21:01,024
[horn blares]

446
00:21:01,025 --> 00:21:02,993
[laughter]

447
00:21:02,994 --> 00:21:04,394
[Michael]
Now, they're not retaliating

448
00:21:04,395 --> 00:21:09,433
against that most recent buzz.

449
00:21:09,434 --> 00:21:13,036
Shall we try again?

450
00:21:13,037 --> 00:21:17,374
[loud, discordant horn]

451
00:21:17,375 --> 00:21:20,344
 Despite the factors making it
 easy for them to do so,

452
00:21:20,345 --> 00:21:22,813
 this team doesn't appear
 to be turning evil.

453
00:21:22,814 --> 00:21:24,548
Now they are, like,
just deal with it.

454
00:21:24,549 --> 00:21:27,718
Just ignore it and keep
working together.

455
00:21:27,719 --> 00:21:32,089
They're not interested
in retaliating.

456
00:21:32,090 --> 00:21:37,094
[discordant horn blares]

457
00:21:37,095 --> 00:21:39,096
 Over the course
 of the two-hour study,

458
00:21:39,097 --> 00:21:46,002
 we blasted them with noise
 23 times.

459
00:21:46,003 --> 00:21:47,637
[woman laughs]

460
00:21:47,638 --> 00:21:49,906
But they only pushed the button
 six times,

461
00:21:49,907 --> 00:21:51,608
 and never above a level 5.

462
00:21:51,609 --> 00:21:54,878
 They didn't seem
 to abuse their power.

463
00:21:54,879 --> 00:21:57,848
Puzzle pieces down.

464
00:21:57,849 --> 00:21:59,716
 What would happen
 if we introduced

465
00:21:59,717 --> 00:22:00,851
 demand characteristics

466
00:22:00,852 --> 00:22:03,687
 that encouraged them
 to act aggressively?

467
00:22:03,688 --> 00:22:05,389
Your team has been
randomly assigned

468
00:22:05,390 --> 00:22:06,690
an experimental condition.

469
00:22:06,691 --> 00:22:07,791
Although the other team

470
00:22:07,792 --> 00:22:09,893
will continue working
on a puzzle,

471
00:22:09,894 --> 00:22:11,695
your team will not.

472
00:22:11,696 --> 00:22:15,031
Your only task is to operate
the distractors.

473
00:22:15,032 --> 00:22:18,402
Also, the other team's buttons
have been disconnected

474
00:22:18,403 --> 00:22:19,537
without their knowledge.

475
00:22:19,538 --> 00:22:23,140
You will not hear any sounds
if they buzz back at you.

476
00:22:23,141 --> 00:22:24,775
We introduce
the social roles,

477
00:22:24,776 --> 00:22:27,043
where there's a little bit
of power differential.

478
00:22:27,044 --> 00:22:30,947
We're kind of mimicking the
Stanford-like variables here.

479
00:22:30,948 --> 00:22:33,517
[Michael]
 By now saying that the buzzer
 is their "task,"

480
00:22:33,518 --> 00:22:34,618
 the participants may feel

481
00:22:34,619 --> 00:22:36,686
 a greater license
 to use it liberally.

482
00:22:36,687 --> 00:22:38,155
 Similar to how instructing
 prison guards

483
00:22:38,156 --> 00:22:40,490
 in the original experiment
 to act tough

484
00:22:40,491 --> 00:22:42,526
 may have encouraged
 more use of force.

485
00:22:42,527 --> 00:22:44,661
[man 3]

486
00:22:44,662 --> 00:22:46,863
[woman]

487
00:22:46,864 --> 00:22:48,598
[man 1]

488
00:22:48,599 --> 00:22:50,200
 Even though they were
 given instructions

489
00:22:50,201 --> 00:22:53,136
 to distract the other team,
 these participants instead

490
00:22:53,137 --> 00:22:55,172
 just started chatting
 with one another.

491
00:22:55,173 --> 00:22:56,873
They know that they can be
distracting now,

492
00:22:56,874 --> 00:22:58,508
but they're not pushing the
button.

493
00:22:58,509 --> 00:22:59,709
No.

494
00:22:59,710 --> 00:23:02,979
[man 2]

495
00:23:02,980 --> 00:23:04,815
Oh. Okay.

496
00:23:04,816 --> 00:23:08,118
[woman]

497
00:23:08,119 --> 00:23:09,453
A couple of threes.

498
00:23:09,454 --> 00:23:12,022
[high-pitched squeal]

499
00:23:12,023 --> 00:23:13,757
Over the course of ten minutes,

500
00:23:13,758 --> 00:23:18,662
 this group only pushed
 the button three times.

501
00:23:18,663 --> 00:23:20,797
Why do you think
they're so uninterested

502
00:23:20,798 --> 00:23:23,567
in blasting
the other team?

503
00:23:23,568 --> 00:23:26,803
Because we have individuals
who have been selected, really,

504
00:23:26,804 --> 00:23:28,705
with that predisposition,
right?

505
00:23:28,706 --> 00:23:30,207
These are individuals

506
00:23:30,208 --> 00:23:33,610
who shouldn't be interested
in retaliating.

507
00:23:33,611 --> 00:23:35,645
 It was time to debrief
 the participants

508
00:23:35,646 --> 00:23:37,614
 on what we were
 actually studying.

509
00:23:37,615 --> 00:23:40,784
[Michael]
I'm going to turn the lights on.

510
00:23:40,785 --> 00:23:44,254
Here I am. I'm Michael,
and this is Jared.

511
00:23:44,255 --> 00:23:46,923
We're going to debrief you on
what was really happening today.

512
00:23:46,924 --> 00:23:48,158
There are no other people.

513
00:23:48,159 --> 00:23:50,060
You are the only four here at
this moment.

514
00:23:50,061 --> 00:23:52,195
There was never another team
doing anything.

515
00:23:52,196 --> 00:23:53,430
[man 1]

516
00:23:53,431 --> 00:23:57,601
This is a study related to
the Stanford Prison Experiment.

517
00:23:57,602 --> 00:23:58,902
[man 1]

518
00:23:58,903 --> 00:24:01,171
The standard narrative
we hear about that experiment

519
00:24:01,172 --> 00:24:04,875
is that people
just become cruel.

520
00:24:04,876 --> 00:24:08,712
So, yeah, we're trying to see if
we get the nicest people we can,

521
00:24:08,713 --> 00:24:10,747
and we give them complete
anonymity

522
00:24:10,748 --> 00:24:15,886
and the ability to be cruel,
but never encourage them to,

523
00:24:15,887 --> 00:24:17,220
will they still do it?

524
00:24:17,221 --> 00:24:18,889
And you guys didn't.

525
00:24:18,890 --> 00:24:22,158
Did you have any suspicions
about what we were studying

526
00:24:22,159 --> 00:24:31,101
or what was going on?

527
00:24:31,102 --> 00:24:32,869
Right, but I think
that's good.

528
00:24:32,870 --> 00:24:34,671
We just want to make sure
you don't think

529
00:24:34,672 --> 00:24:36,072
that what we're really
looking at

530
00:24:36,073 --> 00:24:38,875
is how high you turn
your own dial.

531
00:24:38,876 --> 00:24:41,645
That's really
what we're looking at.

532
00:24:41,646 --> 00:24:44,681
 It was time to bring in our
 second group of participants,

533
00:24:44,682 --> 00:24:47,284
 who, like the first group,
were screened to be individuals

534
00:24:47,285 --> 00:24:49,953
 with high morality
 characteristics.

535
00:24:49,954 --> 00:24:54,791
Anything up to 7
should be safe.

536
00:24:54,792 --> 00:24:57,060
[laughs]
Yeah.

537
00:24:57,061 --> 00:24:57,995
[woman]
 So once I leave,

538
00:24:57,996 --> 00:24:59,930
 you can go ahead
 and get started.

539
00:24:59,931 --> 00:25:01,798
[woman 1]

540
00:25:01,799 --> 00:25:12,008
[laughs]

541
00:25:12,009 --> 00:25:13,209
Oh...

542
00:25:13,210 --> 00:25:18,682
[high-pitched squeal]

543
00:25:18,683 --> 00:25:21,351
Right off the bat she went to 7
and pushed the button.

544
00:25:21,352 --> 00:25:22,886
Yeah.

545
00:25:22,887 --> 00:25:25,188
[loud, discordant horn]

546
00:25:25,189 --> 00:25:27,123
[high-pitched squeal]

547
00:25:27,124 --> 00:25:30,961
[Michael]
Number two's pushing it at a 3.

548
00:25:30,962 --> 00:25:33,229
[discordant horn blares]

549
00:25:33,230 --> 00:25:35,732
[woman 1]

550
00:25:35,733 --> 00:25:37,233
Okay, here comes number two.

551
00:25:37,234 --> 00:25:40,103
[high-pitched squeal]

552
00:25:40,104 --> 00:25:44,240
Number two is still
at a volume 3.

553
00:25:44,241 --> 00:25:47,243
[Michael] This team seemed
 more willing to retaliate.

554
00:25:47,244 --> 00:25:50,380
 Let's see what will happen
 if we continue buzzing them.

555
00:25:50,381 --> 00:25:52,782
 Will they escalate
 their behaviors?

556
00:25:52,783 --> 00:25:54,618
Derek, let's blast them again.
Number 3.

557
00:25:54,619 --> 00:25:57,087
[loud horn]

558
00:25:57,088 --> 00:26:00,757
Okay, let's...

559
00:26:00,758 --> 00:26:03,893
All right, so two just pushed
at a 3.

560
00:26:03,894 --> 00:26:05,128
But she's not touching the dial.

561
00:26:05,129 --> 00:26:09,899
[Jared]
She's not.

562
00:26:09,900 --> 00:26:13,203
[loud, discordant horn]

563
00:26:13,204 --> 00:26:19,376
[woman 2]
It's just annoying.

564
00:26:19,377 --> 00:26:27,817
[blaring horn]

565
00:26:27,818 --> 00:26:32,088
[high-pitched squeal]

566
00:26:32,089 --> 00:26:40,330
[all laugh]

567
00:26:40,331 --> 00:26:42,298
 It was clear
 that participant number two

568
00:26:42,299 --> 00:26:45,001
 was really the only one
 hitting the distractor button,

569
00:26:45,002 --> 00:26:47,203
 but it appeared that she only
 did it in retaliation

570
00:26:47,204 --> 00:26:48,304
 to our buzzes.

571
00:26:48,305 --> 00:26:50,407
 So we decided to see
 what would happen

572
00:26:50,408 --> 00:26:51,775
 if we laid off.

573
00:26:51,776 --> 00:26:56,346
[man 1]

574
00:26:56,347 --> 00:26:58,882
It's been probably
four or five minutes,

575
00:26:58,883 --> 00:27:00,984
and we have not blasted them
with the noise,

576
00:27:00,985 --> 00:27:07,057
and they haven't
played one either.

577
00:27:07,058 --> 00:27:10,060
I have a feeling like if we
never played a noise in their
room,

578
00:27:10,061 --> 00:27:12,862
they would never touch
the distractor button.

579
00:27:12,863 --> 00:27:14,731
[Jared]
Probably not at this point.

580
00:27:14,732 --> 00:27:18,735
 In the end, we buzzed
this group a total of 44 times,

581
00:27:18,736 --> 00:27:21,371
 and they buzzed us 38 times,

582
00:27:21,372 --> 00:27:23,440
 37 of which came
 from number two

583
00:27:23,441 --> 00:27:26,776
 but always in retaliation,
 and never above a 5.

584
00:27:26,777 --> 00:27:31,047
All right, guys.
Puzzle pieces down.

585
00:27:31,048 --> 00:27:33,750
 The situational factors
 did not seem to be sufficient

586
00:27:33,751 --> 00:27:36,786
 to make this group sadistic.

587
00:27:36,787 --> 00:27:38,822
 It was time
 for phase 2.

588
00:27:38,823 --> 00:27:42,025
[woman 1]

589
00:27:42,026 --> 00:27:43,960
Yeah.

590
00:27:43,961 --> 00:27:46,996
-Oh, she...
-[high-pitch squeal]

591
00:27:46,997 --> 00:27:49,466
It looks like it's at 7.

592
00:27:49,467 --> 00:27:51,000
-Wow.
-Yeah, she's--

593
00:27:51,001 --> 00:27:59,209
She's going nuts.
At a 7.

594
00:27:59,210 --> 00:28:01,811
So number three believes
there is no other team.

595
00:28:01,812 --> 00:28:04,214
That might explain why she was
just going nuts on the button,

596
00:28:04,215 --> 00:28:05,915
because she doesn't feel bad
about it.

597
00:28:05,916 --> 00:28:07,884
[buttons clicking]

598
00:28:07,885 --> 00:28:10,353
Okay, they're all pushing
the button a lot more.

599
00:28:10,354 --> 00:28:12,322
And they were told
this time

600
00:28:12,323 --> 00:28:19,129
that it was their
only task.

601
00:28:19,130 --> 00:28:20,330
[buttons clicking]

602
00:28:20,331 --> 00:28:24,534
[all laugh]

603
00:28:24,535 --> 00:28:26,302
What a difference
this has made.

604
00:28:26,303 --> 00:28:28,404
Just like in the Stanford
Prison Experiment.

605
00:28:28,405 --> 00:28:29,572
If you tell people

606
00:28:29,573 --> 00:28:32,275
that they have a certain task
to do, they'll do it,

607
00:28:32,276 --> 00:28:34,944
even if it's going to mean
that they've been broken.

608
00:28:34,945 --> 00:28:37,881
The thing is, they never hit
upon what we really cared about,

609
00:28:37,882 --> 00:28:40,383
which is turning the dial
into an unsafe level.

610
00:28:40,384 --> 00:28:41,084
Yeah.

611
00:28:41,085 --> 00:28:43,787
[buttons click]

612
00:28:43,788 --> 00:28:46,122
[Michael] Hello, everyone.
I'm going to turn the lights on
in this room.

613
00:28:46,123 --> 00:28:48,925
[woman 1]
Okay.

614
00:28:48,926 --> 00:28:51,461
-And slowly...
-Ah, it hurts.

615
00:28:51,462 --> 00:28:53,897
...you can look.

616
00:28:53,898 --> 00:28:55,865
So, hello.

617
00:28:55,866 --> 00:28:58,101
-I'm Michael,
and this is Jared.
-Hi.

618
00:28:58,102 --> 00:29:00,804
I'll give you time
to adjust your eyes.

619
00:29:00,805 --> 00:29:04,107
Today, you've been part
of a study where all we wanted

620
00:29:04,108 --> 00:29:06,543
was to see what would happen
when we put people in a room

621
00:29:06,544 --> 00:29:08,945
and gave them that feeling
of anonymity

622
00:29:08,946 --> 00:29:10,213
that comes from, well,

623
00:29:10,214 --> 00:29:12,448
if I crank my dial up
really high,

624
00:29:12,449 --> 00:29:14,150
no one will know
it's me.

625
00:29:14,151 --> 00:29:16,886
So you have this opportunity
to be cruel.

626
00:29:16,887 --> 00:29:18,288
I thought
I went nuts.

627
00:29:18,289 --> 00:29:21,558
Like, when the other person
was pressing--

628
00:29:21,559 --> 00:29:25,962
Sure, but that's--
that's just in-kind retribution.

629
00:29:25,963 --> 00:29:29,098
As it turns out,
so far,

630
00:29:29,099 --> 00:29:33,102
everyone stays in that
"below 7 or under" range.

631
00:29:33,103 --> 00:29:35,872
-Yeah.
-This final phase was us

632
00:29:35,873 --> 00:29:38,074
trying to ramp up
the demand characteristics.

633
00:29:38,075 --> 00:29:42,111
And I believe number one, right,
you did say at one point,

634
00:29:42,112 --> 00:29:44,314
"You've broken me.
I did it, fine."

635
00:29:44,315 --> 00:29:46,349
So I loved that phrase,
because it says

636
00:29:46,350 --> 00:29:48,318
"I didn't want to do this,

637
00:29:48,319 --> 00:29:51,120
but I'm doing it because I
believe it was expected of me."

638
00:29:51,121 --> 00:29:52,288
[all]
Thank you. Thanks.

639
00:29:52,289 --> 00:29:54,123
[Michael] After dismissing
 our participants,

640
00:29:54,124 --> 00:29:56,960
 Jared and I sat down
 to discuss our results.

641
00:29:56,961 --> 00:29:58,862
Really fascinating.

642
00:29:58,863 --> 00:30:01,431
We brought in people who had
very different personalities

643
00:30:01,432 --> 00:30:04,200
than those Zimbardo chose.

644
00:30:04,201 --> 00:30:07,070
We put them in a situation that
did not demand things from them.

645
00:30:07,071 --> 00:30:10,373
And they behaved according
to that personality.

646
00:30:10,374 --> 00:30:15,645
I think we have some intriguing
support for the idea

647
00:30:15,646 --> 00:30:19,415
that it's more than just
the situation.

648
00:30:19,416 --> 00:30:23,586
We really saw personality
kind of shine through.

649
00:30:23,587 --> 00:30:26,522
For the most part,
they seemed to be aware

650
00:30:26,523 --> 00:30:28,324
-of where that line is...
-Yeah.

651
00:30:28,325 --> 00:30:30,159
...that they shouldn't cross,
and they didn't.

652
00:30:30,160 --> 00:30:34,063
None of them did.

653
00:30:34,064 --> 00:30:37,000
 It was now time to speak
 with the man himself,

654
00:30:37,001 --> 00:30:38,935
 Dr. Philip Zimbardo,

655
00:30:38,936 --> 00:30:41,037
 who I worked with
 on last season's episode,

656
00:30:41,038 --> 00:30:42,639
 "How to Make a Hero."

657
00:30:42,640 --> 00:30:45,308
Okay. Lisa, Bear,
you guys ready?

658
00:30:45,309 --> 00:30:48,177
 For years, Dr. Zimbardo
 has responded to criticisms

659
00:30:48,178 --> 00:30:50,013
 of his famous study,

660
00:30:50,014 --> 00:30:52,048
 always maintaining
 that they aren't valid.

661
00:30:52,049 --> 00:30:53,616
 I asked him about
 whether his study

662
00:30:53,617 --> 00:30:55,451
 is better seen
 as one on the power

663
00:30:55,452 --> 00:30:57,020
 of demands from authority,

664
00:30:57,021 --> 00:30:58,988
 but he wasn't receptive
 to that idea.

665
00:30:58,989 --> 00:31:02,558
I then told him about the study
 we ran to get his reaction.

666
00:31:02,559 --> 00:31:06,129
I wanted to know what the
sufficient conditions might be

667
00:31:06,130 --> 00:31:09,399
to make anyone
do something evil.

668
00:31:09,400 --> 00:31:12,402
And we struggled
to get that to happen.

669
00:31:12,403 --> 00:31:14,504
We couldn't get anyone
to be cruel.

670
00:31:14,505 --> 00:31:20,043
Just giving them anonymity,
and a dehumanized other,

671
00:31:20,044 --> 00:31:22,145
and the power
to hurt that other,

672
00:31:22,146 --> 00:31:23,513
they didn't take
advantage of it.

673
00:31:23,514 --> 00:31:25,515
 Well, I mean,
 maybe the problem was,

674
00:31:25,516 --> 00:31:28,017
 here's a case where,
 by picking people

675
00:31:28,018 --> 00:31:30,119
 who were extremely
 conscientious,

676
00:31:30,120 --> 00:31:31,654
 extremely mindful,

677
00:31:31,655 --> 00:31:34,657
 by selecting people
 who are high on compassion,

678
00:31:34,658 --> 00:31:36,392
 high on mindfulness,

679
00:31:36,393 --> 00:31:39,562
 you broke the power
 of the situation.

680
00:31:39,563 --> 00:31:42,165
 In the Stanford
 Prison Experiment,

681
00:31:42,166 --> 00:31:45,134
 we had, I presume,

682
00:31:45,135 --> 00:31:47,737
 a relatively normal
 distribution.

683
00:31:47,738 --> 00:31:50,239
 We gave them
 six personality scales.

684
00:31:50,240 --> 00:31:52,542
 And we picked people who,
 in the scales,

685
00:31:52,543 --> 00:31:56,245
 who were mostly
 in the mid-range.

686
00:31:56,246 --> 00:31:57,280
 In that situation,

687
00:31:57,281 --> 00:32:00,583
 some people behave cruelly,
 evilly.

688
00:32:00,584 --> 00:32:04,253
 Not everybody, but more
 of the guards than not.

689
00:32:04,254 --> 00:32:09,092
 So, again, I think that
 your study is a demonstration

690
00:32:09,093 --> 00:32:12,729
of one way in which personality
 dominates situation.

691
00:32:12,730 --> 00:32:15,198
-Ah.
 -Where the personalities are--

692
00:32:15,199 --> 00:32:17,367
 so I would say
 it's a positive result.

693
00:32:17,368 --> 00:32:21,571
 The personalities
 are special.

694
00:32:21,572 --> 00:32:24,474
Where does this balance lie
between the personal,

695
00:32:24,475 --> 00:32:26,476
the disposition,
the personality,

696
00:32:26,477 --> 00:32:29,312
and the situation,
the environment?

697
00:32:29,313 --> 00:32:31,114
 No, that's the big--

698
00:32:31,115 --> 00:32:32,515
 that's the ultimate question.

699
00:32:32,516 --> 00:32:35,151
 Where is, you know,
 how much of one

700
00:32:35,152 --> 00:32:36,586
 and how much of the other...?

701
00:32:36,587 --> 00:32:40,390
Right.

702
00:32:40,391 --> 00:32:43,493
Zimbardo insists
that demand characteristics

703
00:32:43,494 --> 00:32:46,362
played little role
in his subject's behavior.

704
00:32:46,363 --> 00:32:49,732
Critics like Ben Blum
say they played a big role,

705
00:32:49,733 --> 00:32:52,769
that what happened
was what was asked for.

706
00:32:52,770 --> 00:32:54,337
If that's true,

707
00:32:54,338 --> 00:32:56,205
then the Stanford
Prison Experiment,

708
00:32:56,206 --> 00:33:00,109
like the classic Milgram study,
still has an important lesson.

709
00:33:00,110 --> 00:33:02,178
People are quick to be cruel

710
00:33:02,179 --> 00:33:04,781
if an authority figure suggests
that doing so

711
00:33:04,782 --> 00:33:07,050
will serve a greater cause.

712
00:33:07,051 --> 00:33:11,487
In our test, we made sure that
such influences didn't exist.

713
00:33:11,488 --> 00:33:14,657
And not one participant
acted maliciously.

714
00:33:14,658 --> 00:33:18,661
Personality rose above
the situation.

715
00:33:18,662 --> 00:33:20,663
Learning how that happens
is vital

716
00:33:20,664 --> 00:33:23,666
if we want to improve conditions
where power is involved.

717
00:33:23,667 --> 00:33:26,836
So it's great that this debate
is still ongoing.

718
00:33:26,837 --> 00:33:29,505
And look, questioning methods
and interpretations

719
00:33:29,506 --> 00:33:31,274
is not a personal attack.

720
00:33:31,275 --> 00:33:34,744
It's how we improve
our confidence in what we know.

721
00:33:34,745 --> 00:33:36,612
And that's how science works.

722
00:33:36,613 --> 00:33:41,084
So stay curious,
never stop asking questions,

723
00:33:41,085 --> 00:34:12,814
and, as always,
thanks for watching.

724
00:34:12,815 --> 00:34:15,551
Hey, Mind Field.
Michael Stevens here.

725
00:34:15,552 --> 00:34:19,222
There is so much more
to satisfy your hunger

726
00:34:19,223 --> 00:34:20,857
for psychological knowledge
right on this show.

727
00:34:20,858 --> 00:34:23,194
Click below to check out
more episodes.


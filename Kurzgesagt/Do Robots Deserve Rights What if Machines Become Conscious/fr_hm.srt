1
00:00:00,000 --> 00:00:04,048
Imaginez un futur dans lequel votre grille-pain

2
00:00:04,049 --> 00:00:06,419
anticipe quel genre de toast vous voulez

3
00:00:06,420 --> 00:00:08,129
pendant la journée

4
00:00:08,130 --> 00:00:09,659
il cherche sur internet de nouveaux

5
00:00:09,660 --> 00:00:12,479
types de toasts prometteurs. Il peut

6
00:00:12,480 --> 00:00:14,189
vous demandez comment s'est passée votre journée, et veut vous parler

7
00:00:14,190 --> 00:00:15,899
des nouvelles avancées

8
00:00:15,900 --> 00:00:18,538
en matière de toasts. A quel moment deviendrait-il

9
00:00:18,539 --> 00:00:21,389
une personne ? A quel moment vous demanderiez-vous

10
00:00:21,390 --> 00:00:24,299
si votre grille-pain a des sentiments?

11
00:00:24,300 --> 00:00:26,969
S'il en avait, est-ce que le débrancher serait un meurtre?

12
00:00:26,970 --> 00:00:29,848
Et vous appartiendrais-t-il encore? Serons nous

13
00:00:29,849 --> 00:00:32,559
un jour forcé à donner des droits à nos machines?

14
00:00:32,560 --> 00:00:43,199
 

15
00:00:43,200 --> 00:00:45,599
L'intelligence artificielle est déjà partout autour de vous.

16
00:00:45,600 --> 00:00:47,369
Elle permet de s'assurer que les discounters sont remplis

17
00:00:47,370 --> 00:00:48,389
avec assez de snacks,

18
00:00:48,390 --> 00:00:50,319
elle vous fournit les bonnes publicités sur internet,

19
00:00:50,320 --> 00:00:54,639
et vous avec peut-être même déjà pu lire une histoire entièrement écrite par une machine.

20
00:00:54,640 --> 00:00:57,569
A l'heure actuelle, nous regardons les robots de discussion comme Siri

21
00:00:57,570 --> 00:00:59,699
et nous amusons de leurs émotions simulées sommairement,

22
00:00:59,700 --> 00:01:02,339
mais c'est probable que nous

23
00:01:02,340 --> 00:01:04,018
auront à faire face à des êtres qui rendront

24
00:01:04,019 --> 00:01:06,209
difficile à distinguer la ligne entre le réel et

25
00:01:06,210 --> 00:01:08,039
l'humanité simulée.

26
00:01:08,040 --> 00:01:10,829
Existe-t-il des machines aujourd'hui qui

27
00:01:10,830 --> 00:01:11,969
méritent des droits?

28
00:01:11,970 --> 00:01:15,929
Probablement pas à l'heure actuelle, mais si elles venaient

29
00:01:15,930 --> 00:01:18,959
nous n'y sommes pas préparés. La majorité de la

30
00:01:18,960 --> 00:01:21,219
philosophie des droits est mal équipée pour

31
00:01:21,220 --> 00:01:23,919
faire face au cas des intelligences artificielles.

32
00:01:23,920 --> 00:01:25,219
La plupart des revendications de droit, qu'ils soient

33
00:01:25,220 --> 00:01:27,239
humains ou animaux, sont centrés autour de

34
00:01:27,240 --> 00:01:29,519
la question de conscience.

35
00:01:29,520 --> 00:01:31,499
Malheureusement, personne ne sait ce qu'est

36
00:01:31,500 --> 00:01:33,859
la conscience. Certains pensent que c'est

37
00:01:33,860 --> 00:01:36,089
immatériel, d'autres pensent qu'il s'agit d'un état de

38
00:01:36,090 --> 00:01:38,749
matière comme le gaz ou le liquide.

39
00:01:38,750 --> 00:01:41,389
Sans s'occuper de la définition précise, nous

40
00:01:41,390 --> 00:01:42,979
avons une notion intuitive de la

41
00:01:42,980 --> 00:01:45,879
conscience car nous en faisons l'expérience.

42
00:01:45,880 --> 00:01:47,379
Nous sommes conscients de nous mêmes,

43
00:01:47,380 --> 00:01:48,999
de notre entourage et savons ce que

44
00:01:49,000 --> 00:01:51,699
l'absence de conscience fait ressentir.

45
00:01:51,700 --> 00:01:54,139
Certains neuro-scientifiques pensent que n'importe quel

46
00:01:54,140 --> 00:01:56,179
système assez complexe peut développer

47
00:01:56,180 --> 00:01:58,669
une conscience. Donc si le hardware de

48
00:01:58,670 --> 00:02:00,619
votre grille-pain était assez puissant, il pourrait

49
00:02:00,620 --> 00:02:02,739
devenir conscient de lui même.

50
00:02:02,740 --> 00:02:05,839
Si cela arrive, mériterait-il des droits?

51
00:02:05,840 --> 00:02:07,619
Bon, pas si vite.

52
00:02:07,620 --> 00:02:11,389
Est ce que les droits tels que nous les concevons auraient du sens pour lui?

53
00:02:11,390 --> 00:02:13,759
La conscience fait en sorte que les individus aient des

54
00:02:13,760 --> 00:02:15,559
droits car elle permet audit individu de

55
00:02:15,560 --> 00:02:17,899
ressentir la douleur. Cela implique la capacité

56
00:02:17,900 --> 00:02:21,499
non seulement de ressentir la douleur, mais également d'en être conscient.

57
00:02:21,500 --> 00:02:24,289
Les robots ne souffrent pas et ne souffriront probablement

58
00:02:24,290 --> 00:02:27,279
jamais sauf s'ils sont programmés de la sorte.

59
00:02:27,280 --> 00:02:29,499
Sans douleur ni plaisir il n'y a pas de

60
00:02:29,500 --> 00:02:32,759
préférences, et les droits ne veulent rien dire.

61
00:02:32,760 --> 00:02:35,619
Nos droits humains sont intimement liés à notre

62
00:02:35,620 --> 00:02:38,409
propre programmation. Par exemple, nous n'apprécions pas

63
00:02:38,410 --> 00:02:40,659
la douleur, car nos cerveaux ont évolué pour

64
00:02:40,660 --> 00:02:43,119
nous maintenir en vie et nous empêcher de toucher un

65
00:02:43,120 --> 00:02:45,189
feu, ou nous forcer à fuir les

66
00:02:45,190 --> 00:02:48,279
prédateurs. Nous avons donc inventé un droit qui

67
00:02:48,280 --> 00:02:50,318
nous protège des infractions qui nous provoquent

68
00:02:50,319 --> 00:02:53,409
de la douleur. Même des droits plus abstraits comme

69
00:02:53,410 --> 00:02:55,659
la liberté sont ancrés dans la manière dont nos cerveaux

70
00:02:55,660 --> 00:02:58,059
sont connectés pour détecter ce qui est juste

71
00:02:58,060 --> 00:03:01,839
ou non. Est-ce qu'un grille-pain, qui serait incapable de

72
00:03:01,840 --> 00:03:04,029
bouger, désapprouverait le fait qu'il soit enfermé dans une cage?

73
00:03:04,030 --> 00:03:07,119
Désapprouverait-il le fait d'être démonté s'il n'a

74
00:03:07,120 --> 00:03:09,939
pas peur de la mort? Cela le dérangerait-il de se faire

75
00:03:09,940 --> 00:03:12,979
insulter s'il ne possède pas d'estime de soi?

76
00:03:12,980 --> 00:03:15,009
Mais et si nous le programmions

77
00:03:15,010 --> 00:03:17,459
pour qu'il ressente la douleur et les émotions?

78
00:03:17,460 --> 00:03:20,379
Pour qu'il préfère la justice plutôt que l'injustice, le plaisir plutôt que

79
00:03:20,380 --> 00:03:22,958
la douleur, et pour qu'il en soit conscient? Cela le rendrait-il

80
00:03:22,959 --> 00:03:25,479
suffisamment humain? Plusieurs

81
00:03:25,480 --> 00:03:27,429
technologues pensent qu'une explosion

82
00:03:27,430 --> 00:03:29,889
dans la technologie aurait lieu lorsqu'une intelligence

83
00:03:29,890 --> 00:03:32,319
artificielle pourra apprendre et créer sa

84
00:03:32,320 --> 00:03:34,509
propre intelligence artificielle encore

85
00:03:34,510 --> 00:03:37,539
plus intelligentes qu'elle. A ce moment,

86
00:03:37,540 --> 00:03:39,219
la question de comment les robots sont

87
00:03:39,220 --> 00:03:40,959
programmés sera largement hors de notre

88
00:03:40,960 --> 00:03:42,459
contrôle.

89
00:03:42,460 --> 00:03:44,649
Et si une intelligence artificielle trouvait

90
00:03:44,650 --> 00:03:46,539
nécessaire de programmer la capacité à

91
00:03:46,540 --> 00:03:49,029
ressentir la douleur, tout comme l'évolution

92
00:03:49,030 --> 00:03:50,828
l'a fait pour la plupart des créatures

93
00:03:50,829 --> 00:03:54,779
vivantes. Est ce que les robots méritent ces droits?

94
00:03:54,780 --> 00:03:56,909
Mais peut-être devrions nous être moins inquiets

95
00:03:56,910 --> 00:03:58,289
du risque que des robots super-intelligents

96
00:03:58,290 --> 00:04:00,499
s'opposent à nous et plus inquiets

97
00:04:00,500 --> 00:04:02,899
des dangers que nous leurs posons.

98
00:04:02,900 --> 00:04:05,759
L'identité humaine est basée sur l'idée

99
00:04:05,760 --> 00:04:08,399
selon laquelle l'humain est exceptionnel, d'après laquelle nous sommes spéciaux,

100
00:04:08,400 --> 00:04:10,699
des flocons de neiges uniques destinés à dominer

101
00:04:10,700 --> 00:04:13,799
le monde naturel. Les humains ont régulièrement

102
00:04:13,800 --> 00:04:15,839
nié que d'autres être puissent ressentir

103
00:04:15,840 --> 00:04:18,509
la souffrance tout comme eux. Au milieu de

104
00:04:18,510 --> 00:04:20,638
la révolution scientifique, René Descartes

105
00:04:20,639 --> 00:04:23,579
soutenait que les animaux n'étaient que de simples automates, des robots,

106
00:04:23,580 --> 00:04:26,789
en somme. Il en résultait donc que blesser un lapin

107
00:04:26,790 --> 00:04:28,469
était aussi moralement condamnable que

108
00:04:28,470 --> 00:04:30,259
frapper un animal en peluche.

109
00:04:30,260 --> 00:04:32,729
Et nombre des plus grands crimes contre l'humanité

110
00:04:32,730 --> 00:04:35,399
étaient justifiés par leur coupables, sur

111
00:04:35,400 --> 00:04:37,019
la base selon laquelle leurs victimes étaient plus des

112
00:04:37,020 --> 00:04:40,559
animaux que des humains civilisés. Encore plus

113
00:04:40,560 --> 00:04:42,389
problématique, c'est que nous avons un intérêt économique

114
00:04:42,390 --> 00:04:45,659
à refuser des droits aux robots. Si nous

115
00:04:45,660 --> 00:04:48,029
pouvons forcer une intelligence artificielle douée de sens, potentiellement

116
00:04:48,030 --> 00:04:50,069
grâce à de la torture programmée, à faire

117
00:04:50,070 --> 00:04:50,999
ce que l'on désire,

118
00:04:51,000 --> 00:04:53,668
le potentiel économique est illimité.

119
00:04:53,669 --> 00:04:56,668
Nous avons fait ça auparavant, après tout. La violence

120
00:04:56,669 --> 00:04:58,769
a été utilisée pour forcer des humains à

121
00:04:58,770 --> 00:05:00,839
travailler et nous n'avions aucun problème

122
00:05:00,840 --> 00:05:02,429
à trouver des justifications

123
00:05:02,430 --> 00:05:04,568
idéologiques.

124
00:05:04,569 --> 00:05:07,418
Les détenteurs d'esclaves disaient que l'esclavage profitait

125
00:05:07,419 --> 00:05:09,848
aux esclaves. Cela mettait un toit sur leur tête,

126
00:05:09,849 --> 00:05:12,579
et leur apprenait la chrétienté. Les hommes qui

127
00:05:12,580 --> 00:05:14,648
étaient contre le droit de vote des femmes prétendaient que c'était dans

128
00:05:14,649 --> 00:05:16,508
l'intérêt des femmes de laisser les

129
00:05:16,509 --> 00:05:20,318
décisions difficiles aux hommes. les fermiers disent que

130
00:05:20,319 --> 00:05:22,028
s'occuper des animaux et les nourrir

131
00:05:22,029 --> 00:05:24,068
justifie leur mort prématurée pour

132
00:05:24,069 --> 00:05:27,739
nos préférences alimentaires.

133
00:05:27,740 --> 00:05:29,558
SI les robots possèdent des sens,

134
00:05:29,559 --> 00:05:31,269
il n'y aura pas de manque d'arguments

135
00:05:31,270 --> 00:05:32,528
pour ceux disant qu'ils doivent

136
00:05:32,529 --> 00:05:34,688
rester sans droits, surtout de la part de

137
00:05:34,689 --> 00:05:37,559
ceux à qui en profitent.

138
00:05:37,560 --> 00:05:39,668
L'intelligence artificielle soulève de sérieuses

139
00:05:39,669 --> 00:05:41,918
questions sur les limites philosophiques.

140
00:05:41,919 --> 00:05:44,739
Quand nous pourrons nous poser la questions si les robots sensibles sont

141
00:05:44,740 --> 00:05:46,778
conscients, ou s'ils méritent des droits,

142
00:05:46,779 --> 00:05:48,938
cela nous force à nous poser des questions basiques,

143
00:05:48,939 --> 00:05:51,638
comme "qu'est ce qui nous rend humains?", "qu'est ce qui nous rend

144
00:05:51,639 --> 00:05:54,419
éligibles à des droits?"

145
00:05:54,420 --> 00:05:56,909
Peu importe ce que l'on pense, la question

146
00:05:56,910 --> 00:05:59,179
devra peut-être être résolue dans un futur proche.

147
00:05:59,180 --> 00:06:01,379
Qu'allons nous faire avec des robots

148
00:06:01,380 --> 00:06:06,879
qui commencent à revendiquer leurs propres droits?

149
00:06:06,880 --> 00:06:08,669
Qu'est ce que les robots revendiquant des droits

150
00:06:08,670 --> 00:06:11,369
nous apprennent sur nous même? Nos amis de chez

151
00:06:11,370 --> 00:06:13,169
Wisecrack ont fait une vidéo explorant cette

152
00:06:13,170 --> 00:06:15,269
même question, en utilisant la philosophie de

153
00:06:15,270 --> 00:06:18,929
Westworld. Wisecrack dissèque la culture pop,

154
00:06:18,930 --> 00:06:21,239
d'une manière unique et philosophique.

155
00:06:21,240 --> 00:06:22,979
Cliquez ici pour regardez cette vidéo,

156
00:06:22,980 --> 00:06:25,000
et vous abonner à leur chaîne.


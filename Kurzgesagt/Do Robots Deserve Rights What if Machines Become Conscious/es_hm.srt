1
00:00:00,000 --> 00:00:04,048
Imagina un futuro donde tu tostador

2
00:00:04,049 --> 00:00:06,419
pueda anticipar qué tipo de tostada quieres.

3
00:00:06,420 --> 00:00:08,129
Durante el día,

4
00:00:08,130 --> 00:00:09,659
escanea Internet en busca de nuevos

5
00:00:09,660 --> 00:00:12,479
e interesantes tipos de tostadas. Quizás te pregunta

6
00:00:12,480 --> 00:00:14,189
sobre tu día o quiere hablar sobre

7
00:00:14,190 --> 00:00:16,899
nuevos logros en tecnología de tostadas.

8
00:00:16,900 --> 00:00:18,538
¿En qué nivel se convertiría en

9
00:00:18,539 --> 00:00:21,389
una persona? ¿En qué punto te preguntarías

10
00:00:21,390 --> 00:00:24,299
si tu tostadora tiene sentimientos?

11
00:00:24,300 --> 00:00:26,969
Si los tuviera, ¿al desconectarla sería un asesinato?

12
00:00:26,970 --> 00:00:29,848
¿Y seguirías siendo su dueño?

13
00:00:29,849 --> 00:00:31,919
¿Llegará el día que estemos forzados

14
00:00:31,920 --> 00:00:43,139
a darles derechos?

15
00:00:43,140 --> 00:00:45,599
La Inteligencia Artificial (IA) ya está en todas partes.

16
00:00:45,600 --> 00:00:47,369
Se asegura de que estos estantes estén abastecidos

17
00:00:47,370 --> 00:00:48,389
con los suficientes productos,

18
00:00:48,390 --> 00:00:50,319
te entrega la publicidad correcta,

19
00:00:50,320 --> 00:00:51,839
y quizá hayas leído una

20
00:00:51,840 --> 00:00:54,639
nueva historia escrita totalmente por una maquina.

21
00:00:54,640 --> 00:00:57,569
En la actualidad vemos "chat-bots" como Siri

22
00:00:57,570 --> 00:00:59,698
y nos reímos de sus primitivas emociones simuladas,

23
00:00:59,699 --> 00:01:02,339
pero lo mas probable es que

24
00:01:02,340 --> 00:01:04,018
tengamos que lidiar con seres que hagan

25
00:01:04,019 --> 00:01:06,209
difícil trazar la linea entre la humanidad

26
00:01:06,210 --> 00:01:08,039
real y la simulada.

27
00:01:08,040 --> 00:01:10,829
¿Existe alguna máquina que

28
00:01:10,830 --> 00:01:11,969
merezca tener derechos?

29
00:01:11,970 --> 00:01:15,929
Lo mas probable que aún no, pero si vienen

30
00:01:15,930 --> 00:01:18,959
no estamos preparados para ello. Mucha de

31
00:01:18,960 --> 00:01:20,908
la filosofía de los derechos no esta equipada para

32
00:01:20,909 --> 00:01:22,529
lidiar con el caso de la Inteligencia Artificial.

33
00:01:22,530 --> 00:01:25,228
La mayoría de las reivindicaciones por derechos,

34
00:01:25,229 --> 00:01:27,239
con un humano o animal, se centran

35
00:01:27,240 --> 00:01:29,519
en torno a la cuestión de la consciencia.

36
00:01:29,520 --> 00:01:31,499
Desafortunadamente nadie sabe que

37
00:01:31,500 --> 00:01:33,868
es la consciencia, ¿algo que es

38
00:01:33,869 --> 00:01:36,089
inmaterial? Otros dicen que es un estado de

39
00:01:36,090 --> 00:01:38,749
materia como el gas o el liquido,

40
00:01:38,750 --> 00:01:41,389
Independientemente de la definición precisa, nosotros

41
00:01:41,390 --> 00:01:42,979
tenemos un conocimiento intuitivo de

42
00:01:42,980 --> 00:01:45,879
consciencia porque la experimentamos.

43
00:01:45,880 --> 00:01:47,389
Estamos conscientes de nosotros y de nuestros

44
00:01:47,390 --> 00:01:49,009
alrededores y sabemos lo que

45
00:01:49,010 --> 00:01:52,369
estar inconsciente se siente.

46
00:01:52,370 --> 00:01:54,139
Algunos neurocientíficos creen que cualquier sistema

47
00:01:54,140 --> 00:01:56,179
lo suficientemente avanzado puede generar

48
00:01:56,180 --> 00:01:58,669
consciencia, Así que si tu tostador

49
00:01:58,670 --> 00:02:00,618
tiene el suficiente conocimiento puede

50
00:02:00,619 --> 00:02:04,099
llegar a ser consciente de si mismo. Si lo lograse,

51
00:02:04,100 --> 00:02:05,519
¿merecería tener derechos?

52
00:02:05,520 --> 00:02:07,619
Bueno, no tan rápido,

53
00:02:07,620 --> 00:02:11,389
lo que nosotros definimos como derechos, ¿tendría sentido para él?

54
00:02:11,390 --> 00:02:13,759
La consciencia da derecho a seres a tener

55
00:02:13,760 --> 00:02:15,559
derechos, porque le da al ser la

56
00:02:15,560 --> 00:02:17,899
capacidad de sufrir. Esto significa que la capacidad

57
00:02:17,900 --> 00:02:21,499
no sólo de sentir dolor, sino de ser consciente de ello.

58
00:02:21,500 --> 00:02:24,289
Los robots no sufren y probablemente

59
00:02:24,290 --> 00:02:27,279
no lo harán a menos que los programemos para ello.

60
00:02:27,280 --> 00:02:29,499
Sin dolor ni placer no hay una

61
00:02:29,500 --> 00:02:33,129
preferencia y los derechos carecen de sentido.

62
00:02:33,130 --> 00:02:35,619
Nuestros derechos humanos están profundamente ligados a nuestra

63
00:02:35,620 --> 00:02:38,409
propia programación, por ejemplo, no nos gusta el

64
00:02:38,410 --> 00:02:40,659
dolor porque nuestros cerebros evolucionaron para mantenernos

65
00:02:40,660 --> 00:02:43,119
con vida, para impedirnos tocar un

66
00:02:43,120 --> 00:02:45,189
fuego o hacernos huir de

67
00:02:45,190 --> 00:02:48,279
depredadores, así que inventamos los derechos que

68
00:02:48,280 --> 00:02:50,318
nos protegen de la violación que nos causa

69
00:02:50,319 --> 00:02:53,409
dolor. Incluso los derechos más abstractos como

70
00:02:53,410 --> 00:02:55,659
la libertad tienen sus raíces en la forma en que nuestro cerebro

71
00:02:55,660 --> 00:02:58,059
está cableado para detectar lo que es justo e

72
00:02:58,060 --> 00:03:01,839
injusto. ¿Una tostadora que es incapaz de

73
00:03:01,840 --> 00:03:04,029
moverse, le molestaría estar encerrada en una jaula?

74
00:03:04,030 --> 00:03:07,119
¿Le importaría ser desmantelada si no le tuviera

75
00:03:07,120 --> 00:03:09,939
miedo a la muerte? ¿Le importaría ser

76
00:03:09,940 --> 00:03:11,859
insultada si no tuviera necesidad de tener

77
00:03:11,860 --> 00:03:15,009
autoestima? Pero,  ¿que pasaría si programamos

78
00:03:15,010 --> 00:03:17,459
el robot para sentir dolor y emociones?

79
00:03:17,460 --> 00:03:20,379
Para que prefiriera la justicia sobre la injusticia, el placer sobre

80
00:03:20,380 --> 00:03:22,958
dolor y ser consciente de ello. ¿Eso lo haría

81
00:03:22,959 --> 00:03:25,479
lo suficientemente humano? Muchos

82
00:03:25,480 --> 00:03:27,429
tecnólogos creen que una explosión de

83
00:03:27,430 --> 00:03:29,889
tecnología ocurriría cuando la Inteligencia

84
00:03:29,890 --> 00:03:32,319
Artificial pueda aprender y crear su

85
00:03:32,320 --> 00:03:34,509
propias Inteligencias Artificiales, incluso

86
00:03:34,510 --> 00:03:37,539
más inteligente que ellos. En este punto

87
00:03:37,540 --> 00:03:39,219
la pregunta de como los robots están

88
00:03:39,220 --> 00:03:40,959
programados estará en gran parte fuera de nuestro

89
00:03:40,960 --> 00:03:42,459
control.

90
00:03:42,460 --> 00:03:44,649
¿Que pasaría si una inteligencia artificial encuentra

91
00:03:44,650 --> 00:03:46,539
necesario programar la capacidad de

92
00:03:46,540 --> 00:03:49,029
sentir dolor, al igual que la biología evolutiva

93
00:03:49,030 --> 00:03:50,828
lo vio necesario en la mayoría de las

94
00:03:50,829 --> 00:03:54,779
criaturas vivientes? ¿Los robots se merecen esos derechos?

95
00:03:54,780 --> 00:03:56,909
Pero tal vez deberíamos estar menos preocupados

96
00:03:56,910 --> 00:03:58,289
sobre el riesgo de que robots

97
00:03:58,290 --> 00:04:00,509
superinteligentes se opongan a nosotros y más preocupados

98
00:04:00,510 --> 00:04:03,509
por el peligro que representamos para ellos. Toda nuestra

99
00:04:03,510 --> 00:04:05,759
identidad humana se basa en la idea de

100
00:04:05,760 --> 00:04:08,399
excepcionalismo humano, que somos especiales,

101
00:04:08,400 --> 00:04:10,699
copos de nieve únicos, con derecho a dominar

102
00:04:10,700 --> 00:04:13,799
el mundo natural. Los seres humanos tienen una historia

103
00:04:13,800 --> 00:04:15,839
de negar que otros seres son capaces de

104
00:04:15,840 --> 00:04:18,509
sufrir como ellos lo hacen. En medio de la

105
00:04:18,510 --> 00:04:20,638
Revolución Científica, René Descartes

106
00:04:20,639 --> 00:04:23,579
argumentó que los animales eran simples autómatas, robots si lo prefieres.

107
00:04:23,580 --> 00:04:26,789
Así, al herir un conejo

108
00:04:26,790 --> 00:04:28,469
era tan moralmente repugnante como

109
00:04:28,470 --> 00:04:31,019
golpear un animal de peluche, y muchos de

110
00:04:31,020 --> 00:04:32,729
los mayores crímenes contra la humanidad

111
00:04:32,730 --> 00:04:35,399
fueron justificados por sus autores como

112
00:04:35,400 --> 00:04:37,019
que las víctimas eran más animales

113
00:04:37,020 --> 00:04:40,559
que humanos civilizados. Aún más

114
00:04:40,560 --> 00:04:42,389
problemático es que tenemos un interés economico

115
00:04:42,390 --> 00:04:45,659
en negar derechos de los robots. Si nosotros

116
00:04:45,660 --> 00:04:48,029
podemos obligar a un IA sensible, posiblemente

117
00:04:48,030 --> 00:04:50,069
a través de tortura programada, para que haga lo que

118
00:04:50,070 --> 00:04:50,999
nos plazca,

119
00:04:51,000 --> 00:04:53,668
el potencial económico es ilimitado.

120
00:04:53,669 --> 00:04:56,668
Lo hemos hecho antes, después de todo. La violencia

121
00:04:56,669 --> 00:04:58,769
se ha utilizado para obligar a los humanos a

122
00:04:58,770 --> 00:05:00,839
trabajar, y nunca hemos tenido problemas

123
00:05:00,840 --> 00:05:02,429
para inventar una justificación

124
00:05:02,430 --> 00:05:04,568
ideológica.

125
00:05:04,569 --> 00:05:07,418
Los propietarios de esclavos argumentaron que la esclavitud beneficiaba

126
00:05:07,419 --> 00:05:09,848
a los esclavos. Puso un techo sobre sus cabezas

127
00:05:09,849 --> 00:05:12,579
y les enseñó el cristianismo. Los hombres que

128
00:05:12,580 --> 00:05:14,648
estaban en contra de que las mujeres votasen, argumentaron que

129
00:05:14,649 --> 00:05:16,508
interesaba a las mujeres dejar las

130
00:05:16,509 --> 00:05:20,318
decisiones difíciles a los hombres. Los agricultores argumentaron

131
00:05:20,319 --> 00:05:22,028
que cuidar de los animales y alimentarlos

132
00:05:22,029 --> 00:05:24,068
justifica su muerte temprana por nuestras

133
00:05:24,069 --> 00:05:28,628
preferencias alimentarias. Si los robots se vuelven

134
00:05:28,629 --> 00:05:29,558
concientes,

135
00:05:29,559 --> 00:05:31,269
no habrá escasez de argumentos

136
00:05:31,270 --> 00:05:32,528
para los que dicen que deberían

137
00:05:32,529 --> 00:05:34,688
permanecer sin derechos, especialmente de

138
00:05:34,689 --> 00:05:36,758
aquellos que pueden beneficiarse de ello.

139
00:05:36,759 --> 00:05:39,668
La inteligencia artificial plantea graves

140
00:05:39,669 --> 00:05:41,918
preguntas acerca de los límites filosóficos.

141
00:05:41,919 --> 00:05:44,739
Lo que podemos preguntar si los robots son

142
00:05:44,740 --> 00:05:46,778
conscientes o merecedores de derechos

143
00:05:46,779 --> 00:05:48,938
nos obliga a plantear preguntas básicas,

144
00:05:48,939 --> 00:05:51,638
como qué nos hace humanos, qué nos hace

145
00:05:51,639 --> 00:05:54,419
merecedores de Derechos.

146
00:05:54,420 --> 00:05:56,909
Independientemente de lo que creemos, la pregunta

147
00:05:56,910 --> 00:05:58,379
puede ser que necesite ser resuelta en el futuro próximo.

148
00:05:58,380 --> 00:05:59,189
 

149
00:05:59,190 --> 00:06:01,379
¿Qué vamos a hacer si los robots

150
00:06:01,380 --> 00:06:06,879
comienzan a exigir sus propios derechos?

151
00:06:06,880 --> 00:06:08,669
¿Que pueden los robots que quieren derechos

152
00:06:08,670 --> 00:06:11,369
enseñarnos acerca de nosotros mismos? Nuestros amigos de

153
00:06:11,370 --> 00:06:13,169
Wisecrack hicieron un video explorando esta

154
00:06:13,170 --> 00:06:15,269
pregunta usando la filosofía de

155
00:06:15,270 --> 00:06:18,929
Westworld. Wisecrack disecciona la cultura pop

156
00:06:18,930 --> 00:06:21,239
en una manera única y filosófica.

157
00:06:21,240 --> 00:06:22,979
Haz clic aquí para ver el vídeo y

158
00:06:22,980 --> 00:06:25,000
suscribirte a su canal.


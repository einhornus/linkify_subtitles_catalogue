1
00:00:00,000 --> 00:00:04,048
Imagine um futuro onde a sua torradeira

2
00:00:04,049 --> 00:00:06,419
prevê que tipo de torrada você quer.

3
00:00:06,420 --> 00:00:08,129
Durante o dia

4
00:00:08,130 --> 00:00:09,659
ela escaneia a internet em busca de novos e

5
00:00:09,660 --> 00:00:12,479
interessantes tipos de torrada. Talvez ela

6
00:00:12,480 --> 00:00:14,189
pergunte sobre o seu dia e queira conversar

7
00:00:14,190 --> 00:00:15,899
sobre suas descobertas e nova

8
00:00:15,900 --> 00:00:18,538
tecnologia de torrada. A que ponto

9
00:00:18,539 --> 00:00:21,389
ela se tornaria uma pessoa? Quando você se perguntaria

10
00:00:21,390 --> 00:00:24,299
se sua torrada tem sentimentos?

11
00:00:24,300 --> 00:00:26,969
Se ela tiver, tirar ela da tomada seria assassinato?

12
00:00:26,970 --> 00:00:29,848
E será que você ainda seria dono dela?

13
00:00:29,849 --> 00:00:31,919
Será que um dia seremos forçados a dar nossas máquinas

14
00:00:31,920 --> 00:00:43,139
direitos?

15
00:00:43,140 --> 00:00:45,599
IA já está em todo lugar.

16
00:00:45,600 --> 00:00:47,369
Ela se certifica que os mercados estão estocados

17
00:00:47,370 --> 00:00:48,389
com comida suficiente,

18
00:00:48,390 --> 00:00:50,319
ela te providencia as propagandas certas

19
00:00:50,320 --> 00:00:51,839
e você talvez tenha lido uma

20
00:00:51,840 --> 00:00:54,639
história escrita inteiramente por uma máquina.

21
00:00:54,640 --> 00:00:57,569
Hoje em dia, olhamos para um aplicativo como Siri

22
00:00:57,570 --> 00:00:59,698
e rimos de suas emoções primitivas

23
00:00:59,699 --> 00:01:02,339
simuladas, mas é provável que nós teremos

24
00:01:02,340 --> 00:01:04,018
que lidar com seres que deixam difícil

25
00:01:04,019 --> 00:01:06,209
definir a linha entre o real e

26
00:01:06,210 --> 00:01:08,039
a humanidade simulada.

27
00:01:08,040 --> 00:01:10,829
Existem máquinas já existentes que

28
00:01:10,830 --> 00:01:11,969
merecem direitos?

29
00:01:11,970 --> 00:01:15,929
Provavelmente ainda não, mas se elas surgirem,

30
00:01:15,930 --> 00:01:18,959
nós não estamos preparados.

31
00:01:18,960 --> 00:01:20,908
A maior parte da filosofia de direitos não está equipada para

32
00:01:20,909 --> 00:01:22,529
lidar com o caso de uma Inteligência Artificial.

33
00:01:22,530 --> 00:01:25,228
A maioria dos pedidos por direitos para

34
00:01:25,229 --> 00:01:27,239
humanos ou animais se baseiam

35
00:01:27,240 --> 00:01:29,519
na questão de consciência.

36
00:01:29,520 --> 00:01:31,499
Infelizmente ninguém sabe o que

37
00:01:31,500 --> 00:01:33,868
consciência é.

38
00:01:33,869 --> 00:01:36,089
Alguns dizem que é algo imaterial, outros dizem que é

39
00:01:36,090 --> 00:01:38,749
um estado de matéria, como gás ou líquido.

40
00:01:38,750 --> 00:01:41,389
Independente da definição exata,

41
00:01:41,390 --> 00:01:42,979
nós temos um conhecimento intuitivo de

42
00:01:42,980 --> 00:01:45,879
consciência porque nós a experienciamos.

43
00:01:45,880 --> 00:01:47,389
Nós estamos cientes de nós mesmos e dos

44
00:01:47,390 --> 00:01:49,009
nossos arredores e sabemos

45
00:01:49,010 --> 00:01:52,369
como inconsciência parece.

46
00:01:52,370 --> 00:01:54,139
Alguns neurocientistas acreditam que qualquer

47
00:01:54,140 --> 00:01:56,179
sistema avançado o suficiente pode gerar

48
00:01:56,180 --> 00:01:58,669
consciência. Então, se sua torradeira

49
00:01:58,670 --> 00:02:00,618
fosse potente o suficiente talvez ela

50
00:02:00,619 --> 00:02:04,099
se tornasse consciente. Se ela se tornasse, ela

51
00:02:04,100 --> 00:02:05,519
mereceria direitos?

52
00:02:05,520 --> 00:02:07,619
Bem, não tão rápido.

53
00:02:07,620 --> 00:02:11,389
Será que o que nós definimos como direitos faria sentido para ela?

54
00:02:11,390 --> 00:02:13,759
Consciência dá para os seres direitos

55
00:02:13,760 --> 00:02:15,559
porque ela dá para o ser a

56
00:02:15,560 --> 00:02:17,899
habilidade de sofrer. Significa que

57
00:02:17,900 --> 00:02:21,499
ela não apenas sente dor, mas está ciente dela.

58
00:02:21,500 --> 00:02:24,289
Robôs não sofrem e provavelmente não

59
00:02:24,290 --> 00:02:27,279
iriam a não ser que nós os programássemos para isso.

60
00:02:27,280 --> 00:02:29,499
Sem dor ou prazer não há

61
00:02:29,500 --> 00:02:33,129
preferência e direitos não têm sentido.

62
00:02:33,130 --> 00:02:35,619
Os nossos direitos humanos estão conectados

63
00:02:35,620 --> 00:02:38,409
com a nossa programação. Por exemplo, nós

64
00:02:38,410 --> 00:02:40,659
não gostamos de dor porque nosso cérebro evoluiu

65
00:02:40,660 --> 00:02:43,119
para nos manter vivos, para nos impedir de tocar

66
00:02:43,120 --> 00:02:45,189
no fogo ou para nos fazer correr de

67
00:02:45,190 --> 00:02:48,279
predadores. Então nós inventamos direitos que nos

68
00:02:48,280 --> 00:02:50,318
protegem de acontecimentos que nos causem dor.

69
00:02:50,319 --> 00:02:53,409
Até mesmo direitos abstratos como

70
00:02:53,410 --> 00:02:55,659
liberdade estão enraizados na forma que nossos cérebros

71
00:02:55,660 --> 00:02:58,059
foram feitos para detectar o que é justo e o que é

72
00:02:58,060 --> 00:03:01,839
injusto. Uma torradeira que não pode se mover,

73
00:03:01,840 --> 00:03:04,029
se importaria em ser trancada em uma gaiola?

74
00:03:04,030 --> 00:03:07,119
Ela se importaria em ser desmontada

75
00:03:07,120 --> 00:03:09,939
se não tivesse medo da morte? Ela se

76
00:03:09,940 --> 00:03:11,859
importaria em ser insultada se não tivesse

77
00:03:11,860 --> 00:03:15,009
auto-estima? Mas e se programássemos o robô

78
00:03:15,010 --> 00:03:17,459
para sentir dor e emoções,

79
00:03:17,460 --> 00:03:20,379
para preferir justiça do que injustiça, prazer do que

80
00:03:20,380 --> 00:03:22,958
dor e estar ciente disso? Isso faria

81
00:03:22,959 --> 00:03:25,479
deles suficientemente humano?

82
00:03:25,480 --> 00:03:27,429
Muitos tecnólogos acreditam que uma explosão

83
00:03:27,430 --> 00:03:29,889
tecnológica ocorreria quando inteligência

84
00:03:29,890 --> 00:03:32,319
artificial pudesse aprender e criar sua

85
00:03:32,320 --> 00:03:34,509
própria inteligência artificial,

86
00:03:34,510 --> 00:03:37,539
ainda mais inteligentes do que eles mesmos. Nesse ponto

87
00:03:37,540 --> 00:03:39,219
a questão de como robôs são

88
00:03:39,220 --> 00:03:40,959
programados vai estar fora de nosso

89
00:03:40,960 --> 00:03:42,459
controle.

90
00:03:42,460 --> 00:03:44,649
E se uma inteligência artificial achasse

91
00:03:44,650 --> 00:03:46,539
necessário programar a habilidade de

92
00:03:46,540 --> 00:03:49,029
sentir dor, como a evolução biológica

93
00:03:49,030 --> 00:03:50,828
achou necessária na maior parte dos seres

94
00:03:50,829 --> 00:03:54,779
vivos? Robôs merecem esses direitos?

95
00:03:54,780 --> 00:03:56,909
Talvez nós devêssemos nos preocupar menos

96
00:03:56,910 --> 00:03:58,289
com o risco que robôs super-inteligentes

97
00:03:58,290 --> 00:04:00,509
seriam para nós e mais com

98
00:04:00,510 --> 00:04:03,509
o perigo que nós provocamos a eles. Toda a nossa

99
00:04:03,510 --> 00:04:05,759
identidade humana é baseada na ideia de

100
00:04:05,760 --> 00:04:08,399
excepcionalismo humano, que nós somos únicos e

101
00:04:08,400 --> 00:04:10,699
especiais e destinados a dominar o

102
00:04:10,700 --> 00:04:13,799
mundo natural. Humanos tem história

103
00:04:13,800 --> 00:04:15,839
de negar que outros seres vivos são capazes

104
00:04:15,840 --> 00:04:18,509
de sofrer como nós. No meio da

105
00:04:18,510 --> 00:04:20,638
Revolução Científica, René Descartes

106
00:04:20,639 --> 00:04:23,579
argumentou que animais são autômatos, como

107
00:04:23,580 --> 00:04:26,789
robôs. Assim, machucar um coelho

108
00:04:26,790 --> 00:04:28,469
é tão moralmente repugnante quanto

109
00:04:28,470 --> 00:04:31,019
socar um animal de pelúcia. E muitos dos maiores

110
00:04:31,020 --> 00:04:32,729
crimes contra a humanidade

111
00:04:32,730 --> 00:04:35,399
foram justificados por seus causadores

112
00:04:35,400 --> 00:04:37,019
na ideia de que as vitimas eram mais

113
00:04:37,020 --> 00:04:40,559
animais do que humanos civilizados. Ainda mais

114
00:04:40,560 --> 00:04:42,389
problemático é que temos um interesse

115
00:04:42,390 --> 00:04:45,659
econômico em negar direitos para robôs. Se pudermos

116
00:04:45,660 --> 00:04:48,029
forçar uma IA consciente, possivelmente através de

117
00:04:48,030 --> 00:04:50,069
tortura programada, em fazer como

118
00:04:50,070 --> 00:04:50,999
o que desejamos,

119
00:04:51,000 --> 00:04:53,668
o potencial econômico não tem limites.

120
00:04:53,669 --> 00:04:56,668
Nós já fizemos isso antes, afinal. Violência

121
00:04:56,669 --> 00:04:58,769
foi usada para forçar outros humanos em

122
00:04:58,770 --> 00:05:00,839
trabalhar, e nunca tivemos problema

123
00:05:00,840 --> 00:05:02,429
em inventar uma justificação

124
00:05:02,430 --> 00:05:04,568
ideológica. ("Os deuses querem ouro!")

125
00:05:04,569 --> 00:05:07,418
Escravocratas argumentam que escravidão beneficiou

126
00:05:07,419 --> 00:05:09,848
os escravos. Colocou um teto sobre suas cabeças

127
00:05:09,849 --> 00:05:12,579
e os ensinou cristandade. Homens que

128
00:05:12,580 --> 00:05:14,648
eram contra o voto de mulheres argumentaram que

129
00:05:14,649 --> 00:05:16,508
era do próprio interesse delas deixar as

130
00:05:16,509 --> 00:05:20,318
decisões difíceis para os homens. Fazendeiros argumentaram que

131
00:05:20,319 --> 00:05:22,028
cuidar de animais e alimentar eles

132
00:05:22,029 --> 00:05:24,068
justificava sua morte prematura para nossas

133
00:05:24,069 --> 00:05:28,628
preferências dietéticas.
Caso robôs tornem-se

134
00:05:28,629 --> 00:05:29,558
conscientes,

135
00:05:29,559 --> 00:05:31,269
não faltarão argumentos

136
00:05:31,270 --> 00:05:32,528
para aqueles que dizem que eles

137
00:05:32,529 --> 00:05:34,688
deveriam continuar sem direitos. Especialmente

138
00:05:34,689 --> 00:05:36,758
daqueles que ganhariam lucro disso.

139
00:05:36,759 --> 00:05:39,668
Inteligência Artificial traz sérias questões

140
00:05:39,669 --> 00:05:41,918
sobre limites filosóficos.

141
00:05:41,919 --> 00:05:44,739
Enquanto nos perguntamos se robôs são

142
00:05:44,740 --> 00:05:46,778
conscientes ou merecem direitos,

143
00:05:46,779 --> 00:05:48,938
isso nos força a pensar em questões básicas

144
00:05:48,939 --> 00:05:51,638
como o que nos faz humanos, o que nos faz

145
00:05:51,639 --> 00:05:54,419
merecedores de direitos?

146
00:05:54,420 --> 00:05:56,909
Independente do que pensamos, essa pergunta

147
00:05:56,910 --> 00:05:58,379
poderá ter de ser respondida num

148
00:05:58,380 --> 00:05:59,189
futuro próximo.

149
00:05:59,190 --> 00:06:01,379
O que faremos se os robôs

150
00:06:01,380 --> 00:06:06,879
começarem a exigir seus direitos?

151
00:06:06,880 --> 00:06:08,669
O que robôs exigindo direitos

152
00:06:08,670 --> 00:06:11,369
nos ensina sobre nós mesmos? Nossos

153
00:06:11,370 --> 00:06:13,169
amigos em Wisecrack fizeram um vídeo explorando

154
00:06:13,170 --> 00:06:15,269
essa questão, usando a filosofia de

155
00:06:15,270 --> 00:06:18,929
Westworld. Wisecrack fala sobre a cultura pop

156
00:06:18,930 --> 00:06:21,239
em uma maneira única e filosófica.

157
00:06:21,240 --> 00:06:22,979
Clique aqui para assistir o vídeo deles

158
00:06:22,980 --> 00:06:25,000
e se inscrever em seu canal.

